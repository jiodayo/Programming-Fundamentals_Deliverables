"""ãƒªã‚½ãƒ¼ã‚¹ã‚’è€ƒæ…®ã—ãŸé…ç½®æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

æ©Ÿèƒ½:
- å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ã®å¯†åº¦åˆ†æã‹ã‚‰å€™è£œåœ°ç‚¹ã‚’è‡ªå‹•ç”Ÿæˆ
- æ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ï¼ˆæ•‘æ€¥è»Šå°æ•°ï¼‰ã‚’è€ƒæ…®ã—ãŸæœ€é©é…ç½®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
- é«˜é€Ÿãªè²ªæ¬²æ³•ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã®é«˜é€Ÿå‹•ä½œ

ä½œæˆ: 2026/02/02
"""

from __future__ import annotations

import json
import pickle
import sqlite3
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Optional

import folium
import geopandas as gpd
import networkx as nx
import numpy as np
import osmnx as ox
import pandas as pd
from scipy import ndimage
from scipy.spatial import cKDTree
from shapely.geometry import Point
from shapely.ops import unary_union

# =============================================================================
# å®šæ•°
# =============================================================================

CACHE_DIR = Path("cache")
GRAPH_PATH = CACHE_DIR / "matsuyama_drive.graphml"
STATIONS_DB_PATH = Path("map.sqlite")
INCIDENTS_DB_PATH = Path("incidents.sqlite")
GEOCODE_CACHE_PATH = CACHE_DIR / "incident_geocode.parquet"

# å„æ¶ˆé˜²ç½²ã®æ•‘æ€¥è»Šå°æ•°ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿æ¨å®šï¼‰
STATION_RESOURCES = {
    "æ±æ¶ˆé˜²ç½²": 4,
    "ä¸­å¤®æ¶ˆé˜²ç½²": 3,
    "è¥¿æ¶ˆé˜²ç½²": 3,
    "å—æ¶ˆé˜²ç½²": 3,
    "åŸåŒ—æ”¯ç½²": 1,
    "åŸæ±æ”¯ç½²": 1,
    "è¥¿éƒ¨æ”¯ç½²": 1,
    "æ±éƒ¨æ”¯ç½²": 1,
    "åŒ—æ¡æ”¯ç½²": 1,
    "æ¹¯å±±å‡ºå¼µæ‰€": 1,
    "ä¹…è°·å‡ºå¼µæ‰€": 1,
    "æ¶ˆé˜²å±€": 1,
    "WS": 1,
}

# æ–°è¦æ¶ˆé˜²ç½²ã®æƒ³å®šãƒªã‚½ãƒ¼ã‚¹
NEW_STATION_AMBULANCES = 2

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå°æ•°
DEFAULT_AMBULANCES = 1


# =============================================================================
# ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# =============================================================================

@dataclass
class OptimizationResult:
    """æœ€é©åŒ–çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    candidate_locations: list[dict]  # å€™è£œåœ°ç‚¹ãƒªã‚¹ãƒˆ
    best_location: dict | None  # æœ€é©åœ°ç‚¹
    coverage_improvement: dict  # ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„
    resource_efficiency: dict  # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡
    computation_time_sec: float  # è¨ˆç®—æ™‚é–“


@dataclass
class CandidateLocation:
    """å€™è£œåœ°ç‚¹"""
    lat: float
    lon: float
    name: str
    reason: str  # å€™è£œç†ç”±
    priority_score: float  # å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰
    incident_density: float  # å‘¨è¾ºå‡ºå‹•å¯†åº¦
    current_coverage_gap: float  # ç¾åœ¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—


# =============================================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# =============================================================================

def load_graph() -> nx.MultiDiGraph:
    """é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿"""
    if GRAPH_PATH.exists():
        return ox.load_graphml(GRAPH_PATH)
    raise FileNotFoundError(f"Graph not found: {GRAPH_PATH}")


def load_stations() -> gpd.GeoDataFrame:
    """æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ä»˜ãï¼‰"""
    if STATIONS_DB_PATH.exists():
        with sqlite3.connect(STATIONS_DB_PATH) as conn:
            df = pd.read_sql("SELECT * FROM stations", conn)
    else:
        df = pd.read_excel("map.xlsx")
    
    df["æ•‘æ€¥è»Šå°æ•°"] = df["ç•¥ç§°"].map(STATION_RESOURCES).fillna(DEFAULT_AMBULANCES).astype(int)
    df["åŒºåˆ†"] = df["ç•¥ç§°"].apply(lambda x: "ç½²" if "æ¶ˆé˜²ç½²" in x or "æ¶ˆé˜²å±€" in x else "æ”¯ç½²ãƒ»å‡ºå¼µæ‰€")
    
    geometry = gpd.points_from_xy(df["çµŒåº¦"], df["ç·¯åº¦"])
    return gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")


def load_incident_locations() -> gpd.GeoDataFrame:
    """å‡ºå‹•åœ°ç‚¹ã®ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    if GEOCODE_CACHE_PATH.exists():
        df = pd.read_parquet(GEOCODE_CACHE_PATH)
        df = df.dropna(subset=["lat", "lon"])
        geometry = gpd.points_from_xy(df["lon"], df["lat"])
        return gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")
    return gpd.GeoDataFrame(columns=["address", "lat", "lon", "geometry"], crs="EPSG:4326")


# =============================================================================
# å€™è£œåœ°ç‚¹ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
# =============================================================================

def generate_candidate_locations(
    stations: gpd.GeoDataFrame,
    incidents: gpd.GeoDataFrame,
    travel_times_cache: dict | None = None,
    n_candidates: int = 10,
    resolution_km: float = 0.5,
    progress_cb: Callable[[float], None] | None = None,
) -> list[CandidateLocation]:
    """å€™è£œåœ°ç‚¹ã‚’è‡ªå‹•ç”Ÿæˆ
    
    ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯:
    1. å‡ºå‹•å¯†åº¦ãŒé«˜ã„åœ°ç‚¹ã‚’æŠ½å‡º
    2. ç¾åœ¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒå¼±ã„åœ°ç‚¹ã‚’æŠ½å‡º
    3. ä¸¡è€…ã‚’çµ„ã¿åˆã‚ã›ã¦å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    Args:
        stations: æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿
        incidents: å‡ºå‹•åœ°ç‚¹ãƒ‡ãƒ¼ã‚¿
        travel_times_cache: äº‹å‰è¨ˆç®—ã—ãŸåˆ°é”æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        n_candidates: ç”Ÿæˆã™ã‚‹å€™è£œæ•°
        resolution_km: ã‚°ãƒªãƒƒãƒ‰è§£åƒåº¦ï¼ˆkmï¼‰
        progress_cb: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    Returns:
        å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸå€™è£œåœ°ç‚¹ãƒªã‚¹ãƒˆ
    """
    start_time = time.time()
    
    if incidents.empty:
        return []
    
    # åˆ†æç¯„å›²ã‚’æ±ºå®š
    bounds = _compute_bounds(stations, incidents, buffer_km=3.0)
    min_lon, min_lat, max_lon, max_lat = bounds
    
    if progress_cb:
        progress_cb(0.1)
    
    # Step 1: å‡ºå‹•å¯†åº¦ãƒãƒƒãƒ—ã‚’ä½œæˆ
    density_map = _compute_incident_density_map(
        incidents, bounds, resolution_km
    )
    
    if progress_cb:
        progress_cb(0.3)
    
    # Step 2: ç¾åœ¨ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ã‚’è¨ˆç®—
    coverage_gap_map = _compute_coverage_gap_map(
        stations, bounds, resolution_km, travel_times_cache
    )
    
    if progress_cb:
        progress_cb(0.6)
    
    # Step 3: è¤‡åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—å€™è£œã‚’æŠ½å‡º
    candidates = _extract_candidates(
        density_map, coverage_gap_map, stations,
        bounds, resolution_km, n_candidates
    )
    
    if progress_cb:
        progress_cb(1.0)
    
    elapsed = time.time() - start_time
    print(f"å€™è£œåœ°ç‚¹ç”Ÿæˆå®Œäº†: {len(candidates)}åœ°ç‚¹ ({elapsed:.2f}ç§’)")
    
    return candidates


def _compute_bounds(
    stations: gpd.GeoDataFrame,
    incidents: gpd.GeoDataFrame,
    buffer_km: float = 3.0,
) -> tuple[float, float, float, float]:
    """åˆ†æç¯„å›²ã‚’è¨ˆç®—"""
    all_lons = list(stations["çµŒåº¦"]) + list(incidents["lon"])
    all_lats = list(stations["ç·¯åº¦"]) + list(incidents["lat"])
    
    lat_buffer = buffer_km / 111.0
    lon_buffer = buffer_km / (111.0 * np.cos(np.radians(np.mean(all_lats))))
    
    return (
        min(all_lons) - lon_buffer,
        min(all_lats) - lat_buffer,
        max(all_lons) + lon_buffer,
        max(all_lats) + lat_buffer,
    )


def _compute_incident_density_map(
    incidents: gpd.GeoDataFrame,
    bounds: tuple[float, float, float, float],
    resolution_km: float,
) -> dict:
    """å‡ºå‹•å¯†åº¦ãƒãƒƒãƒ—ã‚’ä½œæˆï¼ˆKDEï¼‰"""
    min_lon, min_lat, max_lon, max_lat = bounds
    
    lat_step = resolution_km / 111.0
    lon_step = resolution_km / (111.0 * np.cos(np.radians((min_lat + max_lat) / 2)))
    
    lats = np.arange(min_lat, max_lat, lat_step)
    lons = np.arange(min_lon, max_lon, lon_step)
    
    # ã‚°ãƒªãƒƒãƒ‰ã‚’ä½œæˆ
    lon_grid, lat_grid = np.meshgrid(lons, lats)
    
    # å‡ºå‹•åœ°ç‚¹ã®åº§æ¨™
    incident_coords = np.column_stack([incidents["lon"], incidents["lat"]])
    
    if len(incident_coords) == 0:
        return {
            "density": np.zeros_like(lon_grid),
            "lats": lats,
            "lons": lons,
            "lat_grid": lat_grid,
            "lon_grid": lon_grid,
        }
    
    # KDTreeã§è¿‘å‚å¯†åº¦ã‚’é«˜é€Ÿè¨ˆç®—
    tree = cKDTree(incident_coords)
    
    density = np.zeros_like(lon_grid)
    bandwidth_km = 2.0  # ã‚«ãƒ¼ãƒãƒ«å¸¯åŸŸå¹…
    bandwidth_deg = bandwidth_km / 111.0
    
    for i in range(lon_grid.shape[0]):
        for j in range(lon_grid.shape[1]):
            point = [lon_grid[i, j], lat_grid[i, j]]
            # è¿‘å‚ã®å‡ºå‹•ä»¶æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            neighbors = tree.query_ball_point(point, bandwidth_deg)
            density[i, j] = len(neighbors)
    
    # æ­£è¦åŒ–
    if density.max() > 0:
        density = density / density.max()
    
    return {
        "density": density,
        "lats": lats,
        "lons": lons,
        "lat_grid": lat_grid,
        "lon_grid": lon_grid,
    }


def _compute_coverage_gap_map(
    stations: gpd.GeoDataFrame,
    bounds: tuple[float, float, float, float],
    resolution_km: float,
    travel_times_cache: dict | None = None,
) -> dict:
    """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ãƒãƒƒãƒ—ã‚’ä½œæˆ
    
    ç°¡æ˜“ç‰ˆ: å„ã‚°ãƒªãƒƒãƒ‰ã‹ã‚‰æœ€å¯„ã‚Šæ¶ˆé˜²ç½²ã¾ã§ã®è·é›¢ã‚’ãƒ™ãƒ¼ã‚¹ã«è¨ˆç®—
    ï¼ˆæ­£ç¢ºãªåˆ°é”æ™‚é–“ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨ï¼‰
    """
    min_lon, min_lat, max_lon, max_lat = bounds
    
    lat_step = resolution_km / 111.0
    lon_step = resolution_km / (111.0 * np.cos(np.radians((min_lat + max_lat) / 2)))
    
    lats = np.arange(min_lat, max_lat, lat_step)
    lons = np.arange(min_lon, max_lon, lon_step)
    lon_grid, lat_grid = np.meshgrid(lons, lats)
    
    # æ¶ˆé˜²ç½²åº§æ¨™
    station_coords = np.column_stack([stations["çµŒåº¦"], stations["ç·¯åº¦"]])
    station_ambulances = stations["æ•‘æ€¥è»Šå°æ•°"].values
    
    tree = cKDTree(station_coords)
    
    # å„ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã€Œ8åˆ†åœå†…ã®æ•‘æ€¥è»Šå°æ•°ã€ã‚’æ¨å®š
    coverage = np.zeros_like(lon_grid)
    target_distance_km = 8 * 40 / 60  # 8åˆ† Ã— 40km/h â‰ˆ 5.3km
    target_distance_deg = target_distance_km / 111.0
    
    for i in range(lon_grid.shape[0]):
        for j in range(lon_grid.shape[1]):
            point = [lon_grid[i, j], lat_grid[i, j]]
            
            # åˆ°é”å¯èƒ½ãªæ¶ˆé˜²ç½²ã‚’ç‰¹å®š
            indices = tree.query_ball_point(point, target_distance_deg)
            
            # æ•‘æ€¥è»Šå°æ•°ã‚’åˆè¨ˆ
            total_ambulances = sum(station_ambulances[idx] for idx in indices)
            coverage[i, j] = total_ambulances
    
    # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ï¼ˆä½ã„ã»ã©ã‚®ãƒ£ãƒƒãƒ—ãŒå¤§ãã„ï¼‰
    # 2å°ä»¥ä¸Šã‚’ç›®æ¨™ã¨ã—ã¦ã€ä¸è¶³åˆ†ã‚’ã‚®ãƒ£ãƒƒãƒ—ã¨ã™ã‚‹
    target_ambulances = 2
    gap = np.maximum(0, target_ambulances - coverage)
    
    # æ­£è¦åŒ–
    if gap.max() > 0:
        gap = gap / gap.max()
    
    return {
        "gap": gap,
        "coverage": coverage,
        "lats": lats,
        "lons": lons,
        "lat_grid": lat_grid,
        "lon_grid": lon_grid,
    }


def _extract_candidates(
    density_map: dict,
    coverage_gap_map: dict,
    stations: gpd.GeoDataFrame,
    bounds: tuple[float, float, float, float],
    resolution_km: float,
    n_candidates: int,
) -> list[CandidateLocation]:
    """è¤‡åˆã‚¹ã‚³ã‚¢ã‹ã‚‰å€™è£œåœ°ç‚¹ã‚’æŠ½å‡º"""
    
    density = density_map["density"]
    gap = coverage_gap_map["gap"]
    coverage = coverage_gap_map["coverage"]
    lat_grid = density_map["lat_grid"]
    lon_grid = density_map["lon_grid"]
    
    # è¤‡åˆã‚¹ã‚³ã‚¢: å‡ºå‹•å¯†åº¦ Ã— ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—
    # ä¸¡æ–¹ãŒé«˜ã„åœ°ç‚¹ãŒå„ªå…ˆã•ã‚Œã‚‹
    combined_score = density * (gap + 0.1)  # gapãŒ0ã§ã‚‚å¯†åº¦ãŒé«˜ã‘ã‚Œã°å€™è£œã«
    
    # æ—¢å­˜æ¶ˆé˜²ç½²ã®è¿‘ãã¯é™¤å¤–ï¼ˆæœ€ä½1kmé›¢ã‚ŒãŸåœ°ç‚¹ã®ã¿ï¼‰
    station_coords = np.column_stack([stations["çµŒåº¦"], stations["ç·¯åº¦"]])
    station_tree = cKDTree(station_coords)
    min_distance_km = 1.0
    min_distance_deg = min_distance_km / 111.0
    
    # ãƒã‚¹ã‚¯ä½œæˆ
    mask = np.ones_like(combined_score, dtype=bool)
    for i in range(lon_grid.shape[0]):
        for j in range(lon_grid.shape[1]):
            point = [lon_grid[i, j], lat_grid[i, j]]
            dist, _ = station_tree.query(point)
            if dist < min_distance_deg:
                mask[i, j] = False
    
    combined_score = np.where(mask, combined_score, 0)
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚­ã‚·ãƒï¼‰
    # å˜ç´”ã«ä¸Šä½Nå€‹ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    flat_indices = np.argsort(combined_score.ravel())[::-1]
    
    candidates = []
    used_positions = set()
    min_separation_deg = 0.5 / 111.0  # å€™è£œåŒå£«ã¯æœ€ä½0.5kmé›¢ã™
    
    for flat_idx in flat_indices:
        if len(candidates) >= n_candidates:
            break
        
        i, j = np.unravel_index(flat_idx, combined_score.shape)
        lat = lat_grid[i, j]
        lon = lon_grid[i, j]
        
        # æ—¢å­˜å€™è£œã¨è¿‘ã™ããªã„ã‹ãƒã‚§ãƒƒã‚¯
        too_close = False
        for used_lat, used_lon in used_positions:
            dist = np.sqrt((lat - used_lat)**2 + (lon - used_lon)**2)
            if dist < min_separation_deg:
                too_close = True
                break
        
        if too_close:
            continue
        
        score = combined_score[i, j]
        if score <= 0:
            continue
        
        # å€™è£œç†ç”±ã‚’ç”Ÿæˆ
        density_val = density[i, j]
        gap_val = gap[i, j]
        coverage_val = coverage[i, j]
        
        if density_val > 0.7 and gap_val > 0.5:
            reason = "å‡ºå‹•å¯†åº¦é«˜ & ã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³"
        elif density_val > 0.5:
            reason = "å‡ºå‹•å¯†åº¦ãŒé«˜ã„ã‚¨ãƒªã‚¢"
        elif gap_val > 0.7:
            reason = "ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ãŒå¤§ãã„"
        elif coverage_val < 2:
            reason = "å†—é•·æ€§ãŒä¸è¶³ï¼ˆæ•‘æ€¥è»Š1å°ä»¥ä¸‹ï¼‰"
        else:
            reason = "ãƒãƒ©ãƒ³ã‚¹æ”¹å–„"
        
        candidates.append(CandidateLocation(
            lat=float(lat),
            lon=float(lon),
            name=f"å€™è£œåœ°ç‚¹{len(candidates) + 1}",
            reason=reason,
            priority_score=float(score),
            incident_density=float(density_val),
            current_coverage_gap=float(gap_val),
        ))
        
        used_positions.add((lat, lon))
    
    return candidates


# =============================================================================
# æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
# =============================================================================

def simulate_new_station(
    graph: nx.MultiDiGraph,
    stations: gpd.GeoDataFrame,
    incidents: gpd.GeoDataFrame,
    candidate: CandidateLocation,
    threshold_min: int = 8,
    new_ambulances: int = NEW_STATION_AMBULANCES,
) -> dict:
    """æ–°è¦æ¶ˆé˜²ç½²è¿½åŠ ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    
    é«˜é€ŸåŒ–ã®ãŸã‚ã€ç°¡æ˜“è¨ˆç®—ã‚’è¡Œã†
    """
    # å€™è£œåœ°ç‚¹ã®æœ€å¯„ã‚Šãƒãƒ¼ãƒ‰
    try:
        candidate_node = ox.distance.nearest_nodes(graph, candidate.lon, candidate.lat)
    except Exception:
        return {
            "candidate": candidate,
            "error": "ã‚°ãƒ©ãƒ•ä¸Šã®ãƒãƒ¼ãƒ‰ã‚’ç‰¹å®šã§ãã¾ã›ã‚“",
        }
    
    # å‡ºå‹•åœ°ç‚¹ã®æœ€å¯„ã‚Šãƒãƒ¼ãƒ‰ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§é«˜é€ŸåŒ–ï¼‰
    max_samples = min(500, len(incidents))
    sampled_incidents = incidents.sample(n=max_samples, random_state=42) if len(incidents) > max_samples else incidents
    
    try:
        incident_nodes = ox.distance.nearest_nodes(
            graph,
            sampled_incidents["lon"].tolist(),
            sampled_incidents["lat"].tolist()
        )
    except Exception:
        incident_nodes = [
            ox.distance.nearest_nodes(graph, lon, lat)
            for lon, lat in zip(sampled_incidents["lon"], sampled_incidents["lat"])
        ]
    
    # å€™è£œåœ°ç‚¹ã‹ã‚‰ã®åˆ°é”æ™‚é–“ã‚’è¨ˆç®—
    threshold_sec = threshold_min * 60
    try:
        lengths = nx.single_source_dijkstra_path_length(
            graph,
            candidate_node,
            cutoff=threshold_sec,
            weight="travel_time",
        )
    except Exception:
        return {
            "candidate": candidate,
            "error": "åˆ°é”æ™‚é–“ã®è¨ˆç®—ã«å¤±æ•—",
        }
    
    # ã‚«ãƒãƒ¼å¯èƒ½ãªå‡ºå‹•ä»¶æ•°
    covered_count = sum(1 for node in incident_nodes if node in lengths)
    coverage_rate = covered_count / len(sampled_incidents) * 100
    
    # æ—¢å­˜æ¶ˆé˜²ç½²ã§ã‚«ãƒãƒ¼ã§ãã¦ã„ãªã„ä»¶æ•°ã‚’ç‰¹å®š
    # ï¼ˆç°¡æ˜“ç‰ˆ: è·é›¢ãƒ™ãƒ¼ã‚¹ï¼‰
    station_coords = np.column_stack([stations["çµŒåº¦"], stations["ç·¯åº¦"]])
    station_tree = cKDTree(station_coords)
    
    target_distance_km = threshold_min * 40 / 60
    target_distance_deg = target_distance_km / 111.0
    
    newly_covered = 0
    for _, inc in sampled_incidents.iterrows():
        inc_point = [inc["lon"], inc["lat"]]
        
        # æ—¢å­˜ã‚«ãƒãƒ¼ç¢ºèª
        dist, _ = station_tree.query(inc_point)
        existing_covered = dist <= target_distance_deg
        
        # å€™è£œåœ°ç‚¹ã‹ã‚‰ã®è·é›¢
        cand_dist = np.sqrt(
            (inc["lon"] - candidate.lon)**2 + 
            (inc["lat"] - candidate.lat)**2
        )
        new_covered = cand_dist <= target_distance_deg
        
        if new_covered and not existing_covered:
            newly_covered += 1
    
    # ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®å ´åˆï¼‰
    scale_factor = len(incidents) / len(sampled_incidents)
    estimated_newly_covered = int(newly_covered * scale_factor)
    
    return {
        "candidate": candidate,
        "coverage_rate": coverage_rate,
        "covered_incidents": covered_count,
        "newly_covered_incidents": estimated_newly_covered,
        "sampled_total": len(sampled_incidents),
        "actual_total": len(incidents),
        "new_ambulances": new_ambulances,
        "efficiency_score": estimated_newly_covered / new_ambulances if new_ambulances > 0 else 0,
    }


def optimize_placement(
    stations: gpd.GeoDataFrame,
    incidents: gpd.GeoDataFrame,
    candidates: list[CandidateLocation],
    threshold_min: int = 8,
    new_ambulances: int = NEW_STATION_AMBULANCES,
    progress_cb: Callable[[float], None] | None = None,
) -> OptimizationResult:
    """æœ€é©é…ç½®ã‚’æ±ºå®š
    
    è²ªæ¬²æ³•: å„å€™è£œã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€æœ€ã‚‚åŠ¹æœãŒé«˜ã„åœ°ç‚¹ã‚’é¸æŠ
    """
    start_time = time.time()
    
    if not candidates:
        return OptimizationResult(
            candidate_locations=[],
            best_location=None,
            coverage_improvement={},
            resource_efficiency={},
            computation_time_sec=0,
        )
    
    graph = load_graph()
    
    results = []
    for i, candidate in enumerate(candidates):
        result = simulate_new_station(
            graph, stations, incidents, candidate,
            threshold_min=threshold_min,
            new_ambulances=new_ambulances,
        )
        results.append(result)
        
        if progress_cb:
            progress_cb((i + 1) / len(candidates))
    
    # ã‚¨ãƒ©ãƒ¼ã®ãªã„çµæœã®ã¿
    valid_results = [r for r in results if "error" not in r]
    
    if not valid_results:
        return OptimizationResult(
            candidate_locations=[c.__dict__ for c in candidates],
            best_location=None,
            coverage_improvement={"error": "ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¤±æ•—"},
            resource_efficiency={},
            computation_time_sec=time.time() - start_time,
        )
    
    # åŠ¹ç‡ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
    valid_results.sort(key=lambda x: x["efficiency_score"], reverse=True)
    best = valid_results[0]
    
    elapsed = time.time() - start_time
    
    return OptimizationResult(
        candidate_locations=[r["candidate"].__dict__ for r in valid_results],
        best_location={
            "lat": best["candidate"].lat,
            "lon": best["candidate"].lon,
            "name": best["candidate"].name,
            "reason": best["candidate"].reason,
            "newly_covered_incidents": best["newly_covered_incidents"],
            "efficiency_score": best["efficiency_score"],
        },
        coverage_improvement={
            "newly_covered_incidents": best["newly_covered_incidents"],
            "coverage_rate": best["coverage_rate"],
            "sampled_total": best["sampled_total"],
        },
        resource_efficiency={
            "new_ambulances": best["new_ambulances"],
            "incidents_per_ambulance": best["efficiency_score"],
        },
        computation_time_sec=elapsed,
    )


# =============================================================================
# å¯è¦–åŒ–
# =============================================================================

def create_optimization_map(
    stations: gpd.GeoDataFrame,
    incidents: gpd.GeoDataFrame,
    candidates: list[CandidateLocation],
    best_location: dict | None = None,
) -> folium.Map:
    """æœ€é©åŒ–çµæœã®å¯è¦–åŒ–ãƒãƒƒãƒ—"""
    
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    
    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=11,
        tiles="CartoDB Positron",
    )
    
    # å‡ºå‹•åœ°ç‚¹ã‚’ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—é¢¨ã«è¡¨ç¤ºï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
    max_display = 1000
    if len(incidents) > max_display:
        display_incidents = incidents.sample(n=max_display, random_state=42)
    else:
        display_incidents = incidents
    
    for _, inc in display_incidents.iterrows():
        folium.CircleMarker(
            location=[inc["lat"], inc["lon"]],
            radius=2,
            color="#888888",
            fill=True,
            fill_opacity=0.3,
            opacity=0.3,
        ).add_to(m)
    
    # æ—¢å­˜æ¶ˆé˜²ç½²
    for _, station in stations.iterrows():
        folium.Marker(
            location=[station["ç·¯åº¦"], station["çµŒåº¦"]],
            popup=f"{station['ç•¥ç§°']} ({station['æ•‘æ€¥è»Šå°æ•°']}å°)",
            icon=folium.Icon(color="blue", icon="plus", prefix="fa"),
        ).add_to(m)
    
    # å€™è£œåœ°ç‚¹
    for i, cand in enumerate(candidates):
        is_best = best_location and abs(cand.lat - best_location["lat"]) < 0.0001 and abs(cand.lon - best_location["lon"]) < 0.0001
        
        if is_best:
            # æœ€é©åœ°ç‚¹ã¯å¼·èª¿
            folium.Marker(
                location=[cand.lat, cand.lon],
                popup=f"â­ {cand.name}<br>{cand.reason}<br>ã‚¹ã‚³ã‚¢: {cand.priority_score:.3f}",
                icon=folium.Icon(color="red", icon="star", prefix="fa"),
            ).add_to(m)
            
            # åˆ°é”åœã®æ¦‚ç•¥ï¼ˆå††ã§è¡¨ç¤ºï¼‰
            folium.Circle(
                location=[cand.lat, cand.lon],
                radius=5000,  # 8åˆ† Ã— 40km/h â‰ˆ 5km
                color="red",
                fill=True,
                fill_opacity=0.1,
                popup="8åˆ†åˆ°é”åœï¼ˆæ¦‚ç®—ï¼‰",
            ).add_to(m)
        else:
            folium.CircleMarker(
                location=[cand.lat, cand.lon],
                radius=10,
                color="orange",
                fill=True,
                fill_color="orange",
                fill_opacity=0.7,
                popup=f"{cand.name}<br>{cand.reason}<br>ã‚¹ã‚³ã‚¢: {cand.priority_score:.3f}",
            ).add_to(m)
    
    # å‡¡ä¾‹
    legend_html = """
    <div style="position: fixed; bottom: 50px; left: 50px; z-index: 1000; 
                background: white; padding: 10px; border-radius: 5px;
                border: 2px solid gray; font-size: 12px;">
        <b>é…ç½®æœ€é©åŒ–</b><br>
        <span style="color: blue;">ğŸ“</span> æ—¢å­˜æ¶ˆé˜²ç½²<br>
        <span style="color: red;">â­</span> æœ€é©å€™è£œåœ°ç‚¹<br>
        <span style="color: orange;">â—</span> ãã®ä»–å€™è£œåœ°ç‚¹<br>
        <span style="color: gray;">ãƒ»</span> å‡ºå‹•åœ°ç‚¹
    </div>
    """
    m.get_root().html.add_child(folium.Element(legend_html))
    
    return m


# =============================================================================
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥
# =============================================================================

OPTIMIZATION_CACHE_PATH = CACHE_DIR / "optimization_candidates.json"


def save_candidates_cache(candidates: list[CandidateLocation]) -> None:
    """å€™è£œåœ°ç‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    CACHE_DIR.mkdir(exist_ok=True)
    data = [c.__dict__ for c in candidates]
    with open(OPTIMIZATION_CACHE_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def load_candidates_cache() -> list[CandidateLocation] | None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€™è£œåœ°ç‚¹ã‚’èª­ã¿è¾¼ã¿"""
    if OPTIMIZATION_CACHE_PATH.exists():
        with open(OPTIMIZATION_CACHE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
        return [CandidateLocation(**d) for d in data]
    return None


# =============================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =============================================================================

def run_optimization(
    n_candidates: int = 10,
    threshold_min: int = 8,
    use_cache: bool = True,
    progress_cb: Callable[[float], None] | None = None,
) -> OptimizationResult:
    """æœ€é©åŒ–ã‚’å®Ÿè¡Œ
    
    Args:
        n_candidates: ç”Ÿæˆã™ã‚‹å€™è£œåœ°ç‚¹æ•°
        threshold_min: åˆ°é”æ™‚é–“ã®é–¾å€¤ï¼ˆåˆ†ï¼‰
        use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        progress_cb: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
    
    Returns:
        æœ€é©åŒ–çµæœ
    """
    start_time = time.time()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    stations = load_stations()
    incidents = load_incident_locations()
    
    if incidents.empty:
        print("âš ï¸ å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return OptimizationResult(
            candidate_locations=[],
            best_location=None,
            coverage_improvement={},
            resource_efficiency={},
            computation_time_sec=0,
        )
    
    # å€™è£œåœ°ç‚¹ç”Ÿæˆ
    candidates = None
    if use_cache:
        candidates = load_candidates_cache()
    
    if candidates is None:
        if progress_cb:
            progress_cb(0.1)
        candidates = generate_candidate_locations(
            stations, incidents, n_candidates=n_candidates
        )
        save_candidates_cache(candidates)
    
    if progress_cb:
        progress_cb(0.3)
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    result = optimize_placement(
        stations, incidents, candidates,
        threshold_min=threshold_min,
        progress_cb=lambda p: progress_cb(0.3 + p * 0.7) if progress_cb else None,
    )
    
    result.computation_time_sec = time.time() - start_time
    
    return result


# =============================================================================
# CLIå®Ÿè¡Œ
# =============================================================================

def main():
    print("ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…® é…ç½®æœ€é©åŒ–")
    print("=" * 50)
    
    result = run_optimization(
        n_candidates=10,
        threshold_min=8,
        use_cache=False,
    )
    
    print(f"\nâ±ï¸ è¨ˆç®—æ™‚é–“: {result.computation_time_sec:.2f}ç§’")
    
    if result.best_location:
        print(f"\nâ­ æœ€é©å€™è£œåœ°ç‚¹:")
        print(f"   ä½ç½®: ({result.best_location['lat']:.5f}, {result.best_location['lon']:.5f})")
        print(f"   ç†ç”±: {result.best_location['reason']}")
        print(f"   æ–°è¦ã‚«ãƒãƒ¼: {result.best_location['newly_covered_incidents']}ä»¶")
        print(f"   åŠ¹ç‡ã‚¹ã‚³ã‚¢: {result.best_location['efficiency_score']:.1f}ä»¶/å°")
    
    print(f"\nğŸ“ å€™è£œåœ°ç‚¹ä¸€è¦§:")
    for i, cand in enumerate(result.candidate_locations[:5], 1):
        print(f"   {i}. ({cand['lat']:.4f}, {cand['lon']:.4f}) - {cand['reason']}")
    
    # ãƒãƒƒãƒ—ä¿å­˜
    stations = load_stations()
    incidents = load_incident_locations()
    candidates = [CandidateLocation(**c) for c in result.candidate_locations]
    
    m = create_optimization_map(stations, incidents, candidates, result.best_location)
    output_path = Path("optimization_result.html")
    m.save(str(output_path))
    print(f"\nâœ… ãƒãƒƒãƒ—ä¿å­˜: {output_path.resolve()}")


if __name__ == "__main__":
    main()
