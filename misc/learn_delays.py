"""Lightweight delay pattern learning script (no OSMnx dependency)."""

import pandas as pd
import numpy as np
from pathlib import Path
import json

# Constants
DELAY_FACTORS_PATH = Path(__file__).parent.parent / "cache" / "delay_factors.json"


def learn_delay_patterns_from_incidents(
    df: pd.DataFrame,
    time_col: str = "è¦šçŸ¥",
    arrival_time_col: str = "è¦šçŸ¥ï¼ç¾å ´åˆ°ç€",
    distance_col: str = "å‡ºå‹•ï¼ç¾å ´",
    baseline_hour: int = 3,
) -> dict:
    """Learn delay patterns from incident data.
    
    Args:
        df: DataFrame with incident data
        time_col: Column name for incident time
        arrival_time_col: Column name for arrival time (minutes)
        distance_col: Column name for distance (km)
        baseline_hour: Hour to use as baseline (default 3am = lowest traffic)
    
    Returns:
        dict with 'hourly', 'dow', 'matrix' delay factors
    """
    df = df.copy()
    
    # Parse datetime
    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
    df = df[df[time_col].notna()].copy()
    
    df['hour'] = df[time_col].dt.hour
    df['dow'] = df[time_col].dt.dayofweek
    
    # Calculate speed proxy: arrival_time / distance (min/km)
    # Higher value = slower = more traffic
    df['min_per_km'] = np.where(
        df[distance_col] > 0,
        df[arrival_time_col] / df[distance_col],
        np.nan
    )
    
    # Remove outliers (e.g., very short distances or very long times)
    df = df[(df['min_per_km'] > 0.5) & (df['min_per_km'] < 30)]
    
    # Hourly patterns
    hourly_speed = df.groupby('hour')['min_per_km'].mean()
    baseline_speed = hourly_speed.get(baseline_hour, hourly_speed.mean())
    hourly_factors = (hourly_speed / baseline_speed).to_dict()
    
    # Day of week patterns
    dow_speed = df.groupby('dow')['min_per_km'].mean()
    baseline_dow = dow_speed.mean()
    dow_factors = (dow_speed / baseline_dow).to_dict()
    
    # Hour x DOW matrix
    matrix_speed = df.groupby(['hour', 'dow'])['min_per_km'].mean().unstack()
    baseline_matrix = matrix_speed.loc[baseline_hour].mean() if baseline_hour in matrix_speed.index else matrix_speed.values.mean()
    matrix_factors = {}
    for hour in range(24):
        for dow in range(7):
            if hour in matrix_speed.index and dow in matrix_speed.columns:
                val = matrix_speed.loc[hour, dow]
                if pd.notna(val):
                    matrix_factors[f"{hour}_{dow}"] = round(val / baseline_matrix, 3)
    
    # Round values
    hourly_factors = {k: round(v, 3) for k, v in hourly_factors.items()}
    dow_factors = {k: round(v, 3) for k, v in dow_factors.items()}
    
    return {
        "hourly": hourly_factors,
        "dow": dow_factors,
        "matrix": matrix_factors,
    }


def save_delay_factors(hourly: dict, dow: dict = None, matrix: dict = None):
    """Save delay factors to JSON file."""
    data = {
        "hourly": {int(k): v for k, v in hourly.items()},
        "dow": {int(k): v for k, v in (dow or {}).items()},
        "matrix": matrix or {},
        "source": "learned_from_R6",
    }
    DELAY_FACTORS_PATH.parent.mkdir(exist_ok=True)
    with open(DELAY_FACTORS_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def main():
    cache_path = Path(__file__).parent.parent / 'cache' / 'r6_delay_analysis.pkl'
    excel_path = Path(__file__).parent.parent / 'R6.xlsx'
    
    if cache_path.exists():
        print("ğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
        df = pd.read_pickle(cache_path)
    else:
        print("ğŸ“Š Excelèª­ã¿è¾¼ã¿ä¸­ï¼ˆåˆå›ã®ã¿æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰...")
        usecols = ['è¦šçŸ¥', 'è¦šçŸ¥ï¼ç¾å ´åˆ°ç€', 'å‡ºå‹•ï¼ç¾å ´', 'æ›œæ—¥', 'å‡ºå‹•å ´æ‰€', 'å‡ºå‹•éšŠ']
        df = pd.read_excel(excel_path, usecols=usecols)
        cache_path.parent.mkdir(exist_ok=True)
        df.to_pickle(cache_path)
        print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†")
    
    print(f"\nèª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}ä»¶")
    
    # Learn patterns
    print("\nğŸ”¬ é…å»¶ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ä¸­...")
    patterns = learn_delay_patterns_from_incidents(
        df,
        time_col="è¦šçŸ¥",
        arrival_time_col="è¦šçŸ¥ï¼ç¾å ´åˆ°ç€",
        distance_col="å‡ºå‹•ï¼ç¾å ´",
        baseline_hour=3,
    )
    
    # Save
    save_delay_factors(
        hourly=patterns["hourly"],
        dow=patterns["dow"],
        matrix=patterns["matrix"],
    )
    print("âœ… é…å»¶ä¿‚æ•°ã‚’ cache/delay_factors.json ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # Display results
    print("\n" + "="*50)
    print("ğŸ“ˆ æ™‚é–“å¸¯åˆ¥ é…å»¶ä¿‚æ•°ï¼ˆæ·±å¤œ3æ™‚=1.0åŸºæº–ï¼‰")
    print("="*50)
    for h in range(24):
        factor = patterns["hourly"].get(h, 1.0)
        bar = "â–ˆ" * int(factor * 10)
        label = "ğŸ”´" if factor > 1.2 else "ğŸŸ¡" if factor > 1.1 else "ğŸŸ¢"
        print(f"{h:02d}æ™‚: {factor:.3f} {label} {bar}")
    
    print("\n" + "="*50)
    print("ğŸ“… æ›œæ—¥åˆ¥ é…å»¶ä¿‚æ•°")
    print("="*50)
    days = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
    for d in range(7):
        factor = patterns["dow"].get(d, 1.0)
        bar = "â–ˆ" * int(factor * 10)
        print(f"{days[d]}: {factor:.3f} {bar}")
    
    # Summary statistics
    df['è¦šçŸ¥'] = pd.to_datetime(df['è¦šçŸ¥'], errors='coerce')
    df = df[df['è¦šçŸ¥'].notna()].copy()
    df['hour'] = df['è¦šçŸ¥'].dt.hour
    
    print("\n" + "="*50)
    print("ğŸ“Š æ™‚é–“å¸¯åˆ¥ å¹³å‡ç¾ç€æ™‚é–“ï¼ˆåˆ†ï¼‰")
    print("="*50)
    hourly_arrival = df.groupby('hour')['è¦šçŸ¥ï¼ç¾å ´åˆ°ç€'].agg(['mean', 'count'])
    for h in range(24):
        if h in hourly_arrival.index:
            mean = hourly_arrival.loc[h, 'mean']
            count = hourly_arrival.loc[h, 'count']
            print(f"{h:02d}æ™‚: {mean:.1f}åˆ† (n={count:,})")


if __name__ == '__main__':
    main()
