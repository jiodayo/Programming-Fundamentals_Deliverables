"""ãƒªã‚½ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ã®ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

æ©Ÿèƒ½:
- å„åœ°ç‚¹ã‹ã‚‰ã€Œnåˆ†ä»¥å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã€ã‚’è¨ˆç®—
- ã‚«ãƒãƒ¬ãƒƒã‚¸å“è³ªã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆ
- ãƒªã‚½ãƒ¼ã‚¹é…ç½®ã®æœ€é©åŒ–ææ¡ˆ
"""

from __future__ import annotations

import json
import pickle
import sqlite3
from pathlib import Path
from typing import Optional, Tuple, List, Dict

import geopandas as gpd
import networkx as nx
import numpy as np
import osmnx as ox
import pandas as pd
import folium
from shapely.geometry import Point, box

# å„æ¶ˆé˜²ç½²ã®æ•‘æ€¥è»Šå°æ•°ï¼ˆR6ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
STATION_RESOURCES = {
    "æ±æ¶ˆé˜²ç½²": 4,
    "ä¸­å¤®æ¶ˆé˜²ç½²": 3,
    "è¥¿æ¶ˆé˜²ç½²": 3,
    "å—æ¶ˆé˜²ç½²": 3,
    "åŸåŒ—æ”¯ç½²": 1,  # æ”¯ç½²ã¯1å°æƒ³å®š
    "åŸæ±æ”¯ç½²": 1,
    "è¥¿éƒ¨æ”¯ç½²": 1,
    "æ±éƒ¨æ”¯ç½²": 1,
    "åŒ—æ¡æ”¯ç½²": 1,
    "æ¹¯å±±å‡ºå¼µæ‰€": 1,
    "ä¹…è°·å‡ºå¼µæ‰€": 1,
    "æ¶ˆé˜²å±€": 1,  # æ¶ˆé˜²å±€ã‚‚1å°ã¨ã—ã¦è¨ˆç®—
    "WS": 1,  # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
}

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå°æ•°ï¼ˆä¸æ˜ãªç½²ã®å ´åˆï¼‰
DEFAULT_AMBULANCES = 1

CACHE_DIR = Path(__file__).parent.parent / "cache"
GRAPH_PATH = CACHE_DIR / "matsuyama_drive.graphml"  # ã¾ãŸã¯ ehime_drive.graphml


def load_stations() -> gpd.GeoDataFrame:
    """æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    db_path = Path(__file__).parent.parent / "map.sqlite"
    with sqlite3.connect(db_path) as conn:
        df = pd.read_sql("SELECT * FROM stations", conn)
    
    # ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
    df["æ•‘æ€¥è»Šå°æ•°"] = df["ç•¥ç§°"].map(STATION_RESOURCES).fillna(DEFAULT_AMBULANCES).astype(int)
    
    geometry = [Point(lon, lat) for lon, lat in zip(df["çµŒåº¦"], df["ç·¯åº¦"])]
    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")
    return gdf


def load_graph() -> nx.MultiDiGraph:
    """é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•ã‚’èª­ã¿è¾¼ã¿"""
    if GRAPH_PATH.exists():
        return ox.load_graphml(GRAPH_PATH)
    raise FileNotFoundError(f"Graph not found: {GRAPH_PATH}")


def generate_grid_points(
    bounds: tuple[float, float, float, float],
    resolution_km: float = 1.0,
) -> gpd.GeoDataFrame:
    """åˆ†æç”¨ã®ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ç”Ÿæˆ
    
    Args:
        bounds: (min_lon, min_lat, max_lon, max_lat)
        resolution_km: ã‚°ãƒªãƒƒãƒ‰é–“éš”ï¼ˆkmï¼‰
    """
    min_lon, min_lat, max_lon, max_lat = bounds
    
    # ç·¯åº¦çµŒåº¦ã‚’kmã«å¤‰æ›ï¼ˆãŠãŠã‚ˆãï¼‰
    lat_step = resolution_km / 111.0  # 1åº¦ â‰ˆ 111km
    lon_step = resolution_km / (111.0 * np.cos(np.radians((min_lat + max_lat) / 2)))
    
    lats = np.arange(min_lat, max_lat, lat_step)
    lons = np.arange(min_lon, max_lon, lon_step)
    
    points = []
    for lat in lats:
        for lon in lons:
            points.append({"lat": lat, "lon": lon, "geometry": Point(lon, lat)})
    
    return gpd.GeoDataFrame(points, crs="EPSG:4326")


def compute_travel_times_from_stations(
    graph: nx.MultiDiGraph,
    stations: gpd.GeoDataFrame,
    grid: gpd.GeoDataFrame,
    max_time_sec: float = 600,  # 10åˆ†
) -> pd.DataFrame:
    """å„æ¶ˆé˜²ç½²ã‹ã‚‰å„ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®åˆ°é”æ™‚é–“ã‚’è¨ˆç®—
    
    Returns:
        DataFrame with columns: grid_idx, station_name, travel_time_sec, ambulances
    """
    # æ¶ˆé˜²ç½²ã®æœ€å¯„ã‚Šãƒãƒ¼ãƒ‰
    station_nodes = ox.distance.nearest_nodes(
        graph, 
        stations["çµŒåº¦"].tolist(), 
        stations["ç·¯åº¦"].tolist()
    )
    
    # ã‚°ãƒªãƒƒãƒ‰ã®æœ€å¯„ã‚Šãƒãƒ¼ãƒ‰
    grid_nodes = ox.distance.nearest_nodes(
        graph,
        grid["lon"].tolist(),
        grid["lat"].tolist()
    )
    
    results = []
    
    for i, (_, station) in enumerate(stations.iterrows()):
        print(f"  è¨ˆç®—ä¸­: {station['ç•¥ç§°']} ({i+1}/{len(stations)})")
        station_node = station_nodes[i]
        
        # æ¶ˆé˜²ç½²ã‹ã‚‰ã®æœ€çŸ­çµŒè·¯ï¼ˆé€†æ–¹å‘ãªã®ã§æ³¨æ„ï¼‰
        # æ•‘æ€¥è»Šã¯ã€Œæ¶ˆé˜²ç½²ã‹ã‚‰ç¾å ´ã¸ã€å‘ã‹ã†ã®ã§ã€æ¶ˆé˜²ç½²ã‚’èµ·ç‚¹ã¨ã—ãŸæœ€çŸ­çµŒè·¯
        try:
            lengths = nx.single_source_dijkstra_path_length(
                graph,
                station_node,
                cutoff=max_time_sec,
                weight="travel_time",
            )
        except Exception:
            continue
        
        for j, grid_node in enumerate(grid_nodes):
            if grid_node in lengths:
                travel_time = lengths[grid_node]
                results.append({
                    "grid_idx": j,
                    "station_name": station["ç•¥ç§°"],
                    "travel_time_sec": travel_time,
                    "ambulances": station["æ•‘æ€¥è»Šå°æ•°"],
                })
    
    return pd.DataFrame(results)


def compute_coverage_quality(
    travel_times: pd.DataFrame,
    grid: gpd.GeoDataFrame,
    time_thresholds: list[int] = [5, 8, 10],  # åˆ†
) -> gpd.GeoDataFrame:
    """å„ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ã‚«ãƒãƒ¬ãƒƒã‚¸å“è³ªã‚’è¨ˆç®—
    
    ã‚«ãƒãƒ¬ãƒƒã‚¸å“è³ª = nåˆ†ä»¥å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šã®åˆè¨ˆå°æ•°
    """
    grid = grid.copy()
    
    for threshold_min in time_thresholds:
        threshold_sec = threshold_min * 60
        col_name = f"ambulances_{threshold_min}min"
        
        # å„ã‚°ãƒªãƒƒãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã€é–¾å€¤ä»¥å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã‚’é›†è¨ˆ
        reachable = travel_times[travel_times["travel_time_sec"] <= threshold_sec]
        
        # åŒã˜ç½²ã‹ã‚‰ã®é‡è¤‡ã‚’æ’é™¤ã—ã€å°æ•°ã‚’åˆè¨ˆ
        ambulance_counts = (
            reachable.groupby("grid_idx")
            .apply(lambda x: x.drop_duplicates("station_name")["ambulances"].sum())
        )
        
        grid[col_name] = grid.index.map(ambulance_counts).fillna(0).astype(int)
    
    return grid


def compute_optimization_suggestions(
    grid: gpd.GeoDataFrame,
    stations: gpd.GeoDataFrame,
    travel_times: pd.DataFrame,
    target_threshold_min: int = 8,
) -> dict:
    """ãƒªã‚½ãƒ¼ã‚¹é…ç½®ã®æœ€é©åŒ–ææ¡ˆã‚’ç”Ÿæˆ
    
    Returns:
        dict with:
        - current_stats: ç¾çŠ¶ã®çµ±è¨ˆ
        - weak_areas: ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒå¼±ã„ã‚¨ãƒªã‚¢
        - suggestions: æ”¹å–„ææ¡ˆ
    """
    col = f"ambulances_{target_threshold_min}min"
    
    # ç¾çŠ¶çµ±è¨ˆ
    current_stats = {
        "total_grid_points": len(grid),
        "zero_coverage": int((grid[col] == 0).sum()),
        "single_coverage": int((grid[col] == 1).sum()),
        "multi_coverage": int((grid[col] >= 2).sum()),
        "avg_ambulances": float(grid[col].mean()),
        "min_ambulances": int(grid[col].min()),
        "max_ambulances": int(grid[col].max()),
    }
    
    # ã‚«ãƒãƒ¬ãƒƒã‚¸0ã®ã‚¨ãƒªã‚¢
    zero_coverage_points = grid[grid[col] == 0].copy()
    
    # ã‚«ãƒãƒ¬ãƒƒã‚¸1ã®ã‚¨ãƒªã‚¢ï¼ˆå†—é•·æ€§ãŒãªã„ï¼‰
    single_coverage_points = grid[grid[col] == 1].copy()
    
    # å¼±ç‚¹ã‚¨ãƒªã‚¢ã®ä¸­å¿ƒã‚’è¨ˆç®—
    weak_areas = []
    if len(zero_coverage_points) > 0:
        weak_areas.append({
            "type": "ã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—",
            "count": len(zero_coverage_points),
            "center_lat": zero_coverage_points["lat"].mean(),
            "center_lon": zero_coverage_points["lon"].mean(),
            "severity": "é«˜",
        })
    
    if len(single_coverage_points) > 0:
        weak_areas.append({
            "type": "å†—é•·æ€§ãªã—ï¼ˆ1å°ã®ã¿ï¼‰",
            "count": len(single_coverage_points),
            "center_lat": single_coverage_points["lat"].mean(),
            "center_lon": single_coverage_points["lon"].mean(),
            "severity": "ä¸­",
        })
    
    # å„æ¶ˆé˜²ç½²ã®å¢—å¼·åŠ¹æœã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    suggestions = []
    for _, station in stations.iterrows():
        station_name = station["ç•¥ç§°"]
        
        # ã“ã®ç½²ã‹ã‚‰åˆ°é”å¯èƒ½ãªãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—
        station_times = travel_times[travel_times["station_name"] == station_name]
        reachable_in_threshold = station_times[
            station_times["travel_time_sec"] <= target_threshold_min * 60
        ]["grid_idx"].unique()
        
        # ç¾åœ¨ã‚«ãƒãƒ¬ãƒƒã‚¸0ã®ãƒã‚¤ãƒ³ãƒˆã§ã“ã®ç½²ãŒæ•‘ãˆã‚‹æ•°
        zero_points_idx = set(grid[grid[col] == 0].index)
        newly_covered = len(zero_points_idx & set(reachable_in_threshold))
        
        # ç¾åœ¨ã‚«ãƒãƒ¬ãƒƒã‚¸1ã®ãƒã‚¤ãƒ³ãƒˆã§å†—é•·æ€§ã‚’è¿½åŠ ã§ãã‚‹æ•°
        single_points_idx = set(grid[grid[col] == 1].index)
        redundancy_added = len(single_points_idx & set(reachable_in_threshold))
        
        suggestions.append({
            "station_name": station_name,
            "current_ambulances": int(station["æ•‘æ€¥è»Šå°æ•°"]),
            "newly_covered_points": newly_covered,
            "redundancy_improved_points": redundancy_added,
            "total_improvement": newly_covered * 2 + redundancy_added,  # é‡ã¿ä»˜ã
        })
    
    # æ”¹å–„åŠ¹æœãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    suggestions = sorted(suggestions, key=lambda x: x["total_improvement"], reverse=True)
    
    return {
        "current_stats": current_stats,
        "weak_areas": weak_areas,
        "suggestions": suggestions[:5],  # ä¸Šä½5ã¤
    }


def save_coverage_cache(
    grid: gpd.GeoDataFrame,
    travel_times: pd.DataFrame,
    cache_name: str = "coverage_analysis",
):
    """åˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    CACHE_DIR.mkdir(exist_ok=True)
    
    grid.to_pickle(CACHE_DIR / f"{cache_name}_grid.pkl")
    travel_times.to_pickle(CACHE_DIR / f"{cache_name}_times.pkl")
    print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜: {CACHE_DIR / cache_name}_*.pkl")


def load_coverage_cache(
    cache_name: str = "coverage_analysis",
) -> tuple[gpd.GeoDataFrame, pd.DataFrame] | None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿"""
    grid_path = CACHE_DIR / f"{cache_name}_grid.pkl"
    times_path = CACHE_DIR / f"{cache_name}_times.pkl"
    
    if grid_path.exists() and times_path.exists():
        try:
            grid = pd.read_pickle(grid_path)
            travel_times = pd.read_pickle(times_path)
            return grid, travel_times
        except (ModuleNotFoundError, ImportError, pickle.UnpicklingError) as e:
            # NumPyãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆãªã©ã§pickleèª­ã¿è¾¼ã¿å¤±æ•—
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆã®å¯èƒ½æ€§ï¼‰: {e}")
            print("  â†’ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„: python3 misc/coverage_analysis.py")
            return None
        except Exception as e:
            print(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    return None


def create_coverage_map(
    grid: gpd.GeoDataFrame,
    stations: gpd.GeoDataFrame,
    threshold_min: int = 8,
) -> "folium.Map":
    """ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ
    
    Args:
        grid: ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—æ¸ˆã¿ã®ã‚°ãƒªãƒƒãƒ‰
        stations: æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿
        threshold_min: è¡¨ç¤ºã™ã‚‹é–¾å€¤ï¼ˆåˆ†ï¼‰
    """
    import folium
    from folium.plugins import HeatMap
    
    col = f"ambulances_{threshold_min}min"
    
    # åœ°å›³ã®ä¸­å¿ƒ
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    
    m = folium.Map(
        location=[center_lat, center_lon],
        zoom_start=11,
        tiles="cartodbpositron",
    )
    
    # ã‚«ãƒãƒ¬ãƒƒã‚¸ã«å¿œã˜ãŸè‰²åˆ†ã‘
    # 0å°: èµ¤, 1å°: ã‚ªãƒ¬ãƒ³ã‚¸, 2å°: é»„, 3å°ä»¥ä¸Š: ç·‘
    colors = {0: "red", 1: "orange", 2: "yellow"}
    
    for _, row in grid.iterrows():
        ambulances = row[col]
        if ambulances == 0:
            color = "red"
            opacity = 0.7
        elif ambulances == 1:
            color = "orange"
            opacity = 0.5
        elif ambulances == 2:
            color = "yellow"
            opacity = 0.4
        else:
            color = "green"
            opacity = 0.3
        
        folium.CircleMarker(
            location=[row["lat"], row["lon"]],
            radius=5,
            color=color,
            fill=True,
            fill_color=color,
            fill_opacity=opacity,
            opacity=opacity,
            popup=f"æ•‘æ€¥è»Š: {ambulances}å°",
        ).add_to(m)
    
    # æ¶ˆé˜²ç½²ãƒãƒ¼ã‚«ãƒ¼
    for _, station in stations.iterrows():
        folium.Marker(
            location=[station["ç·¯åº¦"], station["çµŒåº¦"]],
            popup=f"{station['ç•¥ç§°']} ({station['æ•‘æ€¥è»Šå°æ•°']}å°)",
            icon=folium.Icon(color="blue", icon="plus", prefix="fa"),
        ).add_to(m)
    
    # å‡¡ä¾‹
    legend_html = f"""
    <div style="position: fixed; bottom: 50px; left: 50px; z-index: 1000; 
                background: white; padding: 10px; border-radius: 5px;
                border: 2px solid gray; font-size: 12px;">
        <b>{threshold_min}åˆ†ä»¥å†…åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°</b><br>
        <span style="color: red;">â—</span> 0å°ï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—ï¼‰<br>
        <span style="color: orange;">â—</span> 1å°ï¼ˆå†—é•·æ€§ãªã—ï¼‰<br>
        <span style="color: yellow;">â—</span> 2å°<br>
        <span style="color: green;">â—</span> 3å°ä»¥ä¸Š
    </div>
    """
    m.get_root().html.add_child(folium.Element(legend_html))
    
    return m


def main():
    print("ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
    print("=" * 50)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\nğŸ“ æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    stations = load_stations()
    print(f"   {len(stations)}ç½²")
    print("\n   æ•‘æ€¥è»Šé…ç½®:")
    for _, s in stations.iterrows():
        print(f"   - {s['ç•¥ç§°']}: {s['æ•‘æ€¥è»Šå°æ•°']}å°")
    print(f"   åˆè¨ˆ: {stations['æ•‘æ€¥è»Šå°æ•°'].sum()}å°")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    cache = load_coverage_cache()
    if cache:
        print("\nğŸ“¦ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰èª­ã¿è¾¼ã¿...")
        grid, travel_times = cache
    else:
        print("\nğŸ—ºï¸ é“è·¯ã‚°ãƒ©ãƒ•èª­ã¿è¾¼ã¿...")
        graph = load_graph()
        
        # ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ
        print("\nğŸ“Š åˆ†æã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ...")
        bounds = (
            stations["çµŒåº¦"].min() - 0.05,
            stations["ç·¯åº¦"].min() - 0.05,
            stations["çµŒåº¦"].max() + 0.05,
            stations["ç·¯åº¦"].max() + 0.05,
        )
        grid = generate_grid_points(bounds, resolution_km=0.5)
        print(f"   {len(grid)}ãƒã‚¤ãƒ³ãƒˆ")
        
        # åˆ°é”æ™‚é–“è¨ˆç®—
        print("\nâ±ï¸ åˆ°é”æ™‚é–“è¨ˆç®—...")
        travel_times = compute_travel_times_from_stations(
            graph, stations, grid, max_time_sec=900  # 15åˆ†
        )
        print(f"   {len(travel_times)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        save_coverage_cache(grid, travel_times)
    
    # ã‚«ãƒãƒ¬ãƒƒã‚¸å“è³ªè¨ˆç®—
    print("\nğŸ“ˆ ã‚«ãƒãƒ¬ãƒƒã‚¸å“è³ªè¨ˆç®—...")
    grid = compute_coverage_quality(travel_times, grid, [5, 8, 10])
    
    # çµæœè¡¨ç¤º
    for threshold in [5, 8, 10]:
        col = f"ambulances_{threshold}min"
        print(f"\n   ã€{threshold}åˆ†ä»¥å†…åˆ°é”ã€‘")
        print(f"   - 0å°: {(grid[col] == 0).sum()}ãƒã‚¤ãƒ³ãƒˆ")
        print(f"   - 1å°: {(grid[col] == 1).sum()}ãƒã‚¤ãƒ³ãƒˆ")
        print(f"   - 2å°ä»¥ä¸Š: {(grid[col] >= 2).sum()}ãƒã‚¤ãƒ³ãƒˆ")
        print(f"   - å¹³å‡: {grid[col].mean():.2f}å°")
    
    # æœ€é©åŒ–ææ¡ˆ
    print("\n" + "=" * 50)
    print("ğŸ¯ ãƒªã‚½ãƒ¼ã‚¹é…ç½® æœ€é©åŒ–ææ¡ˆ")
    print("=" * 50)
    
    suggestions = compute_optimization_suggestions(grid, stations, travel_times, target_threshold_min=8)
    
    stats = suggestions["current_stats"]
    print(f"\nğŸ“Š ç¾çŠ¶ï¼ˆ8åˆ†åœï¼‰:")
    print(f"   - ã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—: {stats['zero_coverage']}ãƒã‚¤ãƒ³ãƒˆ ({stats['zero_coverage']/stats['total_grid_points']*100:.1f}%)")
    print(f"   - 1å°ã®ã¿: {stats['single_coverage']}ãƒã‚¤ãƒ³ãƒˆ ({stats['single_coverage']/stats['total_grid_points']*100:.1f}%)")
    print(f"   - 2å°ä»¥ä¸Š: {stats['multi_coverage']}ãƒã‚¤ãƒ³ãƒˆ ({stats['multi_coverage']/stats['total_grid_points']*100:.1f}%)")
    
    if suggestions["weak_areas"]:
        print(f"\nâš ï¸ å¼±ç‚¹ã‚¨ãƒªã‚¢:")
        for area in suggestions["weak_areas"]:
            print(f"   - {area['type']}: {area['count']}ãƒã‚¤ãƒ³ãƒˆï¼ˆä¸­å¿ƒ: {area['center_lat']:.4f}, {area['center_lon']:.4f}ï¼‰")
    
    print(f"\nğŸ’¡ å¢—å¼·æ¨å¥¨ï¼ˆæ•‘æ€¥è»Š1å°è¿½åŠ æ™‚ã®åŠ¹æœï¼‰:")
    for i, s in enumerate(suggestions["suggestions"], 1):
        print(f"   {i}. {s['station_name']} (ç¾{s['current_ambulances']}å°)")
        print(f"      â†’ æ–°è¦ã‚«ãƒãƒ¼: {s['newly_covered_points']}pt, å†—é•·æ€§è¿½åŠ : {s['redundancy_improved_points']}pt")
    
    # çµæœã‚’JSONã§ä¿å­˜
    result = {
        "analysis_date": pd.Timestamp.now().isoformat(),
        "stats": stats,
        "weak_areas": suggestions["weak_areas"],
        "suggestions": suggestions["suggestions"],
    }
    with open(CACHE_DIR / "coverage_analysis_result.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… çµæœä¿å­˜: {CACHE_DIR / 'coverage_analysis_result.json'}")


if __name__ == "__main__":
    main()
