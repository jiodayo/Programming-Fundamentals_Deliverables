"""Streamlit app that visualizes EMS isochrones on a Google-like map.

$ streamlit run app.py ã§å®Ÿè¡Œ
"""

from __future__ import annotations

import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from pathlib import Path
from typing import Iterable, Callable
import re
import logging

# WebSocketClosedErrorã®ãƒ­ã‚°ã‚’æŠ‘åˆ¶ï¼ˆStreamlitå†æç”»æ™‚ã«ç™ºç”Ÿã™ã‚‹ç„¡å®³ãªã‚¨ãƒ©ãƒ¼ï¼‰
logging.getLogger("tornado.application").setLevel(logging.CRITICAL)

import folium
import geopandas as gpd
import networkx as nx
import osmnx as ox
import pandas as pd
import streamlit as st
from streamlit_folium import st_folium
from shapely.geometry import Point
from shapely.geometry import MultiPoint
from shapely.ops import unary_union

# Traffic-aware isochrones
from traffic_analysis import (
    load_delay_factors,
    get_delay_factor,
    TIME_SLOT_LABELS,
    DOW_LABELS,
    DELAY_FACTORS_PATH,
)

# ãƒªã‚½ãƒ¼ã‚¹ï¼ˆæ•‘æ€¥è»Šå°æ•°ï¼‰æƒ…å ± - R6ï¼ˆ2024å¹´ï¼‰: R4ã€œR6ã§3éšŠå¢—éšŠå¾Œ
STATION_RESOURCES_R6 = {
    "æ±æ¶ˆé˜²ç½²": 4,
    "ä¸­å¤®æ¶ˆé˜²ç½²": 3,
    "è¥¿æ¶ˆé˜²ç½²": 3,
    "å—æ¶ˆé˜²ç½²": 3,
    "åŸåŒ—æ”¯ç½²": 1,
    "åŸæ±æ”¯ç½²": 1,
    "è¥¿éƒ¨æ”¯ç½²": 1,
    "æ±éƒ¨æ”¯ç½²": 1,
    "åŒ—æ¡æ”¯ç½²": 1,
    "æ¹¯å±±å‡ºå¼µæ‰€": 1,
    "ä¹…è°·å‡ºå¼µæ‰€": 1,
    "æ¶ˆé˜²å±€": 1,
    "WS": 1,
}
# åˆè¨ˆ: 22å°

# ãƒªã‚½ãƒ¼ã‚¹ï¼ˆæ•‘æ€¥è»Šå°æ•°ï¼‰æƒ…å ± - H27ï¼ˆ2015å¹´ï¼‰: å¢—éšŠå‰ï¼ˆæ¨å®šï¼‰
# R4ã€œR6ã§3éšŠå¢—éšŠã€H25æ™‚ç‚¹ã§ã¯WSå¸¸é§éšŠãªã—
STATION_RESOURCES_H27_DEFAULT = {
    "æ±æ¶ˆé˜²ç½²": 3,  # æ¨å®š
    "ä¸­å¤®æ¶ˆé˜²ç½²": 2,  # æ¨å®š
    "è¥¿æ¶ˆé˜²ç½²": 2,  # æ¨å®š
    "å—æ¶ˆé˜²ç½²": 3,
    "åŸåŒ—æ”¯ç½²": 1,
    "åŸæ±æ”¯ç½²": 1,
    "è¥¿éƒ¨æ”¯ç½²": 1,
    "æ±éƒ¨æ”¯ç½²": 1,
    "åŒ—æ¡æ”¯ç½²": 1,
    "æ¹¯å±±å‡ºå¼µæ‰€": 1,
    "ä¹…è°·å‡ºå¼µæ‰€": 1,
    "æ¶ˆé˜²å±€": 1,
    "WS": 0,  # H25æ™‚ç‚¹ã§ã¯å¸¸é§éšŠãªã—
}
# åˆè¨ˆ: 18å°ï¼ˆR6ã‚ˆã‚Š4å°å°‘ãªã„ãƒ»æ¨å®šå€¤ï¼‰

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
STATION_RESOURCES = STATION_RESOURCES_R6
DEFAULT_AMBULANCES = 1

ox.settings.use_cache = True
GRAPHML_PATH = Path("cache/matsuyama_drive.graphml")
GRAPHML_PATH.parent.mkdir(parents=True, exist_ok=True)
STATIONS_DB_PATH = Path("map.sqlite")
INCIDENTS_DB_PATH = Path("incidents.sqlite")
ISOCHRONE_CACHE_PATH = Path("cache/isochrones.parquet")
GEOCODE_CACHE_PATH = Path("cache/incident_geocode.parquet")


def graph_data_version() -> float:
    """Return a timestamp that reflects the cached graph version."""
    return GRAPHML_PATH.stat().st_mtime if GRAPHML_PATH.exists() else 0.0


def station_data_version(db_path: Path = STATIONS_DB_PATH, excel_path: str = "map.xlsx") -> float:
    """Return mtime of the current station datasource for cache invalidation."""
    if db_path.exists():
        return db_path.stat().st_mtime
    return Path(excel_path).stat().st_mtime if Path(excel_path).exists() else 0.0


@st.cache_data(show_spinner=False)
def load_incident_data(excel_path: str = "R6.xlsx", db_path: Path = INCIDENTS_DB_PATH) -> pd.DataFrame:
    """Load R6 incident records from SQLite if available, otherwise from Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            # Check if table exists
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='incidents_r6'")
            if cursor.fetchone():
                df = pd.read_sql("SELECT * FROM incidents_r6", conn)
                df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
                df["date"] = pd.to_datetime(df["date"]).dt.date
                return df
    
    # Fallback to Excel
    if not Path(excel_path).exists():
        raise FileNotFoundError(excel_path)
    df = pd.read_excel(excel_path)
    df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
    df = df[df["è¦šçŸ¥"].notna()].copy()
    df["date"] = df["è¦šçŸ¥"].dt.date
    return df


@st.cache_data(show_spinner=False)
def load_incident_data_h27(excel_path: str = "H27.xls", db_path: Path = INCIDENTS_DB_PATH) -> pd.DataFrame:
    """Load H27 incident records from SQLite if available, otherwise from Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            # Check if table exists
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='incidents_h27'")
            if cursor.fetchone():
                df = pd.read_sql("SELECT * FROM incidents_h27", conn)
                df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
                df["date"] = pd.to_datetime(df["date"]).dt.date
                return df
    
    # Fallback to Excel
    if not Path(excel_path).exists():
        raise FileNotFoundError(excel_path)
    df = pd.read_excel(excel_path)
    
    # Build datetime from separate columns
    df["è¦šçŸ¥"] = pd.to_datetime(
        df["è¦šçŸ¥æ—¥ä»˜(å¹´)"].astype(str) + "-" +
        df["è¦šçŸ¥æ—¥ä»˜(æœˆ)"].astype(str).str.zfill(2) + "-" +
        df["è¦šçŸ¥æ—¥ä»˜(æ—¥)"].astype(str).str.zfill(2) + " " +
        df["è¦šçŸ¥æ™‚åˆ»(æ™‚)"].astype(str).str.zfill(2) + ":" +
        df["è¦šçŸ¥æ™‚åˆ»(åˆ†)"].astype(str).str.zfill(2) + ":" +
        df["è¦šçŸ¥æ™‚åˆ»(ç§’)"].fillna(0).astype(int).astype(str).str.zfill(2),
        errors="coerce"
    )
    df = df[df["è¦šçŸ¥"].notna()].copy()
    df["date"] = df["è¦šçŸ¥"].dt.date
    
    # Normalize column names to match R6 format
    df["å‡ºå‹•å ´æ‰€"] = df["å‡ºå ´å ´æ‰€-1"]
    df["å‡ºå‹•éšŠ"] = df["å‡ºå ´éšŠå"]
    df["æ›œæ—¥"] = df["è¦šçŸ¥æ›œæ—¥å"]
    
    return df


def _load_geocode_cache(path: Path = GEOCODE_CACHE_PATH) -> pd.DataFrame:
    if path.exists():
        try:
            return pd.read_parquet(path)
        except Exception:
            return pd.DataFrame(columns=["address", "lat", "lon"])
    return pd.DataFrame(columns=["address", "lat", "lon"])


def _save_geocode_cache(df: pd.DataFrame, path: Path = GEOCODE_CACHE_PATH) -> None:
    try:
        df.to_parquet(path, index=False)
    except Exception:
        pass


def geocode_addresses(addresses: list[str], region_prefix: str = "æ„›åª›çœŒ") -> pd.DataFrame:
    """Geocode addresses with osmnx + Nominatim and persist results locally."""
    cache = _load_geocode_cache().copy()
    seen = set(cache["address"].tolist())
    missing = [a for a in addresses if a not in seen]

    new_records: list[dict] = []
    for addr in missing:
        query = f"{region_prefix} {addr}" if region_prefix else addr
        try:
            lat, lon = ox.geocode(query)
            new_records.append({"address": addr, "lat": lat, "lon": lon})
        except Exception:
            new_records.append({"address": addr, "lat": None, "lon": None})

    if new_records:
        cache = pd.concat([cache, pd.DataFrame(new_records)], ignore_index=True)
        cache = cache.drop_duplicates(subset=["address"], keep="last")
        _save_geocode_cache(cache)

    return cache[cache["address"].isin(addresses)].copy()


@st.cache_data(show_spinner=False)
def load_station_data(
    db_path: Path,
    excel_path: str,
    source_mtime: float,
) -> gpd.GeoDataFrame:
    """Load station records from SQLite when available, otherwise fallback to Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            df = pd.read_sql("SELECT * FROM stations", conn)
    else:
        df = pd.read_excel(excel_path)

    geometry = gpd.points_from_xy(df["çµŒåº¦"], df["ç·¯åº¦"])
    return gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")


@st.cache_resource(show_spinner=False)
def load_graph_cached(bbox: tuple[float, float, float, float]) -> nx.MultiDiGraph:
    north, south, east, west = bbox
    if GRAPHML_PATH.exists():
        graph = ox.load_graphml(GRAPHML_PATH)
    else:
        print("é“è·¯ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...ï¼ˆåˆå›å–å¾—ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
        try:
            graph = ox.graph_from_bbox(bbox=bbox, network_type="drive")
        except ValueError as exc:
            if "no graph nodes" not in str(exc).lower():
                raise
            graph = ox.graph_from_place("Matsuyama, Ehime, Japan", network_type="drive")
        ox.save_graphml(graph, GRAPHML_PATH)

    if "travel_time" not in next(iter(graph.edges(data=True)))[2]:
        graph = ox.add_edge_speeds(graph, hwy_speeds={
            "residential": 30,
            "secondary": 40,
            "tertiary": 40,
            "primary": 50,
            "motorway": 80,
        })
        graph = ox.add_edge_travel_times(graph)
        ox.save_graphml(graph, GRAPHML_PATH)

    return graph


def compute_isochrones(
    graph: nx.MultiDiGraph,
    stations: gpd.GeoDataFrame,
    trip_times: Iterable[int],
    progress_cb: Callable[[float], None] | None = None,
    delay_factor: float = 1.0,
) -> gpd.GeoDataFrame:
    """Compute isochrones with optional traffic delay factor.
    
    Args:
        delay_factor: Multiplier for travel times (>1 means slower due to traffic)
    """
    records: list[dict] = []

    # Vectorized nearest-node lookup to avoid per-row KDTree rebuilds
    xs = stations["çµŒåº¦"].to_list()
    ys = stations["ç·¯åº¦"].to_list()
    try:
        center_nodes = ox.distance.nearest_nodes(graph, xs, ys)
    except Exception:
        # Fallback to per-point lookup if vectorized call fails
        center_nodes = [ox.distance.nearest_nodes(graph, x, y) for x, y in zip(xs, ys)]

    total = len(center_nodes)

    # Pre-extract node coordinates to avoid repeated attribute lookups
    node_xy = {n: (data["x"], data["y"]) for n, data in graph.nodes(data=True)}
    trip_times_sorted = sorted(trip_times)
    # Adjust max_radius by delay factor (if delay_factor > 1, effective range shrinks)
    max_radius = (trip_times_sorted[-1] * 60 / delay_factor) if trip_times_sorted else 0

    def _one_station(payload: tuple[int, tuple]) -> list[dict]:
        _idx, (row, center_node) = payload
        out: list[dict] = []

        # Single-source Dijkstra once up to the maximum requested time
        lengths = nx.single_source_dijkstra_path_length(
            graph,
            center_node,
            cutoff=max_radius,
            weight="travel_time",
        )

        for minutes in trip_times_sorted:
            # With delay_factor > 1, effective reach shrinks (slower travel)
            cutoff = minutes * 60 / delay_factor
            reachable_nodes = [nid for nid, dist in lengths.items() if dist <= cutoff]
            if not reachable_nodes:
                continue
            points = [node_xy[nid] for nid in reachable_nodes if nid in node_xy]
            if not points:
                continue
            hull = MultiPoint(points).convex_hull
            out.append({"name": row.ç•¥ç§°, "time": minutes, "geometry": hull})

        return out

    with ThreadPoolExecutor(max_workers=min(8, max(2, os.cpu_count() or 2))) as ex:  # type: ignore[name-defined]
        futures = [
            ex.submit(_one_station, payload)
            for payload in enumerate(zip(stations.itertuples(index=False), center_nodes))
        ]
        completed = 0
        for fut in as_completed(futures):
            records.extend(fut.result())
            completed += 1
            if progress_cb:
                progress_cb(completed / total)

    if not records:
        raise RuntimeError("åˆ°é”åœãƒãƒªã‚´ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    return gpd.GeoDataFrame(records, crs="EPSG:4326")


@st.cache_data(show_spinner=False)
def precompute_isochrones(
    station_df: pd.DataFrame,
    trip_times: tuple[int, ...],
    graph_version: float,
) -> gpd.GeoDataFrame:
    """Cache-intensive isochrone computation so UI updates stay responsive."""
    geometry = gpd.points_from_xy(station_df["çµŒåº¦"], station_df["ç·¯åº¦"])
    stations = gpd.GeoDataFrame(station_df, geometry=geometry, crs="EPSG:4326")

    graph = ox.load_graphml(GRAPHML_PATH)
    return compute_isochrones(graph, stations, trip_times)


@st.cache_data(show_spinner=False)
def load_precomputed_isochrones(path: Path) -> gpd.GeoDataFrame:
    if not path.exists():
        raise FileNotFoundError(path)
    return gpd.read_parquet(path)


def append_virtual_stations(base: gpd.GeoDataFrame, virtuals: list[dict]) -> gpd.GeoDataFrame:
    """Append in-session virtual stations (lat/lon) to the loaded stations."""
    if not virtuals:
        return base

    df_new = pd.DataFrame(virtuals)
    geom = gpd.points_from_xy(df_new["çµŒåº¦"], df_new["ç·¯åº¦"])
    gdf_new = gpd.GeoDataFrame(df_new, geometry=geom, crs="EPSG:4326")

    # Ensure all columns exist and order matches base
    for col in base.columns:
        if col not in gdf_new.columns:
            gdf_new[col] = None
    gdf_new = gdf_new[base.columns]

    return pd.concat([base, gdf_new], ignore_index=True)


def create_location_picker_map(
    stations: gpd.GeoDataFrame,
    virtual_stations: list[dict] | None = None,
) -> folium.Map:
    """Create an interactive map for picking locations to add virtual stations."""
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles="CartoDB Positron")

    # Show existing stations
    for _, row in stations.iterrows():
        folium.CircleMarker(
            location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
            radius=7,
            color="#1f1f1f",
            weight=2,
            fill=True,
            fill_color="#f6bd60",
            fill_opacity=0.9,
            popup=f"{row['ç•¥ç§°']}",
            tooltip=f"æ—¢å­˜: {row['ç•¥ç§°']}",
        ).add_to(fmap)

    # Show virtual stations (if any)
    if virtual_stations:
        for vs in virtual_stations:
            folium.CircleMarker(
                location=[vs["ç·¯åº¦"], vs["çµŒåº¦"]],
                radius=8,
                color="#e63946",
                weight=2,
                fill=True,
                fill_color="#e63946",
                fill_opacity=0.9,
                popup=f"ä»®æƒ³: {vs['ç•¥ç§°']}",
                tooltip=f"ä»®æƒ³: {vs['ç•¥ç§°']}",
            ).add_to(fmap)

    return fmap


def render_map_html(
    isochrones: gpd.GeoDataFrame,
    stations: gpd.GeoDataFrame,
    tiles: str = "CartoDB Positron",
) -> str:
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles=tiles)

    # Softer palette for better visibility when overlapping
    color_map = {5: "#ff9e9e", 10: "#8aa5ff", 15: "#7dd8c6", 20: "#f7caa0"}
    for minutes in sorted({*isochrones["time"]}):
        layer = isochrones[isochrones["time"] == minutes]
        if layer.empty:
            continue
        color = color_map.get(minutes, "#4a4a4a")
        folium.GeoJson(
            data=layer.__geo_interface__,
            name=f"{minutes}åˆ†åœ",
            style_function=lambda _feature, c=color, m=minutes: {
                "fillColor": c,
                "color": c,
                "weight": 1.0,
                "opacity": 0.6,
                "fillOpacity": 0.18 if m >= 10 else 0.30,
            },
            tooltip=folium.GeoJsonTooltip(fields=["name", "time"], aliases=["æ‹ ç‚¹", "åˆ°é”æ™‚é–“(åˆ†)"])
        ).add_to(fmap)

    for _, row in stations.iterrows():
        folium.CircleMarker(
            location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
            radius=7,
            color="#1f1f1f",
            weight=2,
            fill=True,
            fill_color="#f6bd60",
            fill_opacity=0.9,
            popup=f"{row['ç•¥ç§°']}",
        ).add_to(fmap)

    folium.LayerControl(collapsed=False).add_to(fmap)
    return fmap.get_root().render()


def main() -> None:
    st.set_page_config(page_title="æ„›åª›æ•‘æ€¥è»Š åˆ°é”åœãƒ“ãƒ¥ãƒ¼ã‚¢", layout="wide")
    st.title("ğŸš‘ æ„›åª›çœŒ æ•‘æ€¥è»Šãƒ“ãƒ¥ãƒ¼ã‚¢")
    st.caption("map.xlsx ã§æ‹ ç‚¹åˆ°é”åœã€R6.xlsx ã§å‡ºå‹•åœ°ç‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    stations = load_station_data(
        db_path=STATIONS_DB_PATH,
        excel_path="map.xlsx",
        source_mtime=station_data_version(),
    )

    if "virtual_stations" not in st.session_state:
        st.session_state["virtual_stations"] = []

    tab_summary, tab_iso, tab_inc, tab_coverage, tab_resource, tab_optimize = st.tabs([
        "ğŸ“Š ã‚µãƒãƒªãƒ¼", "åˆ°é”åœ", "å‡ºå‹•åœ°ç‚¹ (R6)", "ã‚«ãƒãƒ¼ç‡åˆ†æ", "ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹åˆ†æ", "â­ é…ç½®æœ€é©åŒ–"
    ])

    # ========== ã‚¿ãƒ–0: ã‚µãƒãƒªãƒ¼ ==========
    with tab_summary:
        st.header("ğŸ“Š æ¾å±±å¸‚ æ•‘æ€¥æ¬é€ãƒ‡ãƒ¼ã‚¿ ã‚µãƒãƒªãƒ¼")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        @st.cache_data
        def load_summary_data():
            summary = {}
            
            # æ¶ˆé˜²ç½²æ•°
            if os.path.exists("map.sqlite"):
                conn = sqlite3.connect("map.sqlite")
                stations = pd.read_sql_query("SELECT * FROM stations", conn)
                conn.close()
                summary["stations_count"] = len(stations)
            else:
                summary["stations_count"] = 0
            
            # R6å‡ºå‹•ãƒ‡ãƒ¼ã‚¿
            if os.path.exists("incidents.sqlite"):
                conn = sqlite3.connect("incidents.sqlite")
                try:
                    r6_df = pd.read_sql_query("SELECT * FROM incidents_r6", conn)
                    summary["r6_total"] = len(r6_df)
                    if "å‡ºå‹•æ—¥" in r6_df.columns:
                        r6_df["å‡ºå‹•æ—¥"] = pd.to_datetime(r6_df["å‡ºå‹•æ—¥"], errors="coerce")
                        summary["r6_days"] = r6_df["å‡ºå‹•æ—¥"].nunique()
                    else:
                        summary["r6_days"] = 0
                except:
                    summary["r6_total"] = 0
                    summary["r6_days"] = 0
                
                try:
                    h27_df = pd.read_sql_query("SELECT * FROM incidents_h27", conn)
                    summary["h27_total"] = len(h27_df)
                except:
                    summary["h27_total"] = 0
                conn.close()
            else:
                summary["r6_total"] = 0
                summary["h27_total"] = 0
                summary["r6_days"] = 0
            
            # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            if os.path.exists("cache/incident_geocode.parquet"):
                geo_df = pd.read_parquet("cache/incident_geocode.parquet")
                summary["geocoded"] = len(geo_df)
            else:
                summary["geocoded"] = 0
            
            return summary
        
        summary = load_summary_data()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰
        st.markdown("### ğŸ“ˆ åŸºæœ¬çµ±è¨ˆ")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ¢ æ¶ˆé˜²ç½²æ•°", f"{summary['stations_count']} ç½²")
        with col2:
            st.metric("ğŸš‘ R6 å‡ºå‹•ä»¶æ•°", f"{summary['r6_total']:,} ä»¶")
        with col3:
            st.metric("ğŸ“… R6 ãƒ‡ãƒ¼ã‚¿æ—¥æ•°", f"{summary['r6_days']} æ—¥")
        with col4:
            st.metric("ğŸ“ ä½ç½®ç‰¹å®šæ¸ˆã¿", f"{summary['geocoded']:,} ä»¶")
        
        st.markdown("---")
        
        # æ¯”è¼ƒ
        st.markdown("### ğŸ“Š å¹´åº¦åˆ¥æ¯”è¼ƒ")
        col_comp1, col_comp2, col_comp3 = st.columns(3)
        
        with col_comp1:
            st.metric("H27 (2015å¹´)", f"{summary['h27_total']:,} ä»¶")
        with col_comp2:
            st.metric("R6 (2024å¹´)", f"{summary['r6_total']:,} ä»¶")
        with col_comp3:
            if summary['h27_total'] > 0:
                change = summary['r6_total'] - summary['h27_total']
                change_pct = (change / summary['h27_total']) * 100
                st.metric("å¤‰åŒ–", f"{change:+,} ä»¶", delta=f"{change_pct:+.1f}%")
            else:
                st.metric("å¤‰åŒ–", "N/A")
        
        st.markdown("---")
        
        # ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ³ã‚¯
        st.markdown("### ğŸ”— å„æ©Ÿèƒ½ã¸")
        st.markdown("""
        | ã‚¿ãƒ– | èª¬æ˜ |
        |------|------|
        | **åˆ°é”åœ** | æ¶ˆé˜²ç½²ã‹ã‚‰ã®5/10/15/20åˆ†åˆ°é”åœã‚’è¡¨ç¤ºã€‚ä»®æƒ³æ¶ˆé˜²ç½²ã®è¿½åŠ ã‚‚å¯èƒ½ |
        | **å‡ºå‹•åœ°ç‚¹ (R6)** | æ—¥åˆ¥ã®å‡ºå‹•åœ°ç‚¹ã‚’ãƒ—ãƒ­ãƒƒãƒˆã€‚ğŸ¬ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤ºå¯¾å¿œ |
        | **ã‚«ãƒãƒ¼ç‡åˆ†æ** | H27ã¨R6ã®ã‚«ãƒãƒ¼ç‡ã‚’æ¯”è¼ƒã€‚æ”¹å–„åº¦ã‚’æ•°å€¤ã§ç¢ºèª |
        | **ãƒªã‚½ãƒ¼ã‚¹åˆ†æ** | å„æ¶ˆé˜²ç½²ã®ãƒªã‚½ãƒ¼ã‚¹ï¼ˆæ•‘æ€¥è»Šå°æ•°ï¼‰ã‚’åˆ†æ |
        | **é…ç½®æœ€é©åŒ–** | æ–°è¦æ¶ˆé˜²ç½²ã®æœ€é©é…ç½®ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ |
        """)
        
        st.info("ğŸ’¡ å„ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è©³ç´°ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„")

    with tab_iso:
        with st.expander("ğŸ—ºï¸ ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ ï¼ˆã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ï¼‰", expanded=False):
            st.markdown("**åœ°å›³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ä»®æƒ³æ¶ˆé˜²ç½²ã®å ´æ‰€ã‚’é¸æŠã—ã¦ãã ã•ã„**")
            st.caption("ã‚¯ãƒªãƒƒã‚¯å¾Œã€åå‰ã‚’å…¥åŠ›ã—ã¦ã€Œè¿½åŠ ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

            # ã‚¯ãƒªãƒƒã‚¯é¸æŠç”¨ã®åœ°å›³ã‚’è¡¨ç¤º
            picker_map = create_location_picker_map(
                stations,
                st.session_state["virtual_stations"],
            )
            map_data = st_folium(
                picker_map,
                width=700,
                height=400,
                key="location_picker",
                returned_objects=["last_clicked"],
            )

            # ã‚¯ãƒªãƒƒã‚¯ã—ãŸåº§æ¨™ã‚’å–å¾—
            clicked_lat = None
            clicked_lon = None
            if map_data and map_data.get("last_clicked"):
                clicked_lat = map_data["last_clicked"]["lat"]
                clicked_lon = map_data["last_clicked"]["lng"]

            col_info, col_add = st.columns([2, 1])
            with col_info:
                if clicked_lat is not None:
                    st.success(f"ğŸ“ é¸æŠä½ç½®: ç·¯åº¦ {clicked_lat:.6f}, çµŒåº¦ {clicked_lon:.6f}")
                else:
                    st.info("ğŸ’¡ åœ°å›³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å ´æ‰€ã‚’é¸æŠã—ã¦ãã ã•ã„")

            with col_add:
                default_name = f"ä»®æƒ³ç½²{len(st.session_state['virtual_stations']) + 1}"
                v_name = st.text_input("åå‰", value=default_name, key="virtual_name_input")

            # è¿½åŠ ãƒœã‚¿ãƒ³
            col_btn1, col_btn2 = st.columns(2)
            with col_btn1:
                if st.button("âœ… ã“ã®å ´æ‰€ã«è¿½åŠ ", type="primary", disabled=(clicked_lat is None)):
                    if clicked_lat is not None:
                        st.session_state["virtual_stations"].append({
                            "ç•¥ç§°": v_name.strip() or default_name,
                            "ç·¯åº¦": clicked_lat,
                            "çµŒåº¦": clicked_lon,
                        })
                        st.success(f"ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {v_name}")
                        st.rerun()
            with col_btn2:
                if st.button("ğŸ—‘ï¸ å…¨ã¦ã‚¯ãƒªã‚¢", type="secondary"):
                    st.session_state["virtual_stations"] = []
                    st.info("ä»®æƒ³æ¶ˆé˜²ç½²ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                    st.rerun()

            # è¿½åŠ æ¸ˆã¿ä»®æƒ³æ¶ˆé˜²ç½²ä¸€è¦§
            if st.session_state["virtual_stations"]:
                st.markdown("---")
                st.markdown("**è¿½åŠ æ¸ˆã¿ä»®æƒ³æ¶ˆé˜²ç½²:**")
                for i, vs in enumerate(st.session_state["virtual_stations"]):
                    col_name, col_coord, col_del = st.columns([2, 3, 1])
                    with col_name:
                        st.write(f"ğŸ”´ {vs['ç•¥ç§°']}")
                    with col_coord:
                        st.caption(f"({vs['ç·¯åº¦']:.5f}, {vs['çµŒåº¦']:.5f})")
                    with col_del:
                        if st.button("å‰Šé™¤", key=f"del_vs_{i}"):
                            st.session_state["virtual_stations"].pop(i)
                            st.rerun()

        has_virtual = bool(st.session_state["virtual_stations"])
        stations_view = append_virtual_stations(stations, st.session_state["virtual_stations"])
        station_names = sorted(stations_view["ç•¥ç§°"].unique())
        trip_options = [5, 10, 15, 20]

        col_left, col_right = st.columns([2, 1])
        with col_left:
            selected_names = st.multiselect(
                "è¡¨ç¤ºã™ã‚‹æ¶ˆé˜²ç½²",
                options=station_names,
                default=station_names,
                help="è¤‡æ•°é¸æŠã§åˆ°é”åœã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚",
            )
        with col_right:
            selected_times = st.multiselect(
                "åˆ°é”æ™‚é–“ (åˆ†)",
                options=trip_options,
                default=[5, 10],
            )

        # ========== ğŸš¦ æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ ==========
        with st.expander("ğŸš¦ æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’æ¸ˆã¿ï¼‰", expanded=False):
            factors_exist = DELAY_FACTORS_PATH.exists()
            if factors_exist:
                st.success("âœ… R6å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸé…å»¶ä¿‚æ•°ã‚’ä½¿ç”¨")
            else:
                st.warning("âš ï¸ misc/learn_delays.py ã‚’å®Ÿè¡Œã™ã‚‹ã¨å­¦ç¿’ã§ãã¾ã™")

            traffic_enabled = st.toggle("æ¸‹æ»ã‚’è€ƒæ…®ã™ã‚‹", value=False, key="traffic_enabled")

            if traffic_enabled:
                col_time, col_dow = st.columns(2)
                with col_time:
                    time_slot = st.selectbox(
                        "æ™‚é–“å¸¯",
                        options=list(TIME_SLOT_LABELS.keys()),
                        index=3,  # æœãƒ©ãƒƒã‚·ãƒ¥
                        key="traffic_time_slot",
                    )
                    # ä»£è¡¨çš„ãªæ™‚é–“ã‚’å–å¾—
                    slot_hours = TIME_SLOT_LABELS[time_slot]
                    selected_hour = slot_hours[len(slot_hours) // 2]

                with col_dow:
                    use_dow = st.checkbox("æ›œæ—¥ã‚‚è€ƒæ…®", value=False, key="traffic_use_dow")
                    if use_dow:
                        dow_label = st.selectbox(
                            "æ›œæ—¥",
                            options=DOW_LABELS,
                            index=0,
                            key="traffic_dow",
                        )
                        selected_dow = DOW_LABELS.index(dow_label)
                    else:
                        selected_dow = None

                delay_factor = get_delay_factor(selected_hour, selected_dow)
                
                # ä¿‚æ•°ã®æ„å‘³ã‚’è¡¨ç¤ºï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿: æ—¥ä¸­ãŒæœ€é€Ÿã€æ·±å¤œãŒé…ã„ï¼‰
                if delay_factor < 1.05:
                    emoji = "ğŸŸ¢"
                    desc = "æœ€é€Ÿï¼ˆæ—¥ä¸­å¸¯ï¼‰"
                elif delay_factor < 1.2:
                    desc = "ã‚„ã‚„é…ã„"
                    emoji = "ğŸŸ¡"
                else:
                    desc = "é…ã„ï¼ˆæ·±å¤œå¸¯ï¼‰"
                    emoji = "ğŸ”´"
                
                st.info(f"{emoji} é…å»¶ä¿‚æ•°: **{delay_factor:.3f}** ({desc})")
                st.caption(f"â†’ ä¾‹: 5åˆ†åœãŒå®Ÿè³ª {5 * delay_factor:.1f}åˆ†åœ ã«ç¸®å°")
            else:
                delay_factor = 1.0
        # ========================================

        if not selected_names:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®æ¶ˆé˜²ç½²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        if not selected_times:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®åˆ°é”æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        filtered = stations_view[stations_view["ç•¥ç§°"].isin(selected_names)].copy()

        padding_deg = 0.1
        west_all, south_all, east_all, north_all = stations_view.total_bounds
        bbox = (north_all + padding_deg, south_all - padding_deg, east_all + padding_deg, west_all - padding_deg)

        with st.spinner("é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            graph = load_graph_cached(bbox)

        # æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã‚ãšå†è¨ˆç®—ï¼ˆä¿‚æ•°ãŒç•°ãªã‚‹ãŸã‚ï¼‰
        use_cache = ISOCHRONE_CACHE_PATH.exists() and not has_virtual and delay_factor == 1.0
        
        if use_cache:
            try:
                with st.spinner("åˆ°é”åœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    all_isochrones = load_precomputed_isochrones(ISOCHRONE_CACHE_PATH)
                display_isochrones = all_isochrones[
                    (all_isochrones["name"].isin(selected_names)) &
                    (all_isochrones["time"].isin(selected_times))
                ].copy()
            except Exception as exc:
                st.warning(f"äº‹å‰è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚å†è¨ˆç®—ã—ã¾ã™: {exc}")
                display_isochrones = None
        else:
            display_isochrones = None

        if display_isochrones is None:
            spinner_msg = "åˆ°é”åœã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™..."
            if delay_factor != 1.0:
                spinner_msg = f"æ¸‹æ»è€ƒæ…®ã§åˆ°é”åœã‚’è¨ˆç®—ä¸­ï¼ˆä¿‚æ•°: {delay_factor:.3f}ï¼‰..."
            with st.spinner(spinner_msg):
                prog = st.progress(0)
                display_isochrones = compute_isochrones(
                    graph=graph,
                    stations=filtered,
                    trip_times=selected_times,
                    progress_cb=lambda p: prog.progress(int(p * 100)),
                    delay_factor=delay_factor,
                )

        if display_isochrones.empty:
            st.error("é¸æŠæ¡ä»¶ã«åˆè‡´ã™ã‚‹åˆ°é”åœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        html_map = render_map_html(display_isochrones, filtered)
        st.components.v1.html(html_map, height=720)

    with tab_inc:
        st.subheader("ğŸ—“ï¸ å‡ºå‹•åœ°ç‚¹ãƒ—ãƒ­ãƒƒãƒˆ")
        
        try:
            incidents = load_incident_data("R6.xlsx")
        except FileNotFoundError:
            st.error("R6.xlsx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        date_options = sorted(incidents["date"].unique())
        if not date_options:
            st.warning("R6.xlsx ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        # æ—¥ä»˜é¸æŠ
        col_date, col_mode = st.columns([2, 1])
        with col_date:
            default_date = date_options[0]
            selected_date = st.selectbox(
                "è¡¨ç¤ºã™ã‚‹æ—¥ä»˜ (è¦šçŸ¥æ—¥)",
                options=date_options,
                format_func=lambda d: d.strftime("%Y-%m-%d"),
                index=0,
            )
        with col_mode:
            display_mode = st.radio(
                "è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰",
                ["ğŸ“ é™çš„è¡¨ç¤º", "ğŸ¬ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³"],
                horizontal=True,
                help="ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯æ™‚ç³»åˆ—ã§å‡ºå‹•ã‚’å†ç”Ÿã§ãã¾ã™"
            )

        day_inc = incidents[incidents["date"] == selected_date].copy()
        addr_series = day_inc["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
        addr_unique = sorted(addr_series.unique())

        st.write(f"{selected_date} ã®å‡ºå‹•ä»¶æ•°: {len(day_inc)} ä»¶ (ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°ç‚¹ {len(addr_unique)} ç®‡æ‰€)")

        with st.spinner("ä½æ‰€ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ã¾ã™ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ..."):
            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")

        merged = day_inc.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
        mapped = merged.dropna(subset=["lat", "lon"]).copy()
        missing_count = len(day_inc) - len(mapped)

        if mapped.empty:
            st.error("ã“ã®æ—¥ã®åœ°ç‚¹ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        st.write(f"åœ°å›³ã«ãƒ—ãƒ­ãƒƒãƒˆã§ããŸä»¶æ•°: {len(mapped)} / {len(day_inc)} (æœªç‰¹å®š {missing_count} ä»¶)")

        # æ›œæ—¥åˆ¥ã‚«ãƒ©ãƒ¼ãƒãƒƒãƒ—
        weekday_colors = {
            "æœˆ": "#f94144",
            "ç«": "#f3722c",
            "æ°´": "#f9c74f",
            "æœ¨": "#90be6d",
            "é‡‘": "#43aa8b",
            "åœŸ": "#577590",
            "æ—¥": "#9d4edd",
        }

        center_lat = mapped["lat"].mean()
        center_lon = mapped["lon"].mean()

        if display_mode == "ğŸ“ é™çš„è¡¨ç¤º":
            # å¾“æ¥ã®é™çš„è¡¨ç¤º
            fmap = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles="CartoDB Positron")

            for _, row in mapped.iterrows():
                wk = str(row.get("æ›œæ—¥", "?"))
                color = weekday_colors.get(wk, "#4a4a4a")
                label_time = row["è¦šçŸ¥"].strftime("%H:%M") if not pd.isna(row.get("è¦šçŸ¥")) else "--:--"
                popup = f"{row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time} | {row.get('æ¬é€åŒºåˆ†(äº‹æ¡ˆ)', '')}"
                folium.CircleMarker(
                    location=[row["lat"], row["lon"]],
                    radius=5,
                    color=color,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.85,
                    weight=1.0,
                    popup=popup,
                ).add_to(fmap)

            st.components.v1.html(fmap.get_root().render(), height=720)

        else:
            # ğŸ¬ ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º
            st.markdown("---")
            st.markdown("### ğŸ¬ å‡ºå‹•ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š")
            
            col_anim1, col_anim2, col_anim3 = st.columns(3)
            with col_anim1:
                period_min = st.slider(
                    "å†ç”Ÿã‚¹ãƒ†ãƒƒãƒ— (åˆ†)",
                    min_value=5,
                    max_value=60,
                    value=15,
                    step=5,
                    help="ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚ãŸã‚Šã®æ™‚é–“"
                )
            with col_anim2:
                duration_min = st.slider(
                    "ãƒã‚¤ãƒ³ãƒˆè¡¨ç¤ºæ™‚é–“ (åˆ†)",
                    min_value=30,
                    max_value=180,
                    value=60,
                    step=30,
                    help="å‡ºå‹•åœ°ç‚¹ãŒåœ°å›³ä¸Šã«è¡¨ç¤ºã•ã‚Œç¶šã‘ã‚‹æ™‚é–“"
                )
            with col_anim3:
                auto_play = st.checkbox("è‡ªå‹•å†ç”Ÿ", value=False)
            
            # GeoJSON FeatureCollection ã‚’ç”Ÿæˆ
            from folium.plugins import TimestampedGeoJson
            
            # æ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆ
            mapped_sorted = mapped.sort_values("è¦šçŸ¥")
            
            features = []
            for _, row in mapped_sorted.iterrows():
                if pd.isna(row.get("è¦šçŸ¥")):
                    continue
                
                wk = str(row.get("æ›œæ—¥", "?"))
                color = weekday_colors.get(wk, "#4a4a4a")
                
                # ISO8601å½¢å¼ã®æ™‚åˆ»
                time_str = row["è¦šçŸ¥"].strftime("%Y-%m-%dT%H:%M:%S")
                
                label_time = row["è¦šçŸ¥"].strftime("%H:%M")
                popup_text = f"{row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time} | {row.get('æ¬é€åŒºåˆ†(äº‹æ¡ˆ)', '')}"
                
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [row["lon"], row["lat"]],  # GeoJSON: [lon, lat]
                    },
                    "properties": {
                        "time": time_str,
                        "popup": popup_text,
                        "icon": "circle",
                        "iconstyle": {
                            "fillColor": color,
                            "fillOpacity": 0.8,
                            "stroke": "true",
                            "color": color,
                            "radius": 8,
                        },
                    },
                }
                features.append(feature)
            
            geojson_data = {
                "type": "FeatureCollection",
                "features": features,
            }
            
            # åœ°å›³ç”Ÿæˆ
            fmap_anim = folium.Map(
                location=[center_lat, center_lon],
                zoom_start=12,
                tiles="CartoDB Positron"
            )
            
            # TimestampedGeoJsonè¿½åŠ 
            TimestampedGeoJson(
                geojson_data,
                period=f"PT{period_min}M",
                duration=f"PT{duration_min}M",
                auto_play=auto_play,
                loop=True,
                loop_button=True,
                date_options="HH:mm",
                time_slider_drag_update=True,
            ).add_to(fmap_anim)
            
            # æ¶ˆé˜²ç½²ã‚‚è¡¨ç¤º
            for _, station in stations.iterrows():
                folium.Marker(
                    location=[station["ç·¯åº¦"], station["çµŒåº¦"]],
                    popup=f"ğŸ¥ {station['ç•¥ç§°']}",
                    icon=folium.Icon(color="blue", icon="plus", prefix="fa"),
                ).add_to(fmap_anim)
            
            st.components.v1.html(fmap_anim.get_root().render(), height=720)
            
            st.caption("""
            **æ“ä½œæ–¹æ³•**: 
            - â–¶ï¸ å†ç”Ÿãƒœã‚¿ãƒ³ã§ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹
            - ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’ãƒ‰ãƒ©ãƒƒã‚°ã—ã¦æ™‚åˆ»ã‚’ç§»å‹•
            - å‡ºå‹•åœ°ç‚¹ã¯æ™‚é–“çµŒéã§å‡ºç¾ãƒ»æ¶ˆæ»…ã—ã¾ã™
            """)
            
            # æ™‚é–“å¸¯åˆ¥ã®å‡ºå‹•ä»¶æ•°ã‚µãƒãƒª
            with st.expander("ğŸ“Š æ™‚é–“å¸¯åˆ¥ å‡ºå‹•ä»¶æ•°", expanded=False):
                mapped_sorted["hour"] = pd.to_datetime(mapped_sorted["è¦šçŸ¥"]).dt.hour
                hourly_counts = mapped_sorted.groupby("hour").size().reset_index(name="ä»¶æ•°")
                hourly_counts.columns = ["æ™‚", "ä»¶æ•°"]
                
                import altair as alt
                chart = alt.Chart(hourly_counts).mark_bar(color="#f94144").encode(
                    x=alt.X("æ™‚:O", title="æ™‚åˆ»"),
                    y=alt.Y("ä»¶æ•°:Q", title="å‡ºå‹•ä»¶æ•°"),
                    tooltip=["æ™‚", "ä»¶æ•°"],
                ).properties(width=600, height=200)
                st.altair_chart(chart, width="stretch")

    with tab_coverage:
        st.subheader("ğŸ“Š å‡ºå‹•åœ°ç‚¹ã®åˆ°é”åœã‚«ãƒãƒ¼ç‡åˆ†æ")
        st.caption("ç¾åœ¨ã®æ¶ˆé˜²ç½²é…ç½®ã§H27ãƒ»R6ã®å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¾ã™ã€‚")

        # Check file availability
        r6_available = Path("R6.xlsx").exists()
        h27_available = Path("H27.xls").exists()

        if not r6_available and not h27_available:
            st.error("å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚R6.xlsx ã¾ãŸã¯ H27.xls ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # Dataset selection
        dataset_options = []
        if r6_available:
            dataset_options.append("R6 (2024å¹´)")
        if h27_available:
            dataset_options.append("H27 (2015å¹´)")
        if r6_available and h27_available:
            dataset_options.append("â­ æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ (R6 vs H27)")

        col_mode, col_resource = st.columns([2, 1])
        with col_mode:
            selected_mode = st.radio(
                "åˆ†æãƒ¢ãƒ¼ãƒ‰",
                options=dataset_options,
                horizontal=True,
            )
        
        is_comparison = "æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰" in selected_mode

        # ========== ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ ==========
        with col_resource:
            resource_mode = st.checkbox(
                "ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®",
                value=False,
                help="å„å‡ºå‹•åœ°ç‚¹ã§åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã‚’è€ƒæ…®ã—ãŸã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ã—ã¾ã™",
            )
        
        if resource_mode:
            st.info("""
            **ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰**: å˜ç´”ãªåˆ°é”åœå†…/å¤–ã§ã¯ãªãã€å„å‡ºå‹•åœ°ç‚¹ã«ã€Œä½•å°ã®æ•‘æ€¥è»ŠãŒåˆ°é”å¯èƒ½ã‹ã€ã‚’åˆ†æã—ã¾ã™ã€‚
            - ğŸŸ¢ **2å°ä»¥ä¸Š**: å†—é•·æ€§ã‚ã‚Šï¼ˆ1å°å‡ºå‹•ä¸­ã§ã‚‚å¯¾å¿œå¯èƒ½ï¼‰
            - ğŸŸ¡ **1å°ã®ã¿**: ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ãŒå†—é•·æ€§ãªã—
            - ğŸ”´ **0å°**: åˆ°é”åœå¤–
            """)
            
            # H27ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã®èª¿æ•´UI
            with st.expander("âš™ï¸ H27ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã‚’èª¿æ•´", expanded=False):
                st.caption("H27å½“æ™‚ã®æ­£ç¢ºãªé…ç½®ãŒä¸æ˜ãªãŸã‚ã€æ‰‹å‹•ã§èª¿æ•´ã§ãã¾ã™")
                
                h27_resources_custom = {}
                cols_h27 = st.columns(3)
                station_names = list(STATION_RESOURCES_H27_DEFAULT.keys())
                for i, station in enumerate(station_names):
                    default_val = STATION_RESOURCES_H27_DEFAULT[station]
                    with cols_h27[i % 3]:
                        h27_resources_custom[station] = st.number_input(
                            station,
                            min_value=0,
                            max_value=10,
                            value=default_val,
                            key=f"h27_res_{station}"
                        )
                
                total_h27 = sum(h27_resources_custom.values())
                total_r6 = sum(STATION_RESOURCES_R6.values())
                st.metric("H27 åˆè¨ˆ", f"{total_h27}å°", delta=f"{total_h27 - total_r6}å° vs R6")
                
                # session_stateã«ä¿å­˜
                st.session_state["h27_resources_custom"] = h27_resources_custom

        # ========== ğŸš¦ æ¸‹æ»è€ƒæ…® & æ™‚é–“å¸¯åˆ¥åˆ†æ ==========
        with st.expander("ğŸš¦ æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡åˆ†æ", expanded=False):
            factors_exist_cov = DELAY_FACTORS_PATH.exists()
            if factors_exist_cov:
                st.success("âœ… R6å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸé…å»¶ä¿‚æ•°ã‚’ä½¿ç”¨")
            else:
                st.warning("âš ï¸ misc/learn_delays.py ã‚’å®Ÿè¡Œã™ã‚‹ã¨å­¦ç¿’ã§ãã¾ã™")

            analysis_type = st.radio(
                "åˆ†æã‚¿ã‚¤ãƒ—",
                ["é€šå¸¸ï¼ˆæ¸‹æ»ãªã—ï¼‰", "ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡", "ğŸš¦ ç‰¹å®šæ™‚é–“å¸¯ã®æ¸‹æ»è€ƒæ…®"],
                horizontal=True,
                key="coverage_analysis_type",
            )

            cov_delay_factor = 1.0
            selected_hour_cov = None
            hourly_analysis = False

            if analysis_type == "ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡":
                hourly_analysis = True
                st.info("ğŸ“Š å‡ºå‹•æ™‚åˆ»ã«åŸºã¥ã„ã¦æ™‚é–“å¸¯åˆ¥ã®ã‚«ãƒãƒ¼ç‡ã‚’é›†è¨ˆã—ã¾ã™")
            
            elif analysis_type == "ğŸš¦ ç‰¹å®šæ™‚é–“å¸¯ã®æ¸‹æ»è€ƒæ…®":
                col_slot, col_info = st.columns([1, 1])
                with col_slot:
                    time_slot_cov = st.selectbox(
                        "æ™‚é–“å¸¯",
                        options=list(TIME_SLOT_LABELS.keys()),
                        index=3,
                        key="coverage_time_slot",
                    )
                    slot_hours_cov = TIME_SLOT_LABELS[time_slot_cov]
                    selected_hour_cov = slot_hours_cov[len(slot_hours_cov) // 2]
                    cov_delay_factor = get_delay_factor(selected_hour_cov)
                
                with col_info:
                    if cov_delay_factor < 1.0:
                        emoji_cov = "ğŸŸ¢"
                        desc_cov = "æ•‘æ€¥å„ªå…ˆèµ°è¡Œã§é€Ÿã„"
                    elif cov_delay_factor < 1.1:
                        emoji_cov = "ğŸŸ¡"
                        desc_cov = "é€šå¸¸"
                    else:
                        emoji_cov = "ğŸ”´"
                        desc_cov = "ã‚„ã‚„æ··é›‘"
                    st.metric(
                        "é…å»¶ä¿‚æ•°",
                        f"{cov_delay_factor:.3f}",
                        delta=desc_cov,
                    )
        # ================================================

        # Load or compute isochrones (shared for all modes)
        stations_cov = load_station_data(
            db_path=STATIONS_DB_PATH,
            excel_path="map.xlsx",
            source_mtime=station_data_version(),
        )
        trip_times_cov = [5, 10]

        # æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã‚ãšå†è¨ˆç®—
        use_cache_cov = ISOCHRONE_CACHE_PATH.exists() and cov_delay_factor == 1.0

        if use_cache_cov:
            try:
                with st.spinner("åˆ°é”åœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    isochrones_cov = load_precomputed_isochrones(ISOCHRONE_CACHE_PATH)
            except Exception as exc:
                st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—: {exc}")
                isochrones_cov = None
        else:
            isochrones_cov = None

        if isochrones_cov is None:
            padding_deg = 0.1
            west_cov, south_cov, east_cov, north_cov = stations_cov.total_bounds
            bbox_cov = (north_cov + padding_deg, south_cov - padding_deg, east_cov + padding_deg, west_cov - padding_deg)
            with st.spinner("é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                graph_cov = load_graph_cached(bbox_cov)
            spinner_msg_cov = "åˆ°é”åœã‚’è¨ˆç®—ä¸­..."
            if cov_delay_factor != 1.0:
                spinner_msg_cov = f"æ¸‹æ»è€ƒæ…®ã§åˆ°é”åœã‚’è¨ˆç®—ä¸­ï¼ˆä¿‚æ•°: {cov_delay_factor:.3f}ï¼‰..."
            with st.spinner(spinner_msg_cov):
                prog_cov = st.progress(0)
                isochrones_cov = compute_isochrones(
                    graph=graph_cov,
                    stations=stations_cov,
                    trip_times=trip_times_cov,
                    progress_cb=lambda p: prog_cov.progress(int(p * 100)),
                    delay_factor=cov_delay_factor,
                )

        def analyze_coverage(incidents_df: pd.DataFrame, label: str, isochrones: gpd.GeoDataFrame = None, with_resources: bool = False) -> dict:
            """Analyze coverage for a given incident dataset.
            
            Args:
                incidents_df: å‡ºå‹•ãƒ‡ãƒ¼ã‚¿
                label: ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«
                isochrones: åˆ°é”åœãƒ‡ãƒ¼ã‚¿
                with_resources: ãƒªã‚½ãƒ¼ã‚¹ï¼ˆæ•‘æ€¥è»Šå°æ•°ï¼‰ã‚’è€ƒæ…®ã™ã‚‹ã‹
            """
            if isochrones is None:
                isochrones = isochrones_cov
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ©ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã‚’é¸æŠ
            if "H27" in label or "2015" in label:
                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                station_resources = st.session_state.get("h27_resources_custom", STATION_RESOURCES_H27_DEFAULT)
                total_amb = sum(station_resources.values())
                resource_label = f"H27ï¼ˆæ¨å®š{total_amb}å°ï¼‰"
            else:
                station_resources = STATION_RESOURCES_R6
                total_amb = sum(station_resources.values())
                resource_label = f"R6ï¼ˆ{total_amb}å°ï¼‰"
            
            addr_series = incidents_df["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
            addr_unique = sorted(addr_series.unique())

            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")
            merged = incidents_df.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
            mapped = merged.dropna(subset=["lat", "lon"]).copy()

            if mapped.empty:
                return None

            incident_points = gpd.GeoDataFrame(
                mapped,
                geometry=gpd.points_from_xy(mapped["lon"], mapped["lat"]),
                crs="EPSG:4326"
            )

            results = {"label": label, "total": len(incidents_df), "geocoded": len(mapped), "resource_config": resource_label}
            
            for minutes in trip_times_cov:
                iso_layer = isochrones[isochrones["time"] == minutes]
                if iso_layer.empty:
                    results[f"covered_{minutes}"] = 0
                    if with_resources:
                        incident_points[f"ambulances_{minutes}min"] = 0
                    continue
                combined_polygon = unary_union(iso_layer.geometry)
                within_mask = incident_points.geometry.within(combined_polygon)
                results[f"covered_{minutes}"] = within_mask.sum()
                incident_points[f"within_{minutes}min"] = within_mask
                
                # ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ï¼šå„å‡ºå‹•åœ°ç‚¹ã§åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã‚’è¨ˆç®—
                if with_resources:
                    ambulance_counts = []
                    for idx, row in incident_points.iterrows():
                        point = row.geometry
                        count = 0
                        # å„æ¶ˆé˜²ç½²ã®åˆ°é”åœã«å«ã¾ã‚Œã‚‹ã‹ç¢ºèª
                        for _, iso_row in iso_layer.iterrows():
                            station_name = iso_row["name"] if "name" in iso_row.index else ""
                            if point.within(iso_row.geometry):
                                # ãã®ç½²ã®æ•‘æ€¥è»Šå°æ•°ã‚’åŠ ç®—ï¼ˆå¹´åº¦åˆ¥ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã‚’ä½¿ç”¨ï¼‰
                                count += station_resources.get(station_name, DEFAULT_AMBULANCES)
                        ambulance_counts.append(count)
                    incident_points[f"ambulances_{minutes}min"] = ambulance_counts
                    
                    # ãƒªã‚½ãƒ¼ã‚¹åˆ¥ã‚«ãƒãƒ¼ç‡
                    results[f"covered_{minutes}_0amb"] = (incident_points[f"ambulances_{minutes}min"] == 0).sum()  # åœå¤–
                    results[f"covered_{minutes}_1amb"] = (incident_points[f"ambulances_{minutes}min"] == 1).sum()  # 1å°ã®ã¿
                    results[f"covered_{minutes}_2amb"] = (incident_points[f"ambulances_{minutes}min"] >= 2).sum()  # 2å°ä»¥ä¸Š

            results["incident_points"] = incident_points
            results["mapped"] = mapped
            return results

        def analyze_hourly_coverage(incidents_df: pd.DataFrame) -> pd.DataFrame:
            """Analyze coverage by hour of day."""
            # æ™‚é–“å¸¯ã®å®šç¾©
            time_bins = {
                "æ·±å¤œ (0-5æ™‚)": list(range(0, 5)),
                "æ—©æœ (5-7æ™‚)": list(range(5, 7)),
                "æœãƒ©ãƒƒã‚·ãƒ¥ (7-9æ™‚)": list(range(7, 9)),
                "åˆå‰ (9-12æ™‚)": list(range(9, 12)),
                "æ˜¼ (12-14æ™‚)": list(range(12, 14)),
                "åˆå¾Œ (14-17æ™‚)": list(range(14, 17)),
                "å¤•ãƒ©ãƒƒã‚·ãƒ¥ (17-19æ™‚)": list(range(17, 19)),
                "å¤œ (19-22æ™‚)": list(range(19, 22)),
                "æ·±å¤œ (22-24æ™‚)": list(range(22, 24)),
            }
            
            # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            addr_series = incidents_df["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
            addr_unique = sorted(addr_series.unique())
            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")
            merged = incidents_df.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
            mapped = merged.dropna(subset=["lat", "lon"]).copy()
            
            if mapped.empty:
                return None
            
            # æ™‚é–“å¸¯åˆ—ã‚’è¿½åŠ 
            mapped["hour"] = pd.to_datetime(mapped["è¦šçŸ¥"], errors="coerce").dt.hour
            
            incident_points = gpd.GeoDataFrame(
                mapped,
                geometry=gpd.points_from_xy(mapped["lon"], mapped["lat"]),
                crs="EPSG:4326"
            )
            
            # æ™‚é–“å¸¯ã”ã¨ã«ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—
            results = []
            for slot_name, hours in time_bins.items():
                slot_points = incident_points[incident_points["hour"].isin(hours)]
                if slot_points.empty:
                    continue
                
                total = len(slot_points)
                row = {"æ™‚é–“å¸¯": slot_name, "ä»¶æ•°": total}
                
                for minutes in trip_times_cov:
                    iso_layer = isochrones_cov[isochrones_cov["time"] == minutes]
                    if iso_layer.empty:
                        row[f"{minutes}åˆ†åœã‚«ãƒãƒ¼"] = 0
                        row[f"{minutes}åˆ†åœç‡"] = 0.0
                        continue
                    combined_polygon = unary_union(iso_layer.geometry)
                    within_mask = slot_points.geometry.within(combined_polygon)
                    covered = within_mask.sum()
                    row[f"{minutes}åˆ†åœã‚«ãƒãƒ¼"] = covered
                    row[f"{minutes}åˆ†åœç‡"] = covered / total * 100 if total > 0 else 0
                
                results.append(row)
            
            return pd.DataFrame(results)

        # ========== æ™‚é–“å¸¯åˆ¥åˆ†æãƒ¢ãƒ¼ãƒ‰ ==========
        if hourly_analysis:
            st.markdown("---")
            st.subheader("ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡åˆ†æ")
            
            import altair as alt
            
            # æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
            if is_comparison:
                col_r6_h, col_h27_h = st.columns(2)
                
                with st.spinner("R6ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    incidents_r6_hourly = load_incident_data("R6.xlsx")
                    hourly_df_r6 = analyze_hourly_coverage(incidents_r6_hourly)
                
                with st.spinner("H27ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    incidents_h27_hourly = load_incident_data_h27("H27.xls")
                    hourly_df_h27 = analyze_hourly_coverage(incidents_h27_hourly)
                
                if hourly_df_r6 is not None and hourly_df_h27 is not None:
                    # ä¸¦ã¹ã¦è¡¨ç¤º
                    with col_r6_h:
                        st.markdown("### ğŸŸ¢ R6 (2024å¹´)")
                        display_df_r6 = hourly_df_r6[["æ™‚é–“å¸¯", "ä»¶æ•°", "5åˆ†åœç‡", "10åˆ†åœç‡"]].copy()
                        display_df_r6["5åˆ†åœç‡"] = display_df_r6["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        display_df_r6["10åˆ†åœç‡"] = display_df_r6["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        st.dataframe(display_df_r6, width="stretch", hide_index=True)
                    
                    with col_h27_h:
                        st.markdown("### ğŸŸ¡ H27 (2015å¹´)")
                        display_df_h27 = hourly_df_h27[["æ™‚é–“å¸¯", "ä»¶æ•°", "5åˆ†åœç‡", "10åˆ†åœç‡"]].copy()
                        display_df_h27["5åˆ†åœç‡"] = display_df_h27["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        display_df_h27["10åˆ†åœç‡"] = display_df_h27["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        st.dataframe(display_df_h27, width="stretch", hide_index=True)
                    
                    # æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆ5åˆ†åœï¼‰
                    st.markdown("### ğŸ“ˆ 5åˆ†åœã‚«ãƒãƒ¼ç‡æ¯”è¼ƒã‚°ãƒ©ãƒ•")
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                    hourly_df_r6["ãƒ‡ãƒ¼ã‚¿"] = "R6 (2024å¹´)"
                    hourly_df_h27["ãƒ‡ãƒ¼ã‚¿"] = "H27 (2015å¹´)"
                    combined_hourly = pd.concat([hourly_df_r6, hourly_df_h27], ignore_index=True)
                    
                    chart_5min = alt.Chart(combined_hourly).mark_bar().encode(
                        x=alt.X("æ™‚é–“å¸¯:N", sort=list(TIME_SLOT_LABELS.keys()), title="æ™‚é–“å¸¯"),
                        y=alt.Y("5åˆ†åœç‡:Q", title="5åˆ†åœã‚«ãƒãƒ¼ç‡ (%)"),
                        color=alt.Color("ãƒ‡ãƒ¼ã‚¿:N", scale=alt.Scale(
                            domain=["R6 (2024å¹´)", "H27 (2015å¹´)"],
                            range=["#2ecc71", "#f1c40f"]
                        )),
                        xOffset="ãƒ‡ãƒ¼ã‚¿:N",
                        tooltip=["æ™‚é–“å¸¯", "ãƒ‡ãƒ¼ã‚¿", alt.Tooltip("5åˆ†åœç‡:Q", format=".1f"), "ä»¶æ•°"],
                    ).properties(
                        width=600,
                        height=350,
                    )
                    st.altair_chart(chart_5min, width="stretch")
                    
                    # å·®åˆ†ãƒ†ãƒ¼ãƒ–ãƒ«
                    st.markdown("### ğŸ“Š æ™‚é–“å¸¯åˆ¥ æ”¹å–„åº¦ï¼ˆR6 - H27ï¼‰")
                    diff_data = []
                    for slot in hourly_df_r6["æ™‚é–“å¸¯"].unique():
                        r6_row = hourly_df_r6[hourly_df_r6["æ™‚é–“å¸¯"] == slot]
                        h27_row = hourly_df_h27[hourly_df_h27["æ™‚é–“å¸¯"] == slot]
                        if not r6_row.empty and not h27_row.empty:
                            diff_5 = r6_row["5åˆ†åœç‡"].values[0] - h27_row["5åˆ†åœç‡"].values[0]
                            diff_10 = r6_row["10åˆ†åœç‡"].values[0] - h27_row["10åˆ†åœç‡"].values[0]
                            diff_data.append({
                                "æ™‚é–“å¸¯": slot,
                                "5åˆ†åœæ”¹å–„": f"{diff_5:+.1f}%",
                                "10åˆ†åœæ”¹å–„": f"{diff_10:+.1f}%",
                            })
                    
                    diff_df = pd.DataFrame(diff_data)
                    st.dataframe(diff_df, width="stretch", hide_index=True)
                    
                    # ã‚µãƒãƒª
                    st.markdown("### ğŸ” åˆ†æã‚µãƒãƒª")
                    avg_diff_5 = (hourly_df_r6["5åˆ†åœç‡"].mean() - hourly_df_h27["5åˆ†åœç‡"].mean())
                    avg_diff_10 = (hourly_df_r6["10åˆ†åœç‡"].mean() - hourly_df_h27["10åˆ†åœç‡"].mean())
                    
                    if avg_diff_5 > 0:
                        st.success(f"âœ… å…¨æ™‚é–“å¸¯å¹³å‡ 5åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_5:+.1f}%** æ”¹å–„")
                    else:
                        st.warning(f"âš ï¸ å…¨æ™‚é–“å¸¯å¹³å‡ 5åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_5:+.1f}%**")
                    
                    if avg_diff_10 > 0:
                        st.success(f"âœ… å…¨æ™‚é–“å¸¯å¹³å‡ 10åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_10:+.1f}%** æ”¹å–„")
                    else:
                        st.warning(f"âš ï¸ å…¨æ™‚é–“å¸¯å¹³å‡ 10åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_10:+.1f}%**")
                else:
                    st.error("æ™‚é–“å¸¯åˆ¥åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
            else:
                # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
                if "R6" in selected_mode:
                    incidents_hourly = load_incident_data("R6.xlsx")
                    data_label_hourly = "R6 (2024å¹´)"
                else:
                    incidents_hourly = load_incident_data_h27("H27.xls")
                    data_label_hourly = "H27 (2015å¹´)"
                
                with st.spinner(f"{data_label_hourly} ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    hourly_df = analyze_hourly_coverage(incidents_hourly)
                
                if hourly_df is not None:
                    st.markdown(f"### ğŸ“Š {data_label_hourly} æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡")
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                    display_df = hourly_df.copy()
                    display_df["5åˆ†åœç‡"] = display_df["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                    display_df["10åˆ†åœç‡"] = display_df["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                    st.dataframe(display_df, width="stretch", hide_index=True)
                    
                    # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                    st.markdown("### ğŸ“ˆ æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚°ãƒ©ãƒ•")
                    
                    chart_data = hourly_df.melt(
                        id_vars=["æ™‚é–“å¸¯", "ä»¶æ•°"],
                        value_vars=["5åˆ†åœç‡", "10åˆ†åœç‡"],
                        var_name="åˆ°é”åœ",
                        value_name="ã‚«ãƒãƒ¼ç‡",
                    )
                    
                    chart = alt.Chart(chart_data).mark_bar().encode(
                        x=alt.X("æ™‚é–“å¸¯:N", sort=list(TIME_SLOT_LABELS.keys()), title="æ™‚é–“å¸¯"),
                        y=alt.Y("ã‚«ãƒãƒ¼ç‡:Q", title="ã‚«ãƒãƒ¼ç‡ (%)"),
                        color=alt.Color("åˆ°é”åœ:N", scale=alt.Scale(
                            domain=["5åˆ†åœç‡", "10åˆ†åœç‡"],
                            range=["#ff9e9e", "#8aa5ff"]
                        )),
                        xOffset="åˆ°é”åœ:N",
                        tooltip=["æ™‚é–“å¸¯", "åˆ°é”åœ", alt.Tooltip("ã‚«ãƒãƒ¼ç‡:Q", format=".1f"), "ä»¶æ•°"],
                    ).properties(
                        width=600,
                        height=400,
                    )
                    st.altair_chart(chart, width="stretch")
                    
                    # æ™‚é–“å¸¯é–“ã®å·®ã‚’åˆ†æ
                    st.markdown("### ğŸ” åˆ†æã‚µãƒãƒª")
                    best_5min = hourly_df.loc[hourly_df["5åˆ†åœç‡"].idxmax()]
                    worst_5min = hourly_df.loc[hourly_df["5åˆ†åœç‡"].idxmin()]
                    
                    col_best, col_worst = st.columns(2)
                    with col_best:
                        st.success(f"âœ… 5åˆ†åœã‚«ãƒãƒ¼ç‡ æœ€é«˜: **{best_5min['æ™‚é–“å¸¯']}** ({best_5min['5åˆ†åœç‡']:.1f}%)")
                    with col_worst:
                        st.warning(f"âš ï¸ 5åˆ†åœã‚«ãƒãƒ¼ç‡ æœ€ä½: **{worst_5min['æ™‚é–“å¸¯']}** ({worst_5min['5åˆ†åœç‡']:.1f}%)")
                    
                    st.info(f"ğŸ“Š æ™‚é–“å¸¯ã«ã‚ˆã‚‹5åˆ†åœã‚«ãƒãƒ¼ç‡ã®å·®: **{best_5min['5åˆ†åœç‡'] - worst_5min['5åˆ†åœç‡']:.1f}%**")
                else:
                    st.error("æ™‚é–“å¸¯åˆ¥åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
            st.stop()  # æ™‚é–“å¸¯åˆ¥åˆ†æãƒ¢ãƒ¼ãƒ‰ã§ã¯ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        # ========================================

        if is_comparison:
            # Comparison mode: analyze both datasets
            st.markdown("---")
            col_r6, col_h27 = st.columns(2)

            with col_r6:
                st.markdown("### ğŸŸ¢ R6 (2024å¹´)")
            with col_h27:
                st.markdown("### ğŸŸ¡ H27 (2015å¹´)")

            with st.spinner("R6ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­..."):
                incidents_r6 = load_incident_data("R6.xlsx")
                results_r6 = analyze_coverage(incidents_r6, "R6", with_resources=resource_mode)

            with st.spinner("H27ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­..."):
                incidents_h27 = load_incident_data_h27("H27.xls")
                results_h27 = analyze_coverage(incidents_h27, "H27", with_resources=resource_mode)

            if results_r6 is None or results_h27 is None:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.stop()

            # Build comparison table
            comparison_data = []
            for minutes in trip_times_cov:
                r6_covered = results_r6[f"covered_{minutes}"]
                r6_total = results_r6["geocoded"]
                r6_pct = r6_covered / r6_total * 100 if r6_total > 0 else 0

                h27_covered = results_h27[f"covered_{minutes}"]
                h27_total = results_h27["geocoded"]
                h27_pct = h27_covered / h27_total * 100 if h27_total > 0 else 0

                diff = r6_pct - h27_pct

                comparison_data.append({
                    "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                    "R6 ã‚«ãƒãƒ¼ç‡": f"{r6_pct:.1f}%",
                    "R6 (ä»¶æ•°)": f"{r6_covered}/{r6_total}",
                    "H27 ã‚«ãƒãƒ¼ç‡": f"{h27_pct:.1f}%",
                    "H27 (ä»¶æ•°)": f"{h27_covered}/{h27_total}",
                    "å·®åˆ†": f"{diff:+.1f}%",
                })

            st.markdown("### ğŸ“Š ã‚«ãƒãƒ¼ç‡æ¯”è¼ƒï¼ˆåˆ°é”åœå†…/å¤–ï¼‰")
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, width="stretch", hide_index=True)

            # ========== ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã®è¿½åŠ è¡¨ç¤º ==========
            if resource_mode:
                st.markdown("---")
                st.markdown("### ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ã‚«ãƒãƒ¼ç‡æ¯”è¼ƒ")
                st.caption("å„å‡ºå‹•åœ°ç‚¹ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã§åˆ†é¡")
                
                # ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã‚’è¡¨ç¤º
                st.info(f"""
                **ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹è¨­å®š**
                - R6: {results_r6.get('resource_config', 'R6')}
                - H27: {results_h27.get('resource_config', 'H27')}
                """)
                
                resource_comparison = []
                for minutes in trip_times_cov:
                    r6_total = results_r6["geocoded"]
                    h27_total = results_h27["geocoded"]
                    
                    # R6ã®ãƒªã‚½ãƒ¼ã‚¹åˆ¥ã‚«ãƒãƒ¼
                    r6_0 = results_r6.get(f"covered_{minutes}_0amb", 0)
                    r6_1 = results_r6.get(f"covered_{minutes}_1amb", 0)
                    r6_2 = results_r6.get(f"covered_{minutes}_2amb", 0)
                    
                    # H27ã®ãƒªã‚½ãƒ¼ã‚¹åˆ¥ã‚«ãƒãƒ¼
                    h27_0 = results_h27.get(f"covered_{minutes}_0amb", 0)
                    h27_1 = results_h27.get(f"covered_{minutes}_1amb", 0)
                    h27_2 = results_h27.get(f"covered_{minutes}_2amb", 0)
                    
                    resource_comparison.append({
                        "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                        "R6 ğŸ”´åœå¤–": f"{r6_0} ({r6_0/r6_total*100:.1f}%)" if r6_total > 0 else "0",
                        "R6 ğŸŸ¡1å°": f"{r6_1} ({r6_1/r6_total*100:.1f}%)" if r6_total > 0 else "0",
                        "R6 ğŸŸ¢2å°+": f"{r6_2} ({r6_2/r6_total*100:.1f}%)" if r6_total > 0 else "0",
                        "H27 ğŸ”´åœå¤–": f"{h27_0} ({h27_0/h27_total*100:.1f}%)" if h27_total > 0 else "0",
                        "H27 ğŸŸ¡1å°": f"{h27_1} ({h27_1/h27_total*100:.1f}%)" if h27_total > 0 else "0",
                        "H27 ğŸŸ¢2å°+": f"{h27_2} ({h27_2/h27_total*100:.1f}%)" if h27_total > 0 else "0",
                    })
                
                resource_df = pd.DataFrame(resource_comparison)
                st.dataframe(resource_df, width="stretch", hide_index=True)
                
                # ãƒªã‚½ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                st.markdown("### ğŸ“Š ãƒªã‚½ãƒ¼ã‚¹å†—é•·æ€§ã®æ¯”è¼ƒ")
                for minutes in trip_times_cov:
                    r6_total = results_r6["geocoded"]
                    h27_total = results_h27["geocoded"]
                    
                    r6_redundant = results_r6.get(f"covered_{minutes}_2amb", 0) / r6_total * 100 if r6_total > 0 else 0
                    h27_redundant = results_h27.get(f"covered_{minutes}_2amb", 0) / h27_total * 100 if h27_total > 0 else 0
                    diff_redundant = r6_redundant - h27_redundant
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(f"R6 {minutes}åˆ† å†—é•·æ€§ã‚ã‚Š", f"{r6_redundant:.1f}%")
                    with col2:
                        st.metric(f"H27 {minutes}åˆ† å†—é•·æ€§ã‚ã‚Š", f"{h27_redundant:.1f}%")
                    with col3:
                        st.metric(
                            f"{minutes}åˆ† å†—é•·æ€§æ”¹å–„",
                            f"{diff_redundant:+.1f}%",
                            delta=f"{diff_redundant:+.1f}%" if diff_redundant != 0 else None,
                            delta_color="normal" if diff_redundant >= 0 else "inverse"
                        )
            # ================================================

            # Metrics side by side
            st.markdown("### ğŸ“ˆ å·®åˆ†ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
            for minutes in trip_times_cov:
                r6_pct = results_r6[f"covered_{minutes}"] / results_r6["geocoded"] * 100
                h27_pct = results_h27[f"covered_{minutes}"] / results_h27["geocoded"] * 100
                diff = r6_pct - h27_pct

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(f"R6 {minutes}åˆ†åˆ°é”åœ", f"{r6_pct:.1f}%")
                with col2:
                    st.metric(f"H27 {minutes}åˆ†åˆ°é”åœ", f"{h27_pct:.1f}%")
                with col3:
                    st.metric(
                        f"{minutes}åˆ†åœ å·®åˆ†",
                        f"{diff:+.1f}%",
                        delta=f"{diff:+.1f}%" if diff != 0 else None,
                        delta_color="normal" if diff >= 0 else "inverse"
                    )

            # Summary
            st.markdown("---")
            st.markdown("### ğŸ“ åˆ†æã‚µãƒãƒª")
            r6_5min_pct = results_r6["covered_5"] / results_r6["geocoded"] * 100
            h27_5min_pct = results_h27["covered_5"] / results_h27["geocoded"] * 100
            r6_10min_pct = results_r6["covered_10"] / results_r6["geocoded"] * 100
            h27_10min_pct = results_h27["covered_10"] / results_h27["geocoded"] * 100

            diff_5 = r6_5min_pct - h27_5min_pct
            diff_10 = r6_10min_pct - h27_10min_pct
            
            st.markdown("""
            **æ³¨æ„**: ã“ã®æ¯”è¼ƒã¯**ç¾åœ¨ã®æ¶ˆé˜²ç½²é…ç½®**ã§ä¸¡å¹´åº¦ã®å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ãŸã‚‚ã®ã§ã™ã€‚
            å®Ÿéš›ã®é…ç½®å¤‰æ›´ï¼ˆR4ã€œR6ã§3éšŠå¢—éšŠï¼‰ã®åŠ¹æœã‚’ç›´æ¥æ¸¬å®šã—ãŸã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
            """)
            
            if diff_5 > 0:
                st.success(f"âœ… 5åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: R6ãŒH27ã‚ˆã‚Š {diff_5:+.1f}% é«˜ã„")
            else:
                st.warning(f"âš ï¸ 5åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: R6ãŒH27ã‚ˆã‚Š {diff_5:+.1f}%")

            if diff_10 > 0:
                st.success(f"âœ… 10åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: R6ãŒH27ã‚ˆã‚Š {diff_10:+.1f}% é«˜ã„")
            else:
                st.warning(f"âš ï¸ 10åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: R6ãŒH27ã‚ˆã‚Š {diff_10:+.1f}%")

            st.info(f"""
            **ãƒ‡ãƒ¼ã‚¿ä»¶æ•°**
            - R6 (2024å¹´): {results_r6['total']:,} ä»¶ (ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results_r6['geocoded']:,} ä»¶)
            - H27 (2015å¹´): {results_h27['total']:,} ä»¶ (ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results_h27['geocoded']:,} ä»¶)
            """)

        else:
            # Single dataset mode
            if "R6" in selected_mode:
                incidents_cov = load_incident_data("R6.xlsx")
                data_label = "R6 (2024å¹´)"
            else:
                incidents_cov = load_incident_data_h27("H27.xls")
                data_label = "H27 (2015å¹´)"

            st.markdown(f"### ğŸ“… {data_label} ã®ã‚«ãƒãƒ¼ç‡åˆ†æ")
            st.write(f"å…¨å‡ºå‹•ä»¶æ•°: {len(incidents_cov):,} ä»¶")

            with st.spinner("ä½æ‰€ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸­..."):
                results = analyze_coverage(incidents_cov, data_label, with_resources=resource_mode)

            if results is None:
                st.error("å‡ºå‹•åœ°ç‚¹ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.stop()

            incident_points = results["incident_points"]
            mapped_cov = results["mapped"]

            geocoded_rate = results["geocoded"] / results["total"] * 100
            st.write(f"ğŸ“ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results['geocoded']:,} / {results['total']:,} ä»¶ ({geocoded_rate:.1f}%)")

            # Calculate coverage for each time threshold
            st.markdown("---")
            st.subheader("ğŸ“Š ã‚«ãƒãƒ¼ç‡çµæœ")

            coverage_results = []
            for minutes in trip_times_cov:
                covered_count = results[f"covered_{minutes}"]
                total_count = results["geocoded"]
                coverage_pct = covered_count / total_count * 100 if total_count > 0 else 0

                coverage_results.append({
                    "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                    "ã‚«ãƒãƒ¼æ•°": covered_count,
                    "å…¨ä»¶æ•°": total_count,
                    "ã‚«ãƒãƒ¼ç‡": f"{coverage_pct:.1f}%",
                })

            if coverage_results:
                coverage_df = pd.DataFrame(coverage_results)
                st.dataframe(coverage_df, width="stretch", hide_index=True)

                # Show metrics
                cols = st.columns(len(coverage_results))
                for i, res in enumerate(coverage_results):
                    with cols[i]:
                        st.metric(
                            label=f"{res['åˆ°é”æ™‚é–“']}åˆ°é”åœ",
                            value=res["ã‚«ãƒãƒ¼ç‡"],
                            delta=f"{res['ã‚«ãƒãƒ¼æ•°']}/{res['å…¨ä»¶æ•°']}ä»¶"
                        )

            # ========== ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã®è¿½åŠ è¡¨ç¤ºï¼ˆå˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼‰ ==========
            if resource_mode:
                st.markdown("---")
                st.subheader("ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ã‚«ãƒãƒ¼ç‡")
                st.caption("å„å‡ºå‹•åœ°ç‚¹ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°ã§åˆ†é¡")
                
                # ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹è¨­å®šã‚’è¡¨ç¤º
                st.info(f"""
                **ä½¿ç”¨ãƒªã‚½ãƒ¼ã‚¹è¨­å®š**: {results.get('resource_config', '')}
                """)
                
                resource_results = []
                for minutes in trip_times_cov:
                    total = results["geocoded"]
                    amb_0 = results.get(f"covered_{minutes}_0amb", 0)
                    amb_1 = results.get(f"covered_{minutes}_1amb", 0)
                    amb_2 = results.get(f"covered_{minutes}_2amb", 0)
                    
                    resource_results.append({
                        "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                        "ğŸ”´ åœå¤– (0å°)": f"{amb_0} ({amb_0/total*100:.1f}%)" if total > 0 else "0",
                        "ğŸŸ¡ 1å°ã®ã¿": f"{amb_1} ({amb_1/total*100:.1f}%)" if total > 0 else "0",
                        "ğŸŸ¢ 2å°ä»¥ä¸Š": f"{amb_2} ({amb_2/total*100:.1f}%)" if total > 0 else "0",
                    })
                
                resource_df = pd.DataFrame(resource_results)
                st.dataframe(resource_df, width="stretch", hide_index=True)
                
                # ãƒªã‚½ãƒ¼ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                for minutes in trip_times_cov:
                    total = results["geocoded"]
                    amb_0 = results.get(f"covered_{minutes}_0amb", 0)
                    amb_1 = results.get(f"covered_{minutes}_1amb", 0)
                    amb_2 = results.get(f"covered_{minutes}_2amb", 0)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(f"{minutes}åˆ† åœå¤–", f"{amb_0/total*100:.1f}%" if total > 0 else "0%", delta_color="inverse")
                    with col2:
                        st.metric(f"{minutes}åˆ† 1å°ã‚«ãƒãƒ¼", f"{amb_1/total*100:.1f}%" if total > 0 else "0%")
                    with col3:
                        st.metric(f"{minutes}åˆ† å†—é•·æ€§ã‚ã‚Š", f"{amb_2/total*100:.1f}%" if total > 0 else "0%")
                
                st.info("""
                **ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…®ã®æ„å‘³**:
                - **åœå¤– (0å°)**: æŒ‡å®šæ™‚é–“å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»ŠãŒãªã„
                - **1å°ã®ã¿**: ã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ãŒã€ãã®1å°ãŒå‡ºå‹•ä¸­ã ã¨å¯¾å¿œä¸å¯
                - **2å°ä»¥ä¸Š**: å†—é•·æ€§ã‚ã‚Šã€‚1å°ãŒå‡ºå‹•ä¸­ã§ã‚‚åˆ¥ã®è»Šä¸¡ã§å¯¾å¿œå¯èƒ½
                """)
            # ================================================

            # Render map with coverage visualization
            st.markdown("---")
            st.subheader("ğŸ—ºï¸ ã‚«ãƒãƒ¼çŠ¶æ³ãƒãƒƒãƒ—")

            center_lat_cov = mapped_cov["lat"].mean()
            center_lon_cov = mapped_cov["lon"].mean()
            fmap_cov = folium.Map(location=[center_lat_cov, center_lon_cov], zoom_start=11, tiles="CartoDB Positron")

            # Add isochrone layers
            color_map_cov = {5: "#ff9e9e", 10: "#8aa5ff"}
            for minutes in trip_times_cov:
                iso_layer = isochrones_cov[isochrones_cov["time"] == minutes]
                if iso_layer.empty:
                    continue
                color = color_map_cov.get(minutes, "#4a4a4a")
                folium.GeoJson(
                    data=iso_layer.__geo_interface__,
                    name=f"{minutes}åˆ†åˆ°é”åœ",
                    style_function=lambda _f, c=color, m=minutes: {
                        "fillColor": c,
                        "color": c,
                        "weight": 1.0,
                        "opacity": 0.5,
                        "fillOpacity": 0.15 if m >= 10 else 0.25,
                    },
                ).add_to(fmap_cov)

            # Add station markers
            for _, row in stations_cov.iterrows():
                folium.CircleMarker(
                    location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
                    radius=6,
                    color="#1f1f1f",
                    weight=2,
                    fill=True,
                    fill_color="#f6bd60",
                    fill_opacity=0.9,
                    popup=f"{row['ç•¥ç§°']}",
                ).add_to(fmap_cov)

            # Add incident markers with coverage status
            for _, row in incident_points.iterrows():
                within_5 = row.get("within_5min", False)
                within_10 = row.get("within_10min", False)

                if within_5:
                    color = "#2ecc71"  # Green - covered by 5min
                    status = "5åˆ†å†…"
                elif within_10:
                    color = "#f39c12"  # Orange - covered by 10min
                    status = "10åˆ†å†…"
                else:
                    color = "#e74c3c"  # Red - not covered
                    status = "åœå¤–"

                label_time = row["è¦šçŸ¥"].strftime("%H:%M") if not pd.isna(row.get("è¦šçŸ¥")) else "--:--"
                popup = f"{status} | {row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time}"
                folium.CircleMarker(
                    location=[row.geometry.y, row.geometry.x],
                    radius=4,
                    color=color,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.8,
                    weight=1.0,
                    popup=popup,
                ).add_to(fmap_cov)

            # Add legend
            legend_html = '''
            <div style="position: fixed; bottom: 50px; left: 50px; z-index: 1000;
                        background-color: white; padding: 10px; border-radius: 5px;
                        border: 2px solid gray; font-size: 12px;">
                <b>å‡ºå‹•åœ°ç‚¹</b><br>
                <span style="color: #2ecc71;">â—</span> 5åˆ†å†…åˆ°é”<br>
                <span style="color: #f39c12;">â—</span> 10åˆ†å†…åˆ°é”<br>
                <span style="color: #e74c3c;">â—</span> åˆ°é”åœå¤–
            </div>
            '''
            fmap_cov.get_root().html.add_child(folium.Element(legend_html))

            folium.LayerControl(collapsed=False).add_to(fmap_cov)
            st.components.v1.html(fmap_cov.get_root().render(), height=720)

            # Show uncovered incidents detail
            if "within_10min" in incident_points.columns:
                uncovered = incident_points[~incident_points["within_10min"]]
                if not uncovered.empty:
                    st.markdown("---")
                    st.subheader(f"âš ï¸ 10åˆ†åˆ°é”åœå¤–ã®å‡ºå‹• ({len(uncovered)} ä»¶)")
                    uncovered_display = uncovered[["date", "è¦šçŸ¥", "å‡ºå‹•å ´æ‰€", "å‡ºå‹•éšŠ"]].copy()
                    uncovered_display.columns = ["æ—¥ä»˜", "è¦šçŸ¥æ™‚åˆ»", "å‡ºå‹•å ´æ‰€", "å‡ºå‹•éšŠ"]
                    st.dataframe(uncovered_display, width="stretch", hide_index=True)

    # ========================================
    # ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹åˆ†æã‚¿ãƒ–
    # ========================================
    with tab_resource:
        st.header("ğŸš‘ ãƒªã‚½ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ")
        st.markdown("""
        å„åœ°ç‚¹ã§ã€Œ**nåˆ†ä»¥å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»ŠãŒä½•å°ã‚ã‚‹ã‹**ã€ã‚’åˆ†æã—ã€
        ãƒªã‚½ãƒ¼ã‚¹é…ç½®ã®æœ€é©åŒ–ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚
        """)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        from misc.coverage_analysis import (
            load_coverage_cache,
            compute_coverage_quality,
            compute_optimization_suggestions,
            load_stations as load_stations_with_resources,
            create_coverage_map,
            STATION_RESOURCES,
        )
        
        cache = load_coverage_cache()
        
        if cache is None:
            st.warning("âš ï¸ ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„ã‹ã€èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆNumPyãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¸æ•´åˆã®å¯èƒ½æ€§ï¼‰ã€‚")
            st.code("python3 misc/coverage_analysis.py", language="bash")
            st.info("ä¸Šè¨˜ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’äº‹å‰è¨ˆç®—ï¼ˆã¾ãŸã¯å†ç”Ÿæˆï¼‰ã—ã¦ãã ã•ã„ã€‚")
        else:
            grid, travel_times = cache
            
            # æ¶ˆé˜²ç½²ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ä»˜ãï¼‰ã‚’èª­ã¿è¾¼ã¿
            stations_res = load_stations_with_resources()
            
            # é–¾å€¤é¸æŠ
            threshold_min = st.selectbox(
                "åˆ°é”æ™‚é–“ã®é–¾å€¤",
                options=[5, 8, 10],
                index=1,
                format_func=lambda x: f"{x}åˆ†ä»¥å†…",
            )
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
            grid = compute_coverage_quality(travel_times, grid, [5, 8, 10])
            col = f"ambulances_{threshold_min}min"
            
            # çµ±è¨ˆè¡¨ç¤º
            st.subheader("ğŸ“Š ç¾çŠ¶ã®ã‚«ãƒãƒ¬ãƒƒã‚¸çŠ¶æ³")
            
            col1, col2, col3, col4 = st.columns(4)
            total_points = len(grid)
            zero_cov = (grid[col] == 0).sum()
            single_cov = (grid[col] == 1).sum()
            multi_cov = (grid[col] >= 2).sum()
            
            with col1:
                st.metric("åˆ†æãƒã‚¤ãƒ³ãƒˆæ•°", f"{total_points:,}")
            with col2:
                st.metric("ã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—", f"{zero_cov:,}", delta=f"{zero_cov/total_points*100:.1f}%", delta_color="inverse")
            with col3:
                st.metric("1å°ã®ã¿", f"{single_cov:,}", delta=f"{single_cov/total_points*100:.1f}%", delta_color="off")
            with col4:
                st.metric("2å°ä»¥ä¸Š", f"{multi_cov:,}", delta=f"{multi_cov/total_points*100:.1f}%", delta_color="normal")
            
            # ãƒªã‚½ãƒ¼ã‚¹é…ç½®è¡¨ç¤º
            st.subheader("ğŸ¥ æ¶ˆé˜²ç½²åˆ¥ãƒªã‚½ãƒ¼ã‚¹é…ç½®")
            resource_df = stations_res[["ç•¥ç§°", "æ•‘æ€¥è»Šå°æ•°", "åŒºåˆ†"]].copy()
            resource_df.columns = ["æ¶ˆé˜²ç½²", "æ•‘æ€¥è»Šå°æ•°", "åŒºåˆ†"]
            st.dataframe(resource_df, width="stretch", hide_index=True)
            st.caption(f"åˆè¨ˆ: {stations_res['æ•‘æ€¥è»Šå°æ•°'].sum()}å°")
            
            # æœ€é©åŒ–ææ¡ˆ
            st.subheader("ğŸ’¡ ãƒªã‚½ãƒ¼ã‚¹é…ç½® æœ€é©åŒ–ææ¡ˆ")
            suggestions = compute_optimization_suggestions(
                grid, stations_res, travel_times, target_threshold_min=threshold_min
            )
            
            # å¼±ç‚¹ã‚¨ãƒªã‚¢
            if suggestions["weak_areas"]:
                st.markdown("**âš ï¸ å¼±ç‚¹ã‚¨ãƒªã‚¢:**")
                for area in suggestions["weak_areas"]:
                    if area["severity"] == "é«˜":
                        st.error(f"ğŸ”´ {area['type']}: {area['count']}ãƒã‚¤ãƒ³ãƒˆ")
                    else:
                        st.warning(f"ğŸŸ¡ {area['type']}: {area['count']}ãƒã‚¤ãƒ³ãƒˆ")
            
            # å¢—å¼·æ¨å¥¨
            st.markdown("**ğŸ“ˆ æ•‘æ€¥è»Š1å°è¿½åŠ æ™‚ã®æ”¹å–„åŠ¹æœï¼ˆä¸Šä½5ç½²ï¼‰:**")
            for i, s in enumerate(suggestions["suggestions"], 1):
                improvement_score = s["total_improvement"]
                emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
                st.markdown(
                    f"{emoji} **{i}. {s['station_name']}** (ç¾{s['current_ambulances']}å°) "
                    f"â†’ æ–°è¦ã‚«ãƒãƒ¼: {s['newly_covered_points']}pt, å†—é•·æ€§è¿½åŠ : {s['redundancy_improved_points']}pt"
                )
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤º
            st.subheader("ğŸ—ºï¸ ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒãƒƒãƒ—")
            st.markdown(f"**{threshold_min}åˆ†ä»¥å†…ã«åˆ°é”å¯èƒ½ãªæ•‘æ€¥è»Šå°æ•°**")
            
            with st.spinner("ãƒãƒƒãƒ—ç”Ÿæˆä¸­..."):
                coverage_map = create_coverage_map(grid, stations_res, threshold_min)
                st.components.v1.html(coverage_map.get_root().render(), height=600)
            
            st.markdown("---")
            st.caption("""
            **å‡¡ä¾‹**: ğŸ”´ 0å°ï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ãªã—ï¼‰, ğŸŸ  1å°ï¼ˆå†—é•·æ€§ãªã—ï¼‰, ğŸŸ¡ 2å°, ğŸŸ¢ 3å°ä»¥ä¸Š  
            **ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: R6å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å„æ¶ˆé˜²ç½²ã®æ•‘æ€¥è»Šå°æ•°ã‚’æ¨å®š
            """)

    # ========================================
    # â­ é…ç½®æœ€é©åŒ–ã‚¿ãƒ–
    # ========================================
    with tab_optimize:
        st.header("â­ ãƒªã‚½ãƒ¼ã‚¹è€ƒæ…® é…ç½®æœ€é©åŒ–")
        st.markdown("""
        å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ã¨æ—¢å­˜ãƒªã‚½ãƒ¼ã‚¹ã‚’åˆ†æã—ã€**æ–°è¦æ¶ˆé˜²ç½²ã®æœ€é©ãªå€™è£œåœ°ç‚¹**ã‚’è‡ªå‹•ã§ææ¡ˆã—ã¾ã™ã€‚
        
        **ç‰¹å¾´:**
        - ğŸ“ å‡ºå‹•å¯†åº¦ã¨ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ã‹ã‚‰å€™è£œåœ°ç‚¹ã‚’è‡ªå‹•æŠ½å‡º
        - ğŸš‘ æ—¢å­˜ã®æ•‘æ€¥è»Šé…ç½®ã‚’è€ƒæ…®ã—ãŸã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        - âš¡ é«˜é€Ÿãªè²ªæ¬²æ³•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆãƒ‡ãƒ¢å‘ã‘ï¼‰
        """)
        
        # ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé…å»¶èª­ã¿è¾¼ã¿ï¼‰
        try:
            from optimization import (
                load_stations as opt_load_stations,
                load_incident_locations,
                generate_candidate_locations,
                optimize_placement,
                create_optimization_map,
                CandidateLocation,
                load_candidates_cache,
                save_candidates_cache,
            )
            optimization_available = True
        except ImportError as e:
            optimization_available = False
            st.error(f"æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        
        if optimization_available:
            # è¨­å®š
            st.subheader("âš™ï¸ æœ€é©åŒ–è¨­å®š")
            
            col_set1, col_set2, col_set3 = st.columns(3)
            with col_set1:
                n_candidates = st.slider(
                    "å€™è£œåœ°ç‚¹æ•°",
                    min_value=5,
                    max_value=20,
                    value=10,
                    help="ç”Ÿæˆã™ã‚‹å€™è£œåœ°ç‚¹ã®æ•°ã€‚å¤šã„ã»ã©ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã™ãŒè¨ˆç®—æ™‚é–“ãŒå¢—ãˆã¾ã™ã€‚"
                )
            with col_set2:
                threshold_min_opt = st.selectbox(
                    "åˆ°é”æ™‚é–“ã®é–¾å€¤",
                    options=[5, 8, 10],
                    index=1,
                    format_func=lambda x: f"{x}åˆ†ä»¥å†…",
                    help="ã“ã®æ™‚é–“å†…ã«åˆ°é”ã§ãã‚‹ã“ã¨ã‚’ç›®æ¨™ã¨ã—ã¾ã™ã€‚"
                )
            with col_set3:
                new_ambulances = st.number_input(
                    "æ–°è¦æ¶ˆé˜²ç½²ã®æ•‘æ€¥è»Šå°æ•°",
                    min_value=1,
                    max_value=5,
                    value=2,
                    help="æ–°ã—ãè¨­ç½®ã™ã‚‹æ¶ˆé˜²ç½²ã«é…å‚™ã™ã‚‹æ•‘æ€¥è»Šã®å°æ•°ã€‚"
                )
            
            use_cache = st.checkbox("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ï¼ˆé«˜é€ŸåŒ–ï¼‰", value=True, help="ä»¥å‰ã®è¨ˆç®—çµæœã‚’å†åˆ©ç”¨ã—ã¾ã™ã€‚")
            
            # å®Ÿè¡Œãƒœã‚¿ãƒ³
            st.markdown("---")
            col_btn1, col_btn2 = st.columns([1, 3])
            with col_btn1:
                run_optimization = st.button("ğŸš€ æœ€é©åŒ–ã‚’å®Ÿè¡Œ", type="primary")
            with col_btn2:
                if st.button("ğŸ—‘ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", type="secondary"):
                    from optimization import OPTIMIZATION_CACHE_PATH
                    if OPTIMIZATION_CACHE_PATH.exists():
                        OPTIMIZATION_CACHE_PATH.unlink()
                        st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                        st.rerun()
            
            if run_optimization:
                st.markdown("---")
                
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    stations_opt = opt_load_stations()
                    incidents_opt = load_incident_locations()
                
                if incidents_opt.empty:
                    st.error("å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                    st.code("python scripts/precompute_incident_geocode.py", language="bash")
                    st.stop()
                
                st.info(f"ğŸ“Š åˆ†æå¯¾è±¡: æ¶ˆé˜²ç½² {len(stations_opt)}ç®‡æ‰€, å‡ºå‹•åœ°ç‚¹ {len(incidents_opt):,}ä»¶")
                
                # å€™è£œåœ°ç‚¹ç”Ÿæˆ
                st.subheader("ğŸ“ å€™è£œåœ°ç‚¹ã®æŠ½å‡º")
                
                candidates = None
                if use_cache:
                    candidates = load_candidates_cache()
                    if candidates:
                        st.success(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(candidates)}å€™è£œã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                
                if candidates is None:
                    with st.spinner("å€™è£œåœ°ç‚¹ã‚’ç”Ÿæˆä¸­..."):
                        prog_cand = st.progress(0)
                        candidates = generate_candidate_locations(
                            stations_opt, incidents_opt,
                            n_candidates=n_candidates,
                            progress_cb=lambda p: prog_cand.progress(int(p * 100)),
                        )
                        save_candidates_cache(candidates)
                        st.success(f"âœ… {len(candidates)}å€™è£œåœ°ç‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
                
                # å€™è£œä¸€è¦§è¡¨ç¤º
                if candidates:
                    candidate_data = []
                    for i, c in enumerate(candidates, 1):
                        candidate_data.append({
                            "é †ä½": i,
                            "ç·¯åº¦": f"{c.lat:.5f}",
                            "çµŒåº¦": f"{c.lon:.5f}",
                            "ç†ç”±": c.reason,
                            "å‡ºå‹•å¯†åº¦": f"{c.incident_density:.2f}",
                            "ã‚®ãƒ£ãƒƒãƒ—": f"{c.current_coverage_gap:.2f}",
                            "ã‚¹ã‚³ã‚¢": f"{c.priority_score:.3f}",
                        })
                    
                    with st.expander(f"ğŸ“‹ å€™è£œåœ°ç‚¹ä¸€è¦§ï¼ˆ{len(candidates)}ä»¶ï¼‰", expanded=False):
                        st.dataframe(pd.DataFrame(candidate_data), width="stretch", hide_index=True)
                
                # æœ€é©åŒ–å®Ÿè¡Œ
                st.subheader("ğŸ¯ æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
                
                with st.spinner("ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­..."):
                    prog_opt = st.progress(0)
                    result = optimize_placement(
                        stations_opt, incidents_opt, candidates,
                        threshold_min=threshold_min_opt,
                        new_ambulances=new_ambulances,
                        progress_cb=lambda p: prog_opt.progress(int(p * 100)),
                    )
                
                st.success(f"âœ… æœ€é©åŒ–å®Œäº†ï¼ˆ{result.computation_time_sec:.2f}ç§’ï¼‰")
                
                # çµæœè¡¨ç¤º
                st.markdown("---")
                st.subheader("â­ æœ€é©åŒ–çµæœ")
                
                if result.best_location:
                    col_res1, col_res2 = st.columns([1, 1])
                    
                    with col_res1:
                        st.markdown("### ğŸ† æœ€é©å€™è£œåœ°ç‚¹")
                        st.markdown(f"""
                        - **ä½ç½®**: ({result.best_location['lat']:.5f}, {result.best_location['lon']:.5f})
                        - **é¸å®šç†ç”±**: {result.best_location['reason']}
                        - **æ–°è¦ã‚«ãƒãƒ¼ä»¶æ•°**: {result.best_location['newly_covered_incidents']:,}ä»¶
                        - **åŠ¹ç‡ã‚¹ã‚³ã‚¢**: {result.best_location['efficiency_score']:.1f}ä»¶/å°
                        """)
                        
                        # Google Maps ãƒªãƒ³ã‚¯
                        gmap_url = f"https://www.google.com/maps?q={result.best_location['lat']},{result.best_location['lon']}"
                        st.markdown(f"[ğŸ“ Google Mapsã§é–‹ã]({gmap_url})")
                    
                    with col_res2:
                        st.markdown("### ğŸ“ˆ æ”¹å–„åŠ¹æœ")
                        st.metric(
                            "æ–°è¦ã‚«ãƒãƒ¼ä»¶æ•°",
                            f"{result.best_location['newly_covered_incidents']:,}ä»¶",
                            delta=f"+{result.best_location['efficiency_score']:.1f}ä»¶/æ•‘æ€¥è»Š1å°"
                        )
                        st.metric(
                            "æŠ•å…¥ãƒªã‚½ãƒ¼ã‚¹",
                            f"æ•‘æ€¥è»Š {new_ambulances}å°",
                        )
                        st.info(f"""
                        **ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡**: æ•‘æ€¥è»Š1å°ã‚ãŸã‚Šç´„{result.best_location['efficiency_score']:.0f}ä»¶ã®
                        å‡ºå‹•ã‚’ã‚«ãƒãƒ¼å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
                        """)
                
                else:
                    st.warning("æœ€é©ãªå€™è£œåœ°ç‚¹ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                # ãƒãƒƒãƒ—è¡¨ç¤º
                st.subheader("ğŸ—ºï¸ æœ€é©åŒ–çµæœãƒãƒƒãƒ—")
                
                with st.spinner("ãƒãƒƒãƒ—ç”Ÿæˆä¸­..."):
                    opt_map = create_optimization_map(
                        stations_opt, incidents_opt, candidates, result.best_location
                    )
                    st.components.v1.html(opt_map.get_root().render(), height=600)
                
                st.caption("""
                **å‡¡ä¾‹**: ğŸ“é’=æ—¢å­˜æ¶ˆé˜²ç½², â­èµ¤=æœ€é©å€™è£œåœ°ç‚¹, â—æ©™=ãã®ä»–å€™è£œ, ãƒ»ç°=å‡ºå‹•åœ°ç‚¹  
                **èµ¤ã„å††**: 8åˆ†åˆ°é”åœï¼ˆæ¦‚ç®—ï¼‰
                """)
                
                # è©³ç´°çµæœ
                with st.expander("ğŸ“Š è©³ç´°åˆ†æãƒ‡ãƒ¼ã‚¿", expanded=False):
                    st.markdown("**ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„:**")
                    st.json(result.coverage_improvement)
                    
                    st.markdown("**ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡:**")
                    st.json(result.resource_efficiency)
                    
                    st.markdown("**å…¨å€™è£œåœ°ç‚¹ã‚¹ã‚³ã‚¢:**")
                    all_candidates_df = pd.DataFrame(result.candidate_locations)
                    st.dataframe(all_candidates_df, width="stretch")
            
            else:
                # å®Ÿè¡Œå‰ã®èª¬æ˜
                st.markdown("---")
                st.info("""
                **ä½¿ã„æ–¹:**
                1. ä¸Šè¨˜ã®è¨­å®šã‚’èª¿æ•´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚‚OKï¼‰
                2. ã€ŒğŸš€ æœ€é©åŒ–ã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
                3. å€™è£œåœ°ç‚¹ã®ç”Ÿæˆ â†’ ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ â†’ çµæœè¡¨ç¤º
                
                **æ‰€è¦æ™‚é–“**: ç´„10ã€œ30ç§’ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨æ™‚ã¯æ•°ç§’ï¼‰
                """)
                
                # ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹é…ç½®ã‚’è¡¨ç¤º
                st.subheader("ğŸ“Š ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹é…ç½®")
                try:
                    stations_preview = opt_load_stations()
                    preview_df = stations_preview[["ç•¥ç§°", "æ•‘æ€¥è»Šå°æ•°", "åŒºåˆ†"]].copy()
                    preview_df.columns = ["æ¶ˆé˜²ç½²", "æ•‘æ€¥è»Šå°æ•°", "åŒºåˆ†"]
                    
                    col_p1, col_p2 = st.columns([2, 1])
                    with col_p1:
                        st.dataframe(preview_df, width="stretch", hide_index=True)
                    with col_p2:
                        st.metric("ç·æ•‘æ€¥è»Šå°æ•°", f"{preview_df['æ•‘æ€¥è»Šå°æ•°'].sum()}å°")
                        st.metric("æ¶ˆé˜²ç½²æ•°", f"{len(preview_df)}ç®‡æ‰€")
                except Exception as e:
                    st.warning(f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºã«å¤±æ•—: {e}")

    st.info("ã‚¢ãƒ—ãƒªã‚’çµ‚äº†ã™ã‚‹ã«ã¯ã€å®Ÿè¡Œä¸­ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
