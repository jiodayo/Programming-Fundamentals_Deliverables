"""Streamlit app that visualizes EMS isochrones on a Google-like map.

$ streamlit run app.py ã§å®Ÿè¡Œ
"""

from __future__ import annotations

import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from pathlib import Path
from typing import Iterable, Callable
import re

import folium
import geopandas as gpd
import networkx as nx
import osmnx as ox
import pandas as pd
import streamlit as st
from shapely.geometry import Point
from shapely.geometry import MultiPoint

ox.settings.use_cache = True
GRAPHML_PATH = Path("cache/ehime_drive.graphml")
GRAPHML_PATH.parent.mkdir(parents=True, exist_ok=True)
STATIONS_DB_PATH = Path("map.sqlite")
ISOCHRONE_CACHE_PATH = Path("cache/isochrones.parquet")
GEOCODE_CACHE_PATH = Path("cache/incident_geocode.parquet")


def graph_data_version() -> float:
    """Return a timestamp that reflects the cached graph version."""
    return GRAPHML_PATH.stat().st_mtime if GRAPHML_PATH.exists() else 0.0


def station_data_version(db_path: Path = STATIONS_DB_PATH, excel_path: str = "map.xlsx") -> float:
    """Return mtime of the current station datasource for cache invalidation."""
    if db_path.exists():
        return db_path.stat().st_mtime
    return Path(excel_path).stat().st_mtime if Path(excel_path).exists() else 0.0


@st.cache_data(show_spinner=False)
def load_incident_data(excel_path: str = "R6.xlsx") -> pd.DataFrame:
    """Load incident records; keeps only rows with a valid ç™ºç”Ÿæ—¥æ™‚."""
    if not Path(excel_path).exists():
        raise FileNotFoundError(excel_path)
    df = pd.read_excel(excel_path)
    df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
    df = df[df["è¦šçŸ¥"].notna()].copy()
    df["date"] = df["è¦šçŸ¥"].dt.date
    return df


def _load_geocode_cache(path: Path = GEOCODE_CACHE_PATH) -> pd.DataFrame:
    if path.exists():
        try:
            return pd.read_parquet(path)
        except Exception:
            return pd.DataFrame(columns=["address", "lat", "lon"])
    return pd.DataFrame(columns=["address", "lat", "lon"])


def _save_geocode_cache(df: pd.DataFrame, path: Path = GEOCODE_CACHE_PATH) -> None:
    try:
        df.to_parquet(path, index=False)
    except Exception:
        pass


def geocode_addresses(addresses: list[str], region_prefix: str = "æ„›åª›çœŒ") -> pd.DataFrame:
    """Geocode addresses with osmnx + Nominatim and persist results locally."""
    cache = _load_geocode_cache().copy()
    seen = set(cache["address"].tolist())
    missing = [a for a in addresses if a not in seen]

    new_records: list[dict] = []
    for addr in missing:
        query = f"{region_prefix} {addr}" if region_prefix else addr
        try:
            lat, lon = ox.geocode(query)
            new_records.append({"address": addr, "lat": lat, "lon": lon})
        except Exception:
            new_records.append({"address": addr, "lat": None, "lon": None})

    if new_records:
        cache = pd.concat([cache, pd.DataFrame(new_records)], ignore_index=True)
        cache = cache.drop_duplicates(subset=["address"], keep="last")
        _save_geocode_cache(cache)

    return cache[cache["address"].isin(addresses)].copy()


@st.cache_data(show_spinner=False)
def load_station_data(
    db_path: Path,
    excel_path: str,
    source_mtime: float,
) -> gpd.GeoDataFrame:
    """Load station records from SQLite when available, otherwise fallback to Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            df = pd.read_sql("SELECT * FROM stations", conn)
    else:
        df = pd.read_excel(excel_path)

    geometry = gpd.points_from_xy(df["çµŒåº¦"], df["ç·¯åº¦"])
    return gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")


@st.cache_resource(show_spinner=False)
def load_graph_cached(bbox: tuple[float, float, float, float]) -> nx.MultiDiGraph:
    north, south, east, west = bbox
    if GRAPHML_PATH.exists():
        graph = ox.load_graphml(GRAPHML_PATH)
    else:
        print("é“è·¯ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...ï¼ˆåˆå›å–å¾—ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
        try:
            graph = ox.graph_from_bbox(bbox=bbox, network_type="drive")
        except ValueError as exc:
            if "no graph nodes" not in str(exc).lower():
                raise
            graph = ox.graph_from_place("Ehime, Japan", network_type="drive")
        ox.save_graphml(graph, GRAPHML_PATH)

    if "travel_time" not in next(iter(graph.edges(data=True)))[2]:
        graph = ox.add_edge_speeds(graph, hwy_speeds={
            "residential": 30,
            "secondary": 40,
            "tertiary": 40,
            "primary": 50,
            "motorway": 80,
        })
        graph = ox.add_edge_travel_times(graph)
        ox.save_graphml(graph, GRAPHML_PATH)

    return graph


def compute_isochrones(
    graph: nx.MultiDiGraph,
    stations: gpd.GeoDataFrame,
    trip_times: Iterable[int],
    progress_cb: Callable[[float], None] | None = None,
) -> gpd.GeoDataFrame:
    records: list[dict] = []

    # Vectorized nearest-node lookup to avoid per-row KDTree rebuilds
    xs = stations["çµŒåº¦"].to_list()
    ys = stations["ç·¯åº¦"].to_list()
    try:
        center_nodes = ox.distance.nearest_nodes(graph, xs, ys)
    except Exception:
        # Fallback to per-point lookup if vectorized call fails
        center_nodes = [ox.distance.nearest_nodes(graph, x, y) for x, y in zip(xs, ys)]

    total = len(center_nodes)

    # Pre-extract node coordinates to avoid repeated attribute lookups
    node_xy = {n: (data["x"], data["y"]) for n, data in graph.nodes(data=True)}
    trip_times_sorted = sorted(trip_times)
    max_radius = trip_times_sorted[-1] * 60 if trip_times_sorted else 0

    def _one_station(payload: tuple[int, tuple]) -> list[dict]:
        _idx, (row, center_node) = payload
        out: list[dict] = []

        # Single-source Dijkstra once up to the maximum requested time
        lengths = nx.single_source_dijkstra_path_length(
            graph,
            center_node,
            cutoff=max_radius,
            weight="travel_time",
        )

        for minutes in trip_times_sorted:
            cutoff = minutes * 60
            reachable_nodes = [nid for nid, dist in lengths.items() if dist <= cutoff]
            if not reachable_nodes:
                continue
            points = [node_xy[nid] for nid in reachable_nodes if nid in node_xy]
            if not points:
                continue
            hull = MultiPoint(points).convex_hull
            out.append({"name": row.ç•¥ç§°, "time": minutes, "geometry": hull})

        return out

    with ThreadPoolExecutor(max_workers=min(8, max(2, os.cpu_count() or 2))) as ex:  # type: ignore[name-defined]
        futures = [
            ex.submit(_one_station, payload)
            for payload in enumerate(zip(stations.itertuples(index=False), center_nodes))
        ]
        completed = 0
        for fut in as_completed(futures):
            records.extend(fut.result())
            completed += 1
            if progress_cb:
                progress_cb(completed / total)

    if not records:
        raise RuntimeError("åˆ°é”åœãƒãƒªã‚´ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    return gpd.GeoDataFrame(records, crs="EPSG:4326")


@st.cache_data(show_spinner=False)
def precompute_isochrones(
    station_df: pd.DataFrame,
    trip_times: tuple[int, ...],
    graph_version: float,
) -> gpd.GeoDataFrame:
    """Cache-intensive isochrone computation so UI updates stay responsive."""
    geometry = gpd.points_from_xy(station_df["çµŒåº¦"], station_df["ç·¯åº¦"])
    stations = gpd.GeoDataFrame(station_df, geometry=geometry, crs="EPSG:4326")

    graph = ox.load_graphml(GRAPHML_PATH)
    return compute_isochrones(graph, stations, trip_times)


@st.cache_data(show_spinner=False)
def load_precomputed_isochrones(path: Path) -> gpd.GeoDataFrame:
    if not path.exists():
        raise FileNotFoundError(path)
    return gpd.read_parquet(path)


def append_virtual_stations(base: gpd.GeoDataFrame, virtuals: list[dict]) -> gpd.GeoDataFrame:
    """Append in-session virtual stations (lat/lon) to the loaded stations."""
    if not virtuals:
        return base

    df_new = pd.DataFrame(virtuals)
    geom = gpd.points_from_xy(df_new["çµŒåº¦"], df_new["ç·¯åº¦"])
    gdf_new = gpd.GeoDataFrame(df_new, geometry=geom, crs="EPSG:4326")

    # Ensure all columns exist and order matches base
    for col in base.columns:
        if col not in gdf_new.columns:
            gdf_new[col] = None
    gdf_new = gdf_new[base.columns]

    return pd.concat([base, gdf_new], ignore_index=True)


def render_map_html(
    isochrones: gpd.GeoDataFrame,
    stations: gpd.GeoDataFrame,
    tiles: str = "CartoDB Positron",
) -> str:
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles=tiles)

    # Softer palette for better visibility when overlapping
    color_map = {5: "#ff9e9e", 10: "#8aa5ff", 15: "#7dd8c6", 20: "#f7caa0"}
    for minutes in sorted({*isochrones["time"]}):
        layer = isochrones[isochrones["time"] == minutes]
        if layer.empty:
            continue
        color = color_map.get(minutes, "#4a4a4a")
        folium.GeoJson(
            data=layer.__geo_interface__,
            name=f"{minutes}åˆ†åœ",
            style_function=lambda _feature, c=color, m=minutes: {
                "fillColor": c,
                "color": c,
                "weight": 1.0,
                "opacity": 0.6,
                "fillOpacity": 0.18 if m >= 10 else 0.30,
            },
            tooltip=folium.GeoJsonTooltip(fields=["name", "time"], aliases=["æ‹ ç‚¹", "åˆ°é”æ™‚é–“(åˆ†)"])
        ).add_to(fmap)

    for _, row in stations.iterrows():
        folium.CircleMarker(
            location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
            radius=7,
            color="#1f1f1f",
            weight=2,
            fill=True,
            fill_color="#f6bd60",
            fill_opacity=0.9,
            popup=f"{row['ç•¥ç§°']}",
        ).add_to(fmap)

    folium.LayerControl(collapsed=False).add_to(fmap)
    return fmap.get_root().render()


def main() -> None:
    st.set_page_config(page_title="æ„›åª›æ•‘æ€¥è»Š åˆ°é”åœãƒ“ãƒ¥ãƒ¼ã‚¢", layout="wide")
    st.title("ğŸš‘ æ„›åª›çœŒ æ•‘æ€¥è»Šãƒ“ãƒ¥ãƒ¼ã‚¢")
    st.caption("map.xlsx ã§æ‹ ç‚¹åˆ°é”åœã€R6.xlsx ã§å‡ºå‹•åœ°ç‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    stations = load_station_data(
        db_path=STATIONS_DB_PATH,
        excel_path="map.xlsx",
        source_mtime=station_data_version(),
    )

    if "virtual_stations" not in st.session_state:
        st.session_state["virtual_stations"] = []

    tab_iso, tab_inc = st.tabs(["åˆ°é”åœ", "å‡ºå‹•åœ°ç‚¹ (R6)" ])

    with tab_iso:
        with st.expander("ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ ï¼ˆã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ï¼‰"):
            with st.form("virtual_station_form"):
                default_name = f"ä»®æƒ³ç½²{len(st.session_state['virtual_stations']) + 1}"
                v_name = st.text_input("ç•¥ç§°", value=default_name)
                v_lat = st.number_input("ç·¯åº¦", value=float(stations["ç·¯åº¦"].mean()))
                v_lon = st.number_input("çµŒåº¦", value=float(stations["çµŒåº¦"].mean()))
                submitted = st.form_submit_button("è¿½åŠ ")
                if submitted:
                    st.session_state["virtual_stations"].append({
                        "ç•¥ç§°": v_name.strip() or default_name,
                        "ç·¯åº¦": v_lat,
                        "çµŒåº¦": v_lon,
                    })
                    st.success(f"ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ : {v_name}")
            if st.button("ä»®æƒ³æ¶ˆé˜²ç½²ã‚’ã‚¯ãƒªã‚¢", type="secondary"):
                st.session_state["virtual_stations"] = []
                st.info("ä»®æƒ³æ¶ˆé˜²ç½²ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

        has_virtual = bool(st.session_state["virtual_stations"])
        stations_view = append_virtual_stations(stations, st.session_state["virtual_stations"])
        station_names = sorted(stations_view["ç•¥ç§°"].unique())
        trip_options = [5, 10, 15, 20]

        col_left, col_right = st.columns([2, 1])
        with col_left:
            selected_names = st.multiselect(
                "è¡¨ç¤ºã™ã‚‹æ¶ˆé˜²ç½²",
                options=station_names,
                default=station_names,
                help="è¤‡æ•°é¸æŠã§åˆ°é”åœã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚",
            )
        with col_right:
            selected_times = st.multiselect(
                "åˆ°é”æ™‚é–“ (åˆ†)",
                options=trip_options,
                default=[5, 10],
            )

        if not selected_names:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®æ¶ˆé˜²ç½²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        if not selected_times:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®åˆ°é”æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        filtered = stations_view[stations_view["ç•¥ç§°"].isin(selected_names)].copy()

        padding_deg = 0.1
        west_all, south_all, east_all, north_all = stations_view.total_bounds
        bbox = (north_all + padding_deg, south_all - padding_deg, east_all + padding_deg, west_all - padding_deg)

        with st.spinner("é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            graph = load_graph_cached(bbox)

        if ISOCHRONE_CACHE_PATH.exists() and not has_virtual:
            try:
                with st.spinner("åˆ°é”åœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    all_isochrones = load_precomputed_isochrones(ISOCHRONE_CACHE_PATH)
                display_isochrones = all_isochrones[
                    (all_isochrones["name"].isin(selected_names)) &
                    (all_isochrones["time"].isin(selected_times))
                ].copy()
            except Exception as exc:
                st.warning(f"äº‹å‰è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚å†è¨ˆç®—ã—ã¾ã™: {exc}")
                display_isochrones = None
        else:
            display_isochrones = None

        if display_isochrones is None:
            with st.spinner("åˆ°é”åœã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™..."):
                prog = st.progress(0)
                display_isochrones = compute_isochrones(
                    graph=graph,
                    stations=filtered,
                    trip_times=selected_times,
                    progress_cb=lambda p: prog.progress(int(p * 100)),
                )

        if display_isochrones.empty:
            st.error("é¸æŠæ¡ä»¶ã«åˆè‡´ã™ã‚‹åˆ°é”åœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        html_map = render_map_html(display_isochrones, filtered)
        st.components.v1.html(html_map, height=720)

    with tab_inc:
        try:
            incidents = load_incident_data("R6.xlsx")
        except FileNotFoundError:
            st.error("R6.xlsx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        date_options = sorted(incidents["date"].unique())
        if not date_options:
            st.warning("R6.xlsx ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        default_date = date_options[0]
        selected_date = st.selectbox(
            "è¡¨ç¤ºã™ã‚‹æ—¥ä»˜ (è¦šçŸ¥æ—¥)",
            options=date_options,
            format_func=lambda d: d.strftime("%Y-%m-%d"),
            index=0,
        )

        day_inc = incidents[incidents["date"] == selected_date].copy()
        addr_series = day_inc["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
        addr_unique = sorted(addr_series.unique())

        st.write(f"{selected_date} ã®å‡ºå‹•ä»¶æ•°: {len(day_inc)} ä»¶ (ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°ç‚¹ {len(addr_unique)} ç®‡æ‰€)")

        with st.spinner("ä½æ‰€ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ã¾ã™ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ..."):
            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")

        merged = day_inc.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
        mapped = merged.dropna(subset=["lat", "lon"]).copy()
        missing_count = len(day_inc) - len(mapped)

        if mapped.empty:
            st.error("ã“ã®æ—¥ã®åœ°ç‚¹ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        st.write(f"åœ°å›³ã«ãƒ—ãƒ­ãƒƒãƒˆã§ããŸä»¶æ•°: {len(mapped)} / {len(day_inc)} (æœªç‰¹å®š {missing_count} ä»¶)")

        center_lat = mapped["lat"].mean()
        center_lon = mapped["lon"].mean()
        fmap = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles="CartoDB Positron")

        # Softer color by weekday to help visually group clusters
        weekday_colors = {
            "æœˆ": "#f94144",
            "ç«": "#f3722c",
            "æ°´": "#f9c74f",
            "æœ¨": "#90be6d",
            "é‡‘": "#43aa8b",
            "åœŸ": "#577590",
            "æ—¥": "#9d4edd",
        }

        for _, row in mapped.iterrows():
            wk = str(row.get("æ›œæ—¥", "?"))
            color = weekday_colors.get(wk, "#4a4a4a")
            label_time = row["è¦šçŸ¥"].strftime("%H:%M") if not pd.isna(row.get("è¦šçŸ¥")) else "--:--"
            popup = f"{row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time} | {row.get('æ¬é€åŒºåˆ†(äº‹æ¡ˆ)', '')}"
            folium.CircleMarker(
                location=[row["lat"], row["lon"]],
                radius=5,
                color=color,
                fill=True,
                fill_color=color,
                fill_opacity=0.85,
                weight=1.0,
                popup=popup,
            ).add_to(fmap)

        st.components.v1.html(fmap.get_root().render(), height=720)

    st.info("ã‚¢ãƒ—ãƒªã‚’çµ‚äº†ã™ã‚‹ã«ã¯ã€å®Ÿè¡Œä¸­ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
