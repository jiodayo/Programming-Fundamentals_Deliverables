"""Streamlit app that visualizes EMS isochrones on a Google-like map.

$ streamlit run app.py ã§å®Ÿè¡Œ
"""

from __future__ import annotations

import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
import os
from pathlib import Path
from typing import Iterable, Callable
import re

import folium
import geopandas as gpd
import networkx as nx
import osmnx as ox
import pandas as pd
import streamlit as st
from streamlit_folium import st_folium
from shapely.geometry import Point
from shapely.geometry import MultiPoint
from shapely.ops import unary_union

# Traffic-aware isochrones
from traffic_analysis import (
    load_delay_factors,
    get_delay_factor,
    TIME_SLOT_LABELS,
    DOW_LABELS,
    DELAY_FACTORS_PATH,
)

ox.settings.use_cache = True
GRAPHML_PATH = Path("cache/matsuyama_drive.graphml")
GRAPHML_PATH.parent.mkdir(parents=True, exist_ok=True)
STATIONS_DB_PATH = Path("map.sqlite")
INCIDENTS_DB_PATH = Path("incidents.sqlite")
ISOCHRONE_CACHE_PATH = Path("cache/isochrones.parquet")
GEOCODE_CACHE_PATH = Path("cache/incident_geocode.parquet")


def graph_data_version() -> float:
    """Return a timestamp that reflects the cached graph version."""
    return GRAPHML_PATH.stat().st_mtime if GRAPHML_PATH.exists() else 0.0


def station_data_version(db_path: Path = STATIONS_DB_PATH, excel_path: str = "map.xlsx") -> float:
    """Return mtime of the current station datasource for cache invalidation."""
    if db_path.exists():
        return db_path.stat().st_mtime
    return Path(excel_path).stat().st_mtime if Path(excel_path).exists() else 0.0


@st.cache_data(show_spinner=False)
def load_incident_data(excel_path: str = "R6.xlsx", db_path: Path = INCIDENTS_DB_PATH) -> pd.DataFrame:
    """Load R6 incident records from SQLite if available, otherwise from Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            # Check if table exists
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='incidents_r6'")
            if cursor.fetchone():
                df = pd.read_sql("SELECT * FROM incidents_r6", conn)
                df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
                df["date"] = pd.to_datetime(df["date"]).dt.date
                return df
    
    # Fallback to Excel
    if not Path(excel_path).exists():
        raise FileNotFoundError(excel_path)
    df = pd.read_excel(excel_path)
    df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
    df = df[df["è¦šçŸ¥"].notna()].copy()
    df["date"] = df["è¦šçŸ¥"].dt.date
    return df


@st.cache_data(show_spinner=False)
def load_incident_data_h27(excel_path: str = "H27.xls", db_path: Path = INCIDENTS_DB_PATH) -> pd.DataFrame:
    """Load H27 incident records from SQLite if available, otherwise from Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            # Check if table exists
            cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='incidents_h27'")
            if cursor.fetchone():
                df = pd.read_sql("SELECT * FROM incidents_h27", conn)
                df["è¦šçŸ¥"] = pd.to_datetime(df["è¦šçŸ¥"], errors="coerce")
                df["date"] = pd.to_datetime(df["date"]).dt.date
                return df
    
    # Fallback to Excel
    if not Path(excel_path).exists():
        raise FileNotFoundError(excel_path)
    df = pd.read_excel(excel_path)
    
    # Build datetime from separate columns
    df["è¦šçŸ¥"] = pd.to_datetime(
        df["è¦šçŸ¥æ—¥ä»˜(å¹´)"].astype(str) + "-" +
        df["è¦šçŸ¥æ—¥ä»˜(æœˆ)"].astype(str).str.zfill(2) + "-" +
        df["è¦šçŸ¥æ—¥ä»˜(æ—¥)"].astype(str).str.zfill(2) + " " +
        df["è¦šçŸ¥æ™‚åˆ»(æ™‚)"].astype(str).str.zfill(2) + ":" +
        df["è¦šçŸ¥æ™‚åˆ»(åˆ†)"].astype(str).str.zfill(2) + ":" +
        df["è¦šçŸ¥æ™‚åˆ»(ç§’)"].fillna(0).astype(int).astype(str).str.zfill(2),
        errors="coerce"
    )
    df = df[df["è¦šçŸ¥"].notna()].copy()
    df["date"] = df["è¦šçŸ¥"].dt.date
    
    # Normalize column names to match R6 format
    df["å‡ºå‹•å ´æ‰€"] = df["å‡ºå ´å ´æ‰€-1"]
    df["å‡ºå‹•éšŠ"] = df["å‡ºå ´éšŠå"]
    df["æ›œæ—¥"] = df["è¦šçŸ¥æ›œæ—¥å"]
    
    return df


def _load_geocode_cache(path: Path = GEOCODE_CACHE_PATH) -> pd.DataFrame:
    if path.exists():
        try:
            return pd.read_parquet(path)
        except Exception:
            return pd.DataFrame(columns=["address", "lat", "lon"])
    return pd.DataFrame(columns=["address", "lat", "lon"])


def _save_geocode_cache(df: pd.DataFrame, path: Path = GEOCODE_CACHE_PATH) -> None:
    try:
        df.to_parquet(path, index=False)
    except Exception:
        pass


def geocode_addresses(addresses: list[str], region_prefix: str = "æ„›åª›çœŒ") -> pd.DataFrame:
    """Geocode addresses with osmnx + Nominatim and persist results locally."""
    cache = _load_geocode_cache().copy()
    seen = set(cache["address"].tolist())
    missing = [a for a in addresses if a not in seen]

    new_records: list[dict] = []
    for addr in missing:
        query = f"{region_prefix} {addr}" if region_prefix else addr
        try:
            lat, lon = ox.geocode(query)
            new_records.append({"address": addr, "lat": lat, "lon": lon})
        except Exception:
            new_records.append({"address": addr, "lat": None, "lon": None})

    if new_records:
        cache = pd.concat([cache, pd.DataFrame(new_records)], ignore_index=True)
        cache = cache.drop_duplicates(subset=["address"], keep="last")
        _save_geocode_cache(cache)

    return cache[cache["address"].isin(addresses)].copy()


@st.cache_data(show_spinner=False)
def load_station_data(
    db_path: Path,
    excel_path: str,
    source_mtime: float,
) -> gpd.GeoDataFrame:
    """Load station records from SQLite when available, otherwise fallback to Excel."""
    if db_path.exists():
        with sqlite3.connect(db_path) as conn:
            df = pd.read_sql("SELECT * FROM stations", conn)
    else:
        df = pd.read_excel(excel_path)

    geometry = gpd.points_from_xy(df["çµŒåº¦"], df["ç·¯åº¦"])
    return gpd.GeoDataFrame(df, geometry=geometry, crs="EPSG:4326")


@st.cache_resource(show_spinner=False)
def load_graph_cached(bbox: tuple[float, float, float, float]) -> nx.MultiDiGraph:
    north, south, east, west = bbox
    if GRAPHML_PATH.exists():
        graph = ox.load_graphml(GRAPHML_PATH)
    else:
        print("é“è·¯ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...ï¼ˆåˆå›å–å¾—ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰")
        try:
            graph = ox.graph_from_bbox(bbox=bbox, network_type="drive")
        except ValueError as exc:
            if "no graph nodes" not in str(exc).lower():
                raise
            graph = ox.graph_from_place("Matsuyama, Ehime, Japan", network_type="drive")
        ox.save_graphml(graph, GRAPHML_PATH)

    if "travel_time" not in next(iter(graph.edges(data=True)))[2]:
        graph = ox.add_edge_speeds(graph, hwy_speeds={
            "residential": 30,
            "secondary": 40,
            "tertiary": 40,
            "primary": 50,
            "motorway": 80,
        })
        graph = ox.add_edge_travel_times(graph)
        ox.save_graphml(graph, GRAPHML_PATH)

    return graph


def compute_isochrones(
    graph: nx.MultiDiGraph,
    stations: gpd.GeoDataFrame,
    trip_times: Iterable[int],
    progress_cb: Callable[[float], None] | None = None,
    delay_factor: float = 1.0,
) -> gpd.GeoDataFrame:
    """Compute isochrones with optional traffic delay factor.
    
    Args:
        delay_factor: Multiplier for travel times (>1 means slower due to traffic)
    """
    records: list[dict] = []

    # Vectorized nearest-node lookup to avoid per-row KDTree rebuilds
    xs = stations["çµŒåº¦"].to_list()
    ys = stations["ç·¯åº¦"].to_list()
    try:
        center_nodes = ox.distance.nearest_nodes(graph, xs, ys)
    except Exception:
        # Fallback to per-point lookup if vectorized call fails
        center_nodes = [ox.distance.nearest_nodes(graph, x, y) for x, y in zip(xs, ys)]

    total = len(center_nodes)

    # Pre-extract node coordinates to avoid repeated attribute lookups
    node_xy = {n: (data["x"], data["y"]) for n, data in graph.nodes(data=True)}
    trip_times_sorted = sorted(trip_times)
    # Adjust max_radius by delay factor (if delay_factor > 1, effective range shrinks)
    max_radius = (trip_times_sorted[-1] * 60 / delay_factor) if trip_times_sorted else 0

    def _one_station(payload: tuple[int, tuple]) -> list[dict]:
        _idx, (row, center_node) = payload
        out: list[dict] = []

        # Single-source Dijkstra once up to the maximum requested time
        lengths = nx.single_source_dijkstra_path_length(
            graph,
            center_node,
            cutoff=max_radius,
            weight="travel_time",
        )

        for minutes in trip_times_sorted:
            # With delay_factor > 1, effective reach shrinks (slower travel)
            cutoff = minutes * 60 / delay_factor
            reachable_nodes = [nid for nid, dist in lengths.items() if dist <= cutoff]
            if not reachable_nodes:
                continue
            points = [node_xy[nid] for nid in reachable_nodes if nid in node_xy]
            if not points:
                continue
            hull = MultiPoint(points).convex_hull
            out.append({"name": row.ç•¥ç§°, "time": minutes, "geometry": hull})

        return out

    with ThreadPoolExecutor(max_workers=min(8, max(2, os.cpu_count() or 2))) as ex:  # type: ignore[name-defined]
        futures = [
            ex.submit(_one_station, payload)
            for payload in enumerate(zip(stations.itertuples(index=False), center_nodes))
        ]
        completed = 0
        for fut in as_completed(futures):
            records.extend(fut.result())
            completed += 1
            if progress_cb:
                progress_cb(completed / total)

    if not records:
        raise RuntimeError("åˆ°é”åœãƒãƒªã‚´ãƒ³ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
    return gpd.GeoDataFrame(records, crs="EPSG:4326")


@st.cache_data(show_spinner=False)
def precompute_isochrones(
    station_df: pd.DataFrame,
    trip_times: tuple[int, ...],
    graph_version: float,
) -> gpd.GeoDataFrame:
    """Cache-intensive isochrone computation so UI updates stay responsive."""
    geometry = gpd.points_from_xy(station_df["çµŒåº¦"], station_df["ç·¯åº¦"])
    stations = gpd.GeoDataFrame(station_df, geometry=geometry, crs="EPSG:4326")

    graph = ox.load_graphml(GRAPHML_PATH)
    return compute_isochrones(graph, stations, trip_times)


@st.cache_data(show_spinner=False)
def load_precomputed_isochrones(path: Path) -> gpd.GeoDataFrame:
    if not path.exists():
        raise FileNotFoundError(path)
    return gpd.read_parquet(path)


def append_virtual_stations(base: gpd.GeoDataFrame, virtuals: list[dict]) -> gpd.GeoDataFrame:
    """Append in-session virtual stations (lat/lon) to the loaded stations."""
    if not virtuals:
        return base

    df_new = pd.DataFrame(virtuals)
    geom = gpd.points_from_xy(df_new["çµŒåº¦"], df_new["ç·¯åº¦"])
    gdf_new = gpd.GeoDataFrame(df_new, geometry=geom, crs="EPSG:4326")

    # Ensure all columns exist and order matches base
    for col in base.columns:
        if col not in gdf_new.columns:
            gdf_new[col] = None
    gdf_new = gdf_new[base.columns]

    return pd.concat([base, gdf_new], ignore_index=True)


def create_location_picker_map(
    stations: gpd.GeoDataFrame,
    virtual_stations: list[dict] | None = None,
) -> folium.Map:
    """Create an interactive map for picking locations to add virtual stations."""
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles="CartoDB Positron")

    # Show existing stations
    for _, row in stations.iterrows():
        folium.CircleMarker(
            location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
            radius=7,
            color="#1f1f1f",
            weight=2,
            fill=True,
            fill_color="#f6bd60",
            fill_opacity=0.9,
            popup=f"{row['ç•¥ç§°']}",
            tooltip=f"æ—¢å­˜: {row['ç•¥ç§°']}",
        ).add_to(fmap)

    # Show virtual stations (if any)
    if virtual_stations:
        for vs in virtual_stations:
            folium.CircleMarker(
                location=[vs["ç·¯åº¦"], vs["çµŒåº¦"]],
                radius=8,
                color="#e63946",
                weight=2,
                fill=True,
                fill_color="#e63946",
                fill_opacity=0.9,
                popup=f"ä»®æƒ³: {vs['ç•¥ç§°']}",
                tooltip=f"ä»®æƒ³: {vs['ç•¥ç§°']}",
            ).add_to(fmap)

    return fmap


def render_map_html(
    isochrones: gpd.GeoDataFrame,
    stations: gpd.GeoDataFrame,
    tiles: str = "CartoDB Positron",
) -> str:
    center_lat = stations["ç·¯åº¦"].mean()
    center_lon = stations["çµŒåº¦"].mean()
    fmap = folium.Map(location=[center_lat, center_lon], zoom_start=11, tiles=tiles)

    # Softer palette for better visibility when overlapping
    color_map = {5: "#ff9e9e", 10: "#8aa5ff", 15: "#7dd8c6", 20: "#f7caa0"}
    for minutes in sorted({*isochrones["time"]}):
        layer = isochrones[isochrones["time"] == minutes]
        if layer.empty:
            continue
        color = color_map.get(minutes, "#4a4a4a")
        folium.GeoJson(
            data=layer.__geo_interface__,
            name=f"{minutes}åˆ†åœ",
            style_function=lambda _feature, c=color, m=minutes: {
                "fillColor": c,
                "color": c,
                "weight": 1.0,
                "opacity": 0.6,
                "fillOpacity": 0.18 if m >= 10 else 0.30,
            },
            tooltip=folium.GeoJsonTooltip(fields=["name", "time"], aliases=["æ‹ ç‚¹", "åˆ°é”æ™‚é–“(åˆ†)"])
        ).add_to(fmap)

    for _, row in stations.iterrows():
        folium.CircleMarker(
            location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
            radius=7,
            color="#1f1f1f",
            weight=2,
            fill=True,
            fill_color="#f6bd60",
            fill_opacity=0.9,
            popup=f"{row['ç•¥ç§°']}",
        ).add_to(fmap)

    folium.LayerControl(collapsed=False).add_to(fmap)
    return fmap.get_root().render()


def main() -> None:
    st.set_page_config(page_title="æ„›åª›æ•‘æ€¥è»Š åˆ°é”åœãƒ“ãƒ¥ãƒ¼ã‚¢", layout="wide")
    st.title("ğŸš‘ æ„›åª›çœŒ æ•‘æ€¥è»Šãƒ“ãƒ¥ãƒ¼ã‚¢")
    st.caption("map.xlsx ã§æ‹ ç‚¹åˆ°é”åœã€R6.xlsx ã§å‡ºå‹•åœ°ç‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚")

    stations = load_station_data(
        db_path=STATIONS_DB_PATH,
        excel_path="map.xlsx",
        source_mtime=station_data_version(),
    )

    if "virtual_stations" not in st.session_state:
        st.session_state["virtual_stations"] = []

    tab_iso, tab_inc, tab_coverage = st.tabs(["åˆ°é”åœ", "å‡ºå‹•åœ°ç‚¹ (R6)", "ã‚«ãƒãƒ¼ç‡åˆ†æ"])

    with tab_iso:
        with st.expander("ğŸ—ºï¸ ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ ï¼ˆã“ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ï¼‰", expanded=False):
            st.markdown("**åœ°å›³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ä»®æƒ³æ¶ˆé˜²ç½²ã®å ´æ‰€ã‚’é¸æŠã—ã¦ãã ã•ã„**")
            st.caption("ã‚¯ãƒªãƒƒã‚¯å¾Œã€åå‰ã‚’å…¥åŠ›ã—ã¦ã€Œè¿½åŠ ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

            # ã‚¯ãƒªãƒƒã‚¯é¸æŠç”¨ã®åœ°å›³ã‚’è¡¨ç¤º
            picker_map = create_location_picker_map(
                stations,
                st.session_state["virtual_stations"],
            )
            map_data = st_folium(
                picker_map,
                width=700,
                height=400,
                key="location_picker",
                returned_objects=["last_clicked"],
            )

            # ã‚¯ãƒªãƒƒã‚¯ã—ãŸåº§æ¨™ã‚’å–å¾—
            clicked_lat = None
            clicked_lon = None
            if map_data and map_data.get("last_clicked"):
                clicked_lat = map_data["last_clicked"]["lat"]
                clicked_lon = map_data["last_clicked"]["lng"]

            col_info, col_add = st.columns([2, 1])
            with col_info:
                if clicked_lat is not None:
                    st.success(f"ğŸ“ é¸æŠä½ç½®: ç·¯åº¦ {clicked_lat:.6f}, çµŒåº¦ {clicked_lon:.6f}")
                else:
                    st.info("ğŸ’¡ åœ°å›³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦å ´æ‰€ã‚’é¸æŠã—ã¦ãã ã•ã„")

            with col_add:
                default_name = f"ä»®æƒ³ç½²{len(st.session_state['virtual_stations']) + 1}"
                v_name = st.text_input("åå‰", value=default_name, key="virtual_name_input")

            # è¿½åŠ ãƒœã‚¿ãƒ³
            col_btn1, col_btn2 = st.columns(2)
            with col_btn1:
                if st.button("âœ… ã“ã®å ´æ‰€ã«è¿½åŠ ", type="primary", disabled=(clicked_lat is None)):
                    if clicked_lat is not None:
                        st.session_state["virtual_stations"].append({
                            "ç•¥ç§°": v_name.strip() or default_name,
                            "ç·¯åº¦": clicked_lat,
                            "çµŒåº¦": clicked_lon,
                        })
                        st.success(f"ä»®æƒ³æ¶ˆé˜²ç½²ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {v_name}")
                        st.rerun()
            with col_btn2:
                if st.button("ğŸ—‘ï¸ å…¨ã¦ã‚¯ãƒªã‚¢", type="secondary"):
                    st.session_state["virtual_stations"] = []
                    st.info("ä»®æƒ³æ¶ˆé˜²ç½²ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
                    st.rerun()

            # è¿½åŠ æ¸ˆã¿ä»®æƒ³æ¶ˆé˜²ç½²ä¸€è¦§
            if st.session_state["virtual_stations"]:
                st.markdown("---")
                st.markdown("**è¿½åŠ æ¸ˆã¿ä»®æƒ³æ¶ˆé˜²ç½²:**")
                for i, vs in enumerate(st.session_state["virtual_stations"]):
                    col_name, col_coord, col_del = st.columns([2, 3, 1])
                    with col_name:
                        st.write(f"ğŸ”´ {vs['ç•¥ç§°']}")
                    with col_coord:
                        st.caption(f"({vs['ç·¯åº¦']:.5f}, {vs['çµŒåº¦']:.5f})")
                    with col_del:
                        if st.button("å‰Šé™¤", key=f"del_vs_{i}"):
                            st.session_state["virtual_stations"].pop(i)
                            st.rerun()

        has_virtual = bool(st.session_state["virtual_stations"])
        stations_view = append_virtual_stations(stations, st.session_state["virtual_stations"])
        station_names = sorted(stations_view["ç•¥ç§°"].unique())
        trip_options = [5, 10, 15, 20]

        col_left, col_right = st.columns([2, 1])
        with col_left:
            selected_names = st.multiselect(
                "è¡¨ç¤ºã™ã‚‹æ¶ˆé˜²ç½²",
                options=station_names,
                default=station_names,
                help="è¤‡æ•°é¸æŠã§åˆ°é”åœã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚",
            )
        with col_right:
            selected_times = st.multiselect(
                "åˆ°é”æ™‚é–“ (åˆ†)",
                options=trip_options,
                default=[5, 10],
            )

        # ========== ğŸš¦ æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ ==========
        with st.expander("ğŸš¦ æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’æ¸ˆã¿ï¼‰", expanded=False):
            factors_exist = DELAY_FACTORS_PATH.exists()
            if factors_exist:
                st.success("âœ… R6å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸé…å»¶ä¿‚æ•°ã‚’ä½¿ç”¨")
            else:
                st.warning("âš ï¸ misc/learn_delays.py ã‚’å®Ÿè¡Œã™ã‚‹ã¨å­¦ç¿’ã§ãã¾ã™")

            traffic_enabled = st.toggle("æ¸‹æ»ã‚’è€ƒæ…®ã™ã‚‹", value=False, key="traffic_enabled")

            if traffic_enabled:
                col_time, col_dow = st.columns(2)
                with col_time:
                    time_slot = st.selectbox(
                        "æ™‚é–“å¸¯",
                        options=list(TIME_SLOT_LABELS.keys()),
                        index=3,  # æœãƒ©ãƒƒã‚·ãƒ¥
                        key="traffic_time_slot",
                    )
                    # ä»£è¡¨çš„ãªæ™‚é–“ã‚’å–å¾—
                    slot_hours = TIME_SLOT_LABELS[time_slot]
                    selected_hour = slot_hours[len(slot_hours) // 2]

                with col_dow:
                    use_dow = st.checkbox("æ›œæ—¥ã‚‚è€ƒæ…®", value=False, key="traffic_use_dow")
                    if use_dow:
                        dow_label = st.selectbox(
                            "æ›œæ—¥",
                            options=DOW_LABELS,
                            index=0,
                            key="traffic_dow",
                        )
                        selected_dow = DOW_LABELS.index(dow_label)
                    else:
                        selected_dow = None

                delay_factor = get_delay_factor(selected_hour, selected_dow)
                
                # ä¿‚æ•°ã®æ„å‘³ã‚’è¡¨ç¤º
                if delay_factor < 1.0:
                    emoji = "ğŸŸ¢"
                    desc = "æ·±å¤œã‚ˆã‚Šé€Ÿã„ï¼ˆæ•‘æ€¥å„ªå…ˆèµ°è¡Œã®åŠ¹æœå¤§ï¼‰"
                elif delay_factor < 1.1:
                    desc = "é€šå¸¸"
                    emoji = "ğŸŸ¡"
                else:
                    desc = "ã‚„ã‚„æ··é›‘"
                    emoji = "ğŸ”´"
                
                st.info(f"{emoji} é…å»¶ä¿‚æ•°: **{delay_factor:.3f}** ({desc})")
                st.caption(f"â†’ ä¾‹: 5åˆ†åœãŒå®Ÿè³ª {5 * delay_factor:.1f}åˆ†åœ ã«ç¸®å°")
            else:
                delay_factor = 1.0
        # ========================================

        if not selected_names:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®æ¶ˆé˜²ç½²ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()
        if not selected_times:
            st.warning("å°‘ãªãã¨ã‚‚1ã¤ã®åˆ°é”æ™‚é–“ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        filtered = stations_view[stations_view["ç•¥ç§°"].isin(selected_names)].copy()

        padding_deg = 0.1
        west_all, south_all, east_all, north_all = stations_view.total_bounds
        bbox = (north_all + padding_deg, south_all - padding_deg, east_all + padding_deg, west_all - padding_deg)

        with st.spinner("é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            graph = load_graph_cached(bbox)

        # æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã‚ãšå†è¨ˆç®—ï¼ˆä¿‚æ•°ãŒç•°ãªã‚‹ãŸã‚ï¼‰
        use_cache = ISOCHRONE_CACHE_PATH.exists() and not has_virtual and delay_factor == 1.0
        
        if use_cache:
            try:
                with st.spinner("åˆ°é”åœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    all_isochrones = load_precomputed_isochrones(ISOCHRONE_CACHE_PATH)
                display_isochrones = all_isochrones[
                    (all_isochrones["name"].isin(selected_names)) &
                    (all_isochrones["time"].isin(selected_times))
                ].copy()
            except Exception as exc:
                st.warning(f"äº‹å‰è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸãŸã‚å†è¨ˆç®—ã—ã¾ã™: {exc}")
                display_isochrones = None
        else:
            display_isochrones = None

        if display_isochrones is None:
            spinner_msg = "åˆ°é”åœã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™..."
            if delay_factor != 1.0:
                spinner_msg = f"æ¸‹æ»è€ƒæ…®ã§åˆ°é”åœã‚’è¨ˆç®—ä¸­ï¼ˆä¿‚æ•°: {delay_factor:.3f}ï¼‰..."
            with st.spinner(spinner_msg):
                prog = st.progress(0)
                display_isochrones = compute_isochrones(
                    graph=graph,
                    stations=filtered,
                    trip_times=selected_times,
                    progress_cb=lambda p: prog.progress(int(p * 100)),
                    delay_factor=delay_factor,
                )

        if display_isochrones.empty:
            st.error("é¸æŠæ¡ä»¶ã«åˆè‡´ã™ã‚‹åˆ°é”åœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        html_map = render_map_html(display_isochrones, filtered)
        st.components.v1.html(html_map, height=720)

    with tab_inc:
        try:
            incidents = load_incident_data("R6.xlsx")
        except FileNotFoundError:
            st.error("R6.xlsx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒˆã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        date_options = sorted(incidents["date"].unique())
        if not date_options:
            st.warning("R6.xlsx ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            st.stop()

        default_date = date_options[0]
        selected_date = st.selectbox(
            "è¡¨ç¤ºã™ã‚‹æ—¥ä»˜ (è¦šçŸ¥æ—¥)",
            options=date_options,
            format_func=lambda d: d.strftime("%Y-%m-%d"),
            index=0,
        )

        day_inc = incidents[incidents["date"] == selected_date].copy()
        addr_series = day_inc["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
        addr_unique = sorted(addr_series.unique())

        st.write(f"{selected_date} ã®å‡ºå‹•ä»¶æ•°: {len(day_inc)} ä»¶ (ãƒ¦ãƒ‹ãƒ¼ã‚¯åœ°ç‚¹ {len(addr_unique)} ç®‡æ‰€)")

        with st.spinner("ä½æ‰€ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ã„ã¾ã™ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨) ..."):
            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")

        merged = day_inc.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
        mapped = merged.dropna(subset=["lat", "lon"]).copy()
        missing_count = len(day_inc) - len(mapped)

        if mapped.empty:
            st.error("ã“ã®æ—¥ã®åœ°ç‚¹ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.stop()

        st.write(f"åœ°å›³ã«ãƒ—ãƒ­ãƒƒãƒˆã§ããŸä»¶æ•°: {len(mapped)} / {len(day_inc)} (æœªç‰¹å®š {missing_count} ä»¶)")

        center_lat = mapped["lat"].mean()
        center_lon = mapped["lon"].mean()
        fmap = folium.Map(location=[center_lat, center_lon], zoom_start=12, tiles="CartoDB Positron")

        # Softer color by weekday to help visually group clusters
        weekday_colors = {
            "æœˆ": "#f94144",
            "ç«": "#f3722c",
            "æ°´": "#f9c74f",
            "æœ¨": "#90be6d",
            "é‡‘": "#43aa8b",
            "åœŸ": "#577590",
            "æ—¥": "#9d4edd",
        }

        for _, row in mapped.iterrows():
            wk = str(row.get("æ›œæ—¥", "?"))
            color = weekday_colors.get(wk, "#4a4a4a")
            label_time = row["è¦šçŸ¥"].strftime("%H:%M") if not pd.isna(row.get("è¦šçŸ¥")) else "--:--"
            popup = f"{row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time} | {row.get('æ¬é€åŒºåˆ†(äº‹æ¡ˆ)', '')}"
            folium.CircleMarker(
                location=[row["lat"], row["lon"]],
                radius=5,
                color=color,
                fill=True,
                fill_color=color,
                fill_opacity=0.85,
                weight=1.0,
                popup=popup,
            ).add_to(fmap)

        st.components.v1.html(fmap.get_root().render(), height=720)

    with tab_coverage:
        st.subheader("ğŸ“Š å‡ºå‹•åœ°ç‚¹ã®åˆ°é”åœã‚«ãƒãƒ¼ç‡åˆ†æ")
        st.caption("é…ç½®å¤‰æ›´å‰(H27)ã¨å¤‰æ›´å¾Œ(R6)ã®ã‚«ãƒãƒ¼ç‡ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚")

        # Check file availability
        r6_available = Path("R6.xlsx").exists()
        h27_available = Path("H27.xls").exists()

        if not r6_available and not h27_available:
            st.error("å‡ºå‹•ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚R6.xlsx ã¾ãŸã¯ H27.xls ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        # Dataset selection
        dataset_options = []
        if r6_available:
            dataset_options.append("R6 (é…ç½®å¤‰æ›´å¾Œãƒ»2024å¹´)")
        if h27_available:
            dataset_options.append("H27 (é…ç½®å¤‰æ›´å‰ãƒ»2015å¹´)")
        if r6_available and h27_available:
            dataset_options.append("â­ æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ (R6 vs H27)")

        col_mode, col_traffic = st.columns([2, 1])
        with col_mode:
            selected_mode = st.radio(
                "åˆ†æãƒ¢ãƒ¼ãƒ‰",
                options=dataset_options,
                horizontal=True,
            )
        
        is_comparison = "æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰" in selected_mode

        # ========== ğŸš¦ æ¸‹æ»è€ƒæ…® & æ™‚é–“å¸¯åˆ¥åˆ†æ ==========
        with st.expander("ğŸš¦ æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡åˆ†æ", expanded=False):
            factors_exist_cov = DELAY_FACTORS_PATH.exists()
            if factors_exist_cov:
                st.success("âœ… R6å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã—ãŸé…å»¶ä¿‚æ•°ã‚’ä½¿ç”¨")
            else:
                st.warning("âš ï¸ misc/learn_delays.py ã‚’å®Ÿè¡Œã™ã‚‹ã¨å­¦ç¿’ã§ãã¾ã™")

            analysis_type = st.radio(
                "åˆ†æã‚¿ã‚¤ãƒ—",
                ["é€šå¸¸ï¼ˆæ¸‹æ»ãªã—ï¼‰", "ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡", "ğŸš¦ ç‰¹å®šæ™‚é–“å¸¯ã®æ¸‹æ»è€ƒæ…®"],
                horizontal=True,
                key="coverage_analysis_type",
            )

            cov_delay_factor = 1.0
            selected_hour_cov = None
            hourly_analysis = False

            if analysis_type == "ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡":
                hourly_analysis = True
                st.info("ğŸ“Š å‡ºå‹•æ™‚åˆ»ã«åŸºã¥ã„ã¦æ™‚é–“å¸¯åˆ¥ã®ã‚«ãƒãƒ¼ç‡ã‚’é›†è¨ˆã—ã¾ã™")
            
            elif analysis_type == "ğŸš¦ ç‰¹å®šæ™‚é–“å¸¯ã®æ¸‹æ»è€ƒæ…®":
                col_slot, col_info = st.columns([1, 1])
                with col_slot:
                    time_slot_cov = st.selectbox(
                        "æ™‚é–“å¸¯",
                        options=list(TIME_SLOT_LABELS.keys()),
                        index=3,
                        key="coverage_time_slot",
                    )
                    slot_hours_cov = TIME_SLOT_LABELS[time_slot_cov]
                    selected_hour_cov = slot_hours_cov[len(slot_hours_cov) // 2]
                    cov_delay_factor = get_delay_factor(selected_hour_cov)
                
                with col_info:
                    if cov_delay_factor < 1.0:
                        emoji_cov = "ğŸŸ¢"
                        desc_cov = "æ•‘æ€¥å„ªå…ˆèµ°è¡Œã§é€Ÿã„"
                    elif cov_delay_factor < 1.1:
                        emoji_cov = "ğŸŸ¡"
                        desc_cov = "é€šå¸¸"
                    else:
                        emoji_cov = "ğŸ”´"
                        desc_cov = "ã‚„ã‚„æ··é›‘"
                    st.metric(
                        "é…å»¶ä¿‚æ•°",
                        f"{cov_delay_factor:.3f}",
                        delta=desc_cov,
                    )
        # ================================================

        # Load or compute isochrones (shared for all modes)
        stations_cov = load_station_data(
            db_path=STATIONS_DB_PATH,
            excel_path="map.xlsx",
            source_mtime=station_data_version(),
        )
        trip_times_cov = [5, 10]

        # æ¸‹æ»è€ƒæ…®ãƒ¢ãƒ¼ãƒ‰ã§ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã‚ãšå†è¨ˆç®—
        use_cache_cov = ISOCHRONE_CACHE_PATH.exists() and cov_delay_factor == 1.0

        if use_cache_cov:
            try:
                with st.spinner("åˆ°é”åœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                    isochrones_cov = load_precomputed_isochrones(ISOCHRONE_CACHE_PATH)
            except Exception as exc:
                st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿å¤±æ•—: {exc}")
                isochrones_cov = None
        else:
            isochrones_cov = None

        if isochrones_cov is None:
            padding_deg = 0.1
            west_cov, south_cov, east_cov, north_cov = stations_cov.total_bounds
            bbox_cov = (north_cov + padding_deg, south_cov - padding_deg, east_cov + padding_deg, west_cov - padding_deg)
            with st.spinner("é“è·¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
                graph_cov = load_graph_cached(bbox_cov)
            spinner_msg_cov = "åˆ°é”åœã‚’è¨ˆç®—ä¸­..."
            if cov_delay_factor != 1.0:
                spinner_msg_cov = f"æ¸‹æ»è€ƒæ…®ã§åˆ°é”åœã‚’è¨ˆç®—ä¸­ï¼ˆä¿‚æ•°: {cov_delay_factor:.3f}ï¼‰..."
            with st.spinner(spinner_msg_cov):
                prog_cov = st.progress(0)
                isochrones_cov = compute_isochrones(
                    graph=graph_cov,
                    stations=stations_cov,
                    trip_times=trip_times_cov,
                    progress_cb=lambda p: prog_cov.progress(int(p * 100)),
                    delay_factor=cov_delay_factor,
                )

        def analyze_coverage(incidents_df: pd.DataFrame, label: str, isochrones: gpd.GeoDataFrame = None) -> dict:
            """Analyze coverage for a given incident dataset."""
            if isochrones is None:
                isochrones = isochrones_cov
            
            addr_series = incidents_df["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
            addr_unique = sorted(addr_series.unique())

            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")
            merged = incidents_df.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
            mapped = merged.dropna(subset=["lat", "lon"]).copy()

            if mapped.empty:
                return None

            incident_points = gpd.GeoDataFrame(
                mapped,
                geometry=gpd.points_from_xy(mapped["lon"], mapped["lat"]),
                crs="EPSG:4326"
            )

            results = {"label": label, "total": len(incidents_df), "geocoded": len(mapped)}
            for minutes in trip_times_cov:
                iso_layer = isochrones[isochrones["time"] == minutes]
                if iso_layer.empty:
                    results[f"covered_{minutes}"] = 0
                    continue
                combined_polygon = unary_union(iso_layer.geometry)
                within_mask = incident_points.geometry.within(combined_polygon)
                results[f"covered_{minutes}"] = within_mask.sum()
                incident_points[f"within_{minutes}min"] = within_mask

            results["incident_points"] = incident_points
            results["mapped"] = mapped
            return results

        def analyze_hourly_coverage(incidents_df: pd.DataFrame) -> pd.DataFrame:
            """Analyze coverage by hour of day."""
            # æ™‚é–“å¸¯ã®å®šç¾©
            time_bins = {
                "æ·±å¤œ (0-5æ™‚)": list(range(0, 5)),
                "æ—©æœ (5-7æ™‚)": list(range(5, 7)),
                "æœãƒ©ãƒƒã‚·ãƒ¥ (7-9æ™‚)": list(range(7, 9)),
                "åˆå‰ (9-12æ™‚)": list(range(9, 12)),
                "æ˜¼ (12-14æ™‚)": list(range(12, 14)),
                "åˆå¾Œ (14-17æ™‚)": list(range(14, 17)),
                "å¤•ãƒ©ãƒƒã‚·ãƒ¥ (17-19æ™‚)": list(range(17, 19)),
                "å¤œ (19-22æ™‚)": list(range(19, 22)),
                "æ·±å¤œ (22-24æ™‚)": list(range(22, 24)),
            }
            
            # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            addr_series = incidents_df["å‡ºå‹•å ´æ‰€"].dropna().astype(str)
            addr_unique = sorted(addr_series.unique())
            geo_df = geocode_addresses(addr_unique, region_prefix="æ„›åª›çœŒ")
            merged = incidents_df.merge(geo_df, left_on="å‡ºå‹•å ´æ‰€", right_on="address", how="left")
            mapped = merged.dropna(subset=["lat", "lon"]).copy()
            
            if mapped.empty:
                return None
            
            # æ™‚é–“å¸¯åˆ—ã‚’è¿½åŠ 
            mapped["hour"] = pd.to_datetime(mapped["è¦šçŸ¥"], errors="coerce").dt.hour
            
            incident_points = gpd.GeoDataFrame(
                mapped,
                geometry=gpd.points_from_xy(mapped["lon"], mapped["lat"]),
                crs="EPSG:4326"
            )
            
            # æ™‚é–“å¸¯ã”ã¨ã«ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—
            results = []
            for slot_name, hours in time_bins.items():
                slot_points = incident_points[incident_points["hour"].isin(hours)]
                if slot_points.empty:
                    continue
                
                total = len(slot_points)
                row = {"æ™‚é–“å¸¯": slot_name, "ä»¶æ•°": total}
                
                for minutes in trip_times_cov:
                    iso_layer = isochrones_cov[isochrones_cov["time"] == minutes]
                    if iso_layer.empty:
                        row[f"{minutes}åˆ†åœã‚«ãƒãƒ¼"] = 0
                        row[f"{minutes}åˆ†åœç‡"] = 0.0
                        continue
                    combined_polygon = unary_union(iso_layer.geometry)
                    within_mask = slot_points.geometry.within(combined_polygon)
                    covered = within_mask.sum()
                    row[f"{minutes}åˆ†åœã‚«ãƒãƒ¼"] = covered
                    row[f"{minutes}åˆ†åœç‡"] = covered / total * 100 if total > 0 else 0
                
                results.append(row)
            
            return pd.DataFrame(results)

        # ========== æ™‚é–“å¸¯åˆ¥åˆ†æãƒ¢ãƒ¼ãƒ‰ ==========
        if hourly_analysis:
            st.markdown("---")
            st.subheader("ğŸ• æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡åˆ†æ")
            
            import altair as alt
            
            # æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æ
            if is_comparison:
                col_r6_h, col_h27_h = st.columns(2)
                
                with st.spinner("R6ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    incidents_r6_hourly = load_incident_data("R6.xlsx")
                    hourly_df_r6 = analyze_hourly_coverage(incidents_r6_hourly)
                
                with st.spinner("H27ãƒ‡ãƒ¼ã‚¿ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    incidents_h27_hourly = load_incident_data_h27("H27.xls")
                    hourly_df_h27 = analyze_hourly_coverage(incidents_h27_hourly)
                
                if hourly_df_r6 is not None and hourly_df_h27 is not None:
                    # ä¸¦ã¹ã¦è¡¨ç¤º
                    with col_r6_h:
                        st.markdown("### ğŸŸ¢ R6 (2024å¹´)")
                        display_df_r6 = hourly_df_r6[["æ™‚é–“å¸¯", "ä»¶æ•°", "5åˆ†åœç‡", "10åˆ†åœç‡"]].copy()
                        display_df_r6["5åˆ†åœç‡"] = display_df_r6["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        display_df_r6["10åˆ†åœç‡"] = display_df_r6["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        st.dataframe(display_df_r6, use_container_width=True, hide_index=True)
                    
                    with col_h27_h:
                        st.markdown("### ğŸŸ¡ H27 (2015å¹´)")
                        display_df_h27 = hourly_df_h27[["æ™‚é–“å¸¯", "ä»¶æ•°", "5åˆ†åœç‡", "10åˆ†åœç‡"]].copy()
                        display_df_h27["5åˆ†åœç‡"] = display_df_h27["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        display_df_h27["10åˆ†åœç‡"] = display_df_h27["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                        st.dataframe(display_df_h27, use_container_width=True, hide_index=True)
                    
                    # æ¯”è¼ƒã‚°ãƒ©ãƒ•ï¼ˆ5åˆ†åœï¼‰
                    st.markdown("### ğŸ“ˆ 5åˆ†åœã‚«ãƒãƒ¼ç‡æ¯”è¼ƒã‚°ãƒ©ãƒ•")
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                    hourly_df_r6["ãƒ‡ãƒ¼ã‚¿"] = "R6 (2024å¹´)"
                    hourly_df_h27["ãƒ‡ãƒ¼ã‚¿"] = "H27 (2015å¹´)"
                    combined_hourly = pd.concat([hourly_df_r6, hourly_df_h27], ignore_index=True)
                    
                    chart_5min = alt.Chart(combined_hourly).mark_bar().encode(
                        x=alt.X("æ™‚é–“å¸¯:N", sort=list(TIME_SLOT_LABELS.keys()), title="æ™‚é–“å¸¯"),
                        y=alt.Y("5åˆ†åœç‡:Q", title="5åˆ†åœã‚«ãƒãƒ¼ç‡ (%)"),
                        color=alt.Color("ãƒ‡ãƒ¼ã‚¿:N", scale=alt.Scale(
                            domain=["R6 (2024å¹´)", "H27 (2015å¹´)"],
                            range=["#2ecc71", "#f1c40f"]
                        )),
                        xOffset="ãƒ‡ãƒ¼ã‚¿:N",
                        tooltip=["æ™‚é–“å¸¯", "ãƒ‡ãƒ¼ã‚¿", alt.Tooltip("5åˆ†åœç‡:Q", format=".1f"), "ä»¶æ•°"],
                    ).properties(
                        width=600,
                        height=350,
                    )
                    st.altair_chart(chart_5min, use_container_width=True)
                    
                    # å·®åˆ†ãƒ†ãƒ¼ãƒ–ãƒ«
                    st.markdown("### ğŸ“Š æ™‚é–“å¸¯åˆ¥ æ”¹å–„åº¦ï¼ˆR6 - H27ï¼‰")
                    diff_data = []
                    for slot in hourly_df_r6["æ™‚é–“å¸¯"].unique():
                        r6_row = hourly_df_r6[hourly_df_r6["æ™‚é–“å¸¯"] == slot]
                        h27_row = hourly_df_h27[hourly_df_h27["æ™‚é–“å¸¯"] == slot]
                        if not r6_row.empty and not h27_row.empty:
                            diff_5 = r6_row["5åˆ†åœç‡"].values[0] - h27_row["5åˆ†åœç‡"].values[0]
                            diff_10 = r6_row["10åˆ†åœç‡"].values[0] - h27_row["10åˆ†åœç‡"].values[0]
                            diff_data.append({
                                "æ™‚é–“å¸¯": slot,
                                "5åˆ†åœæ”¹å–„": f"{diff_5:+.1f}%",
                                "10åˆ†åœæ”¹å–„": f"{diff_10:+.1f}%",
                            })
                    
                    diff_df = pd.DataFrame(diff_data)
                    st.dataframe(diff_df, use_container_width=True, hide_index=True)
                    
                    # ã‚µãƒãƒª
                    st.markdown("### ğŸ” åˆ†æã‚µãƒãƒª")
                    avg_diff_5 = (hourly_df_r6["5åˆ†åœç‡"].mean() - hourly_df_h27["5åˆ†åœç‡"].mean())
                    avg_diff_10 = (hourly_df_r6["10åˆ†åœç‡"].mean() - hourly_df_h27["10åˆ†åœç‡"].mean())
                    
                    if avg_diff_5 > 0:
                        st.success(f"âœ… å…¨æ™‚é–“å¸¯å¹³å‡ 5åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_5:+.1f}%** æ”¹å–„")
                    else:
                        st.warning(f"âš ï¸ å…¨æ™‚é–“å¸¯å¹³å‡ 5åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_5:+.1f}%**")
                    
                    if avg_diff_10 > 0:
                        st.success(f"âœ… å…¨æ™‚é–“å¸¯å¹³å‡ 10åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_10:+.1f}%** æ”¹å–„")
                    else:
                        st.warning(f"âš ï¸ å…¨æ™‚é–“å¸¯å¹³å‡ 10åˆ†åœã‚«ãƒãƒ¼ç‡: **{avg_diff_10:+.1f}%**")
                else:
                    st.error("æ™‚é–“å¸¯åˆ¥åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
            else:
                # å˜ä¸€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰
                if "R6" in selected_mode:
                    incidents_hourly = load_incident_data("R6.xlsx")
                    data_label_hourly = "R6 (2024å¹´)"
                else:
                    incidents_hourly = load_incident_data_h27("H27.xls")
                    data_label_hourly = "H27 (2015å¹´)"
                
                with st.spinner(f"{data_label_hourly} ã®æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚’è¨ˆç®—ä¸­..."):
                    hourly_df = analyze_hourly_coverage(incidents_hourly)
                
                if hourly_df is not None:
                    st.markdown(f"### ğŸ“Š {data_label_hourly} æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡")
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
                    display_df = hourly_df.copy()
                    display_df["5åˆ†åœç‡"] = display_df["5åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                    display_df["10åˆ†åœç‡"] = display_df["10åˆ†åœç‡"].apply(lambda x: f"{x:.1f}%")
                    st.dataframe(display_df, use_container_width=True, hide_index=True)
                    
                    # ã‚°ãƒ©ãƒ•è¡¨ç¤º
                    st.markdown("### ğŸ“ˆ æ™‚é–“å¸¯åˆ¥ã‚«ãƒãƒ¼ç‡ã‚°ãƒ©ãƒ•")
                    
                    chart_data = hourly_df.melt(
                        id_vars=["æ™‚é–“å¸¯", "ä»¶æ•°"],
                        value_vars=["5åˆ†åœç‡", "10åˆ†åœç‡"],
                        var_name="åˆ°é”åœ",
                        value_name="ã‚«ãƒãƒ¼ç‡",
                    )
                    
                    chart = alt.Chart(chart_data).mark_bar().encode(
                        x=alt.X("æ™‚é–“å¸¯:N", sort=list(TIME_SLOT_LABELS.keys()), title="æ™‚é–“å¸¯"),
                        y=alt.Y("ã‚«ãƒãƒ¼ç‡:Q", title="ã‚«ãƒãƒ¼ç‡ (%)"),
                        color=alt.Color("åˆ°é”åœ:N", scale=alt.Scale(
                            domain=["5åˆ†åœç‡", "10åˆ†åœç‡"],
                            range=["#ff9e9e", "#8aa5ff"]
                        )),
                        xOffset="åˆ°é”åœ:N",
                        tooltip=["æ™‚é–“å¸¯", "åˆ°é”åœ", alt.Tooltip("ã‚«ãƒãƒ¼ç‡:Q", format=".1f"), "ä»¶æ•°"],
                    ).properties(
                        width=600,
                        height=400,
                    )
                    st.altair_chart(chart, use_container_width=True)
                    
                    # æ™‚é–“å¸¯é–“ã®å·®ã‚’åˆ†æ
                    st.markdown("### ğŸ” åˆ†æã‚µãƒãƒª")
                    best_5min = hourly_df.loc[hourly_df["5åˆ†åœç‡"].idxmax()]
                    worst_5min = hourly_df.loc[hourly_df["5åˆ†åœç‡"].idxmin()]
                    
                    col_best, col_worst = st.columns(2)
                    with col_best:
                        st.success(f"âœ… 5åˆ†åœã‚«ãƒãƒ¼ç‡ æœ€é«˜: **{best_5min['æ™‚é–“å¸¯']}** ({best_5min['5åˆ†åœç‡']:.1f}%)")
                    with col_worst:
                        st.warning(f"âš ï¸ 5åˆ†åœã‚«ãƒãƒ¼ç‡ æœ€ä½: **{worst_5min['æ™‚é–“å¸¯']}** ({worst_5min['5åˆ†åœç‡']:.1f}%)")
                    
                    st.info(f"ğŸ“Š æ™‚é–“å¸¯ã«ã‚ˆã‚‹5åˆ†åœã‚«ãƒãƒ¼ç‡ã®å·®: **{best_5min['5åˆ†åœç‡'] - worst_5min['5åˆ†åœç‡']:.1f}%**")
                else:
                    st.error("æ™‚é–“å¸¯åˆ¥åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            
            st.stop()  # æ™‚é–“å¸¯åˆ¥åˆ†æãƒ¢ãƒ¼ãƒ‰ã§ã¯ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—
        # ========================================

        if is_comparison:
            # Comparison mode: analyze both datasets
            st.markdown("---")
            col_r6, col_h27 = st.columns(2)

            with col_r6:
                st.markdown("### ğŸŸ¢ R6 (é…ç½®å¤‰æ›´å¾Œãƒ»2024å¹´)")
            with col_h27:
                st.markdown("### ğŸŸ¡ H27 (é…ç½®å¤‰æ›´å‰ãƒ»2015å¹´)")

            with st.spinner("R6ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­..."):
                incidents_r6 = load_incident_data("R6.xlsx")
                results_r6 = analyze_coverage(incidents_r6, "R6")

            with st.spinner("H27ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­..."):
                incidents_h27 = load_incident_data_h27("H27.xls")
                results_h27 = analyze_coverage(incidents_h27, "H27")

            if results_r6 is None or results_h27 is None:
                st.error("ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.stop()

            # Build comparison table
            comparison_data = []
            for minutes in trip_times_cov:
                r6_covered = results_r6[f"covered_{minutes}"]
                r6_total = results_r6["geocoded"]
                r6_pct = r6_covered / r6_total * 100 if r6_total > 0 else 0

                h27_covered = results_h27[f"covered_{minutes}"]
                h27_total = results_h27["geocoded"]
                h27_pct = h27_covered / h27_total * 100 if h27_total > 0 else 0

                diff = r6_pct - h27_pct

                comparison_data.append({
                    "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                    "R6 ã‚«ãƒãƒ¼ç‡": f"{r6_pct:.1f}%",
                    "R6 (ä»¶æ•°)": f"{r6_covered}/{r6_total}",
                    "H27 ã‚«ãƒãƒ¼ç‡": f"{h27_pct:.1f}%",
                    "H27 (ä»¶æ•°)": f"{h27_covered}/{h27_total}",
                    "å·®åˆ†": f"{diff:+.1f}%",
                })

            st.markdown("### ğŸ“Š ã‚«ãƒãƒ¼ç‡æ¯”è¼ƒ")
            comparison_df = pd.DataFrame(comparison_data)
            st.dataframe(comparison_df, use_container_width=True, hide_index=True)

            # Metrics side by side
            st.markdown("### ğŸ“ˆ å·®åˆ†ãƒ¡ãƒˆãƒªã‚¯ã‚¹")
            for minutes in trip_times_cov:
                r6_pct = results_r6[f"covered_{minutes}"] / results_r6["geocoded"] * 100
                h27_pct = results_h27[f"covered_{minutes}"] / results_h27["geocoded"] * 100
                diff = r6_pct - h27_pct

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(f"R6 {minutes}åˆ†åˆ°é”åœ", f"{r6_pct:.1f}%")
                with col2:
                    st.metric(f"H27 {minutes}åˆ†åˆ°é”åœ", f"{h27_pct:.1f}%")
                with col3:
                    st.metric(
                        f"{minutes}åˆ†åœ æ”¹å–„åº¦",
                        f"{diff:+.1f}%",
                        delta=f"{diff:+.1f}%" if diff != 0 else None,
                        delta_color="normal" if diff >= 0 else "inverse"
                    )

            # Summary
            st.markdown("---")
            st.markdown("### ğŸ“ åˆ†æã‚µãƒãƒª")
            r6_5min_pct = results_r6["covered_5"] / results_r6["geocoded"] * 100
            h27_5min_pct = results_h27["covered_5"] / results_h27["geocoded"] * 100
            r6_10min_pct = results_r6["covered_10"] / results_r6["geocoded"] * 100
            h27_10min_pct = results_h27["covered_10"] / results_h27["geocoded"] * 100

            if r6_5min_pct > h27_5min_pct:
                st.success(f"âœ… 5åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: é…ç½®å¤‰æ›´å¾Œ {r6_5min_pct - h27_5min_pct:+.1f}% æ”¹å–„")
            else:
                st.warning(f"âš ï¸ 5åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: é…ç½®å¤‰æ›´å¾Œ {r6_5min_pct - h27_5min_pct:+.1f}%")

            if r6_10min_pct > h27_10min_pct:
                st.success(f"âœ… 10åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: é…ç½®å¤‰æ›´å¾Œ {r6_10min_pct - h27_10min_pct:+.1f}% æ”¹å–„")
            else:
                st.warning(f"âš ï¸ 10åˆ†åˆ°é”åœã‚«ãƒãƒ¼ç‡: é…ç½®å¤‰æ›´å¾Œ {r6_10min_pct - h27_10min_pct:+.1f}%")

            st.info(f"""
            **ãƒ‡ãƒ¼ã‚¿ä»¶æ•°**
            - R6 (2024å¹´): {results_r6['total']:,} ä»¶ (ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results_r6['geocoded']:,} ä»¶)
            - H27 (2015å¹´): {results_h27['total']:,} ä»¶ (ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results_h27['geocoded']:,} ä»¶)
            """)

        else:
            # Single dataset mode
            if "R6" in selected_mode:
                incidents_cov = load_incident_data("R6.xlsx")
                data_label = "R6 (2024å¹´)"
            else:
                incidents_cov = load_incident_data_h27("H27.xls")
                data_label = "H27 (2015å¹´)"

            st.markdown(f"### ğŸ“… {data_label} ã®ã‚«ãƒãƒ¼ç‡åˆ†æ")
            st.write(f"å…¨å‡ºå‹•ä»¶æ•°: {len(incidents_cov):,} ä»¶")

            with st.spinner("ä½æ‰€ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ä¸­..."):
                results = analyze_coverage(incidents_cov, data_label)

            if results is None:
                st.error("å‡ºå‹•åœ°ç‚¹ã‚’ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                st.stop()

            incident_points = results["incident_points"]
            mapped_cov = results["mapped"]

            geocoded_rate = results["geocoded"] / results["total"] * 100
            st.write(f"ğŸ“ ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸ: {results['geocoded']:,} / {results['total']:,} ä»¶ ({geocoded_rate:.1f}%)")

            # Calculate coverage for each time threshold
            st.markdown("---")
            st.subheader("ğŸ“Š ã‚«ãƒãƒ¼ç‡çµæœ")

            coverage_results = []
            for minutes in trip_times_cov:
                covered_count = results[f"covered_{minutes}"]
                total_count = results["geocoded"]
                coverage_pct = covered_count / total_count * 100 if total_count > 0 else 0

                coverage_results.append({
                    "åˆ°é”æ™‚é–“": f"{minutes}åˆ†",
                    "ã‚«ãƒãƒ¼æ•°": covered_count,
                    "å…¨ä»¶æ•°": total_count,
                    "ã‚«ãƒãƒ¼ç‡": f"{coverage_pct:.1f}%",
                })

            if coverage_results:
                coverage_df = pd.DataFrame(coverage_results)
                st.dataframe(coverage_df, use_container_width=True, hide_index=True)

                # Show metrics
                cols = st.columns(len(coverage_results))
                for i, res in enumerate(coverage_results):
                    with cols[i]:
                        st.metric(
                            label=f"{res['åˆ°é”æ™‚é–“']}åˆ°é”åœ",
                            value=res["ã‚«ãƒãƒ¼ç‡"],
                            delta=f"{res['ã‚«ãƒãƒ¼æ•°']}/{res['å…¨ä»¶æ•°']}ä»¶"
                        )

            # Render map with coverage visualization
            st.markdown("---")
            st.subheader("ğŸ—ºï¸ ã‚«ãƒãƒ¼çŠ¶æ³ãƒãƒƒãƒ—")

            center_lat_cov = mapped_cov["lat"].mean()
            center_lon_cov = mapped_cov["lon"].mean()
            fmap_cov = folium.Map(location=[center_lat_cov, center_lon_cov], zoom_start=11, tiles="CartoDB Positron")

            # Add isochrone layers
            color_map_cov = {5: "#ff9e9e", 10: "#8aa5ff"}
            for minutes in trip_times_cov:
                iso_layer = isochrones_cov[isochrones_cov["time"] == minutes]
                if iso_layer.empty:
                    continue
                color = color_map_cov.get(minutes, "#4a4a4a")
                folium.GeoJson(
                    data=iso_layer.__geo_interface__,
                    name=f"{minutes}åˆ†åˆ°é”åœ",
                    style_function=lambda _f, c=color, m=minutes: {
                        "fillColor": c,
                        "color": c,
                        "weight": 1.0,
                        "opacity": 0.5,
                        "fillOpacity": 0.15 if m >= 10 else 0.25,
                    },
                ).add_to(fmap_cov)

            # Add station markers
            for _, row in stations_cov.iterrows():
                folium.CircleMarker(
                    location=[row["ç·¯åº¦"], row["çµŒåº¦"]],
                    radius=6,
                    color="#1f1f1f",
                    weight=2,
                    fill=True,
                    fill_color="#f6bd60",
                    fill_opacity=0.9,
                    popup=f"{row['ç•¥ç§°']}",
                ).add_to(fmap_cov)

            # Add incident markers with coverage status
            for _, row in incident_points.iterrows():
                within_5 = row.get("within_5min", False)
                within_10 = row.get("within_10min", False)

                if within_5:
                    color = "#2ecc71"  # Green - covered by 5min
                    status = "5åˆ†å†…"
                elif within_10:
                    color = "#f39c12"  # Orange - covered by 10min
                    status = "10åˆ†å†…"
                else:
                    color = "#e74c3c"  # Red - not covered
                    status = "åœå¤–"

                label_time = row["è¦šçŸ¥"].strftime("%H:%M") if not pd.isna(row.get("è¦šçŸ¥")) else "--:--"
                popup = f"{status} | {row.get('å‡ºå‹•éšŠ', 'ä¸æ˜')} | {label_time}"
                folium.CircleMarker(
                    location=[row.geometry.y, row.geometry.x],
                    radius=4,
                    color=color,
                    fill=True,
                    fill_color=color,
                    fill_opacity=0.8,
                    weight=1.0,
                    popup=popup,
                ).add_to(fmap_cov)

            # Add legend
            legend_html = '''
            <div style="position: fixed; bottom: 50px; left: 50px; z-index: 1000;
                        background-color: white; padding: 10px; border-radius: 5px;
                        border: 2px solid gray; font-size: 12px;">
                <b>å‡ºå‹•åœ°ç‚¹</b><br>
                <span style="color: #2ecc71;">â—</span> 5åˆ†å†…åˆ°é”<br>
                <span style="color: #f39c12;">â—</span> 10åˆ†å†…åˆ°é”<br>
                <span style="color: #e74c3c;">â—</span> åˆ°é”åœå¤–
            </div>
            '''
            fmap_cov.get_root().html.add_child(folium.Element(legend_html))

            folium.LayerControl(collapsed=False).add_to(fmap_cov)
            st.components.v1.html(fmap_cov.get_root().render(), height=720)

            # Show uncovered incidents detail
            if "within_10min" in incident_points.columns:
                uncovered = incident_points[~incident_points["within_10min"]]
                if not uncovered.empty:
                    st.markdown("---")
                    st.subheader(f"âš ï¸ 10åˆ†åˆ°é”åœå¤–ã®å‡ºå‹• ({len(uncovered)} ä»¶)")
                    uncovered_display = uncovered[["date", "è¦šçŸ¥", "å‡ºå‹•å ´æ‰€", "å‡ºå‹•éšŠ"]].copy()
                    uncovered_display.columns = ["æ—¥ä»˜", "è¦šçŸ¥æ™‚åˆ»", "å‡ºå‹•å ´æ‰€", "å‡ºå‹•éšŠ"]
                    st.dataframe(uncovered_display, use_container_width=True, hide_index=True)

    st.info("ã‚¢ãƒ—ãƒªã‚’çµ‚äº†ã™ã‚‹ã«ã¯ã€å®Ÿè¡Œä¸­ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ Ctrl+C ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
